<?xml version="1.0" encoding="UTF-8"?>
<office:document-content xmlns:office="urn:oasis:names:tc:opendocument:xmlns:office:1.0" xmlns:ooo="http://openoffice.org/2004/office" xmlns:fo="urn:oasis:names:tc:opendocument:xmlns:xsl-fo-compatible:1.0" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:meta="urn:oasis:names:tc:opendocument:xmlns:meta:1.0" xmlns:style="urn:oasis:names:tc:opendocument:xmlns:style:1.0" xmlns:text="urn:oasis:names:tc:opendocument:xmlns:text:1.0" xmlns:rpt="http://openoffice.org/2005/report" xmlns:draw="urn:oasis:names:tc:opendocument:xmlns:drawing:1.0" xmlns:dr3d="urn:oasis:names:tc:opendocument:xmlns:dr3d:1.0" xmlns:svg="urn:oasis:names:tc:opendocument:xmlns:svg-compatible:1.0" xmlns:chart="urn:oasis:names:tc:opendocument:xmlns:chart:1.0" xmlns:table="urn:oasis:names:tc:opendocument:xmlns:table:1.0" xmlns:number="urn:oasis:names:tc:opendocument:xmlns:datastyle:1.0" xmlns:ooow="http://openoffice.org/2004/writer" xmlns:oooc="http://openoffice.org/2004/calc" xmlns:of="urn:oasis:names:tc:opendocument:xmlns:of:1.2" xmlns:xforms="http://www.w3.org/2002/xforms" xmlns:tableooo="http://openoffice.org/2009/table" xmlns:calcext="urn:org:documentfoundation:names:experimental:calc:xmlns:calcext:1.0" xmlns:drawooo="http://openoffice.org/2010/draw" xmlns:xhtml="http://www.w3.org/1999/xhtml" xmlns:loext="urn:org:documentfoundation:names:experimental:office:xmlns:loext:1.0" xmlns:field="urn:openoffice:names:experimental:ooo-ms-interop:xmlns:field:1.0" xmlns:math="http://www.w3.org/1998/Math/MathML" xmlns:form="urn:oasis:names:tc:opendocument:xmlns:form:1.0" xmlns:script="urn:oasis:names:tc:opendocument:xmlns:script:1.0" xmlns:formx="urn:openoffice:names:experimental:ooxml-odf-interop:xmlns:form:1.0" xmlns:dom="http://www.w3.org/2001/xml-events" xmlns:xsd="http://www.w3.org/2001/XMLSchema" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:grddl="http://www.w3.org/2003/g/data-view#" xmlns:css3t="http://www.w3.org/TR/css3-text/" xmlns:officeooo="http://openoffice.org/2009/office" office:version="1.3"><office:scripts/><office:font-face-decls><style:font-face style:name="Arial Unicode MS" svg:font-family="&apos;Arial Unicode MS&apos;" style:font-family-generic="swiss"/><style:font-face style:name="Arial Unicode MS1" svg:font-family="&apos;Arial Unicode MS&apos;" style:font-family-generic="system" style:font-pitch="variable"/><style:font-face style:name="Calibri" svg:font-family="Calibri" style:font-adornments="Regular" style:font-family-generic="swiss" style:font-pitch="variable"/><style:font-face style:name="Liberation Sans" svg:font-family="&apos;Liberation Sans&apos;" style:font-family-generic="swiss" style:font-pitch="variable"/><style:font-face style:name="Liberation Serif" svg:font-family="&apos;Liberation Serif&apos;" style:font-family-generic="roman" style:font-pitch="variable"/><style:font-face style:name="PingFang SC" svg:font-family="&apos;PingFang SC&apos;" style:font-family-generic="system" style:font-pitch="variable"/><style:font-face style:name="Songti SC" svg:font-family="&apos;Songti SC&apos;" style:font-family-generic="system" style:font-pitch="variable"/></office:font-face-decls><office:automatic-styles><style:style style:name="P1" style:family="paragraph" style:parent-style-name="Header"><loext:graphic-properties draw:fill-gradient-name="gradient" draw:fill-hatch-name="hatch"/><style:paragraph-properties fo:line-height="100%" fo:text-align="end" style:justify-single-word="false"/></style:style><style:style style:name="P2" style:family="paragraph" style:parent-style-name="Header"><loext:graphic-properties draw:fill-gradient-name="gradient" draw:fill-hatch-name="hatch"/><style:paragraph-properties fo:line-height="100%"/><style:text-properties officeooo:rsid="0022a018" officeooo:paragraph-rsid="00242fc2"/></style:style><style:style style:name="P3" style:family="paragraph" style:parent-style-name="Footer"><style:paragraph-properties fo:text-align="end" style:justify-single-word="false"/><style:text-properties fo:language="en" fo:country="US"/></style:style><style:style style:name="P4" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US"/></style:style><style:style style:name="P5" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00358a2a"/></style:style><style:style style:name="P6" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="003682c7"/></style:style><style:style style:name="P7" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="00415f3c" officeooo:paragraph-rsid="00415f3c"/></style:style><style:style style:name="P8" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="00460fe0" officeooo:paragraph-rsid="00415f3c"/></style:style><style:style style:name="P9" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="004c56b5"/></style:style><style:style style:name="P10" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00560699"/></style:style><style:style style:name="P11" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="0056f98d"/></style:style><style:style style:name="P12" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="005815c8" officeooo:paragraph-rsid="005815c8"/></style:style><style:style style:name="P13" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00682738"/></style:style><style:style style:name="P14" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="006c7d80"/></style:style><style:style style:name="P15" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00748eff"/></style:style><style:style style:name="P16" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="007c82d7"/></style:style><style:style style:name="P17" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="0080c64c"/></style:style><style:style style:name="P18" style:family="paragraph" style:parent-style-name="Bibliography_20_Heading"><style:paragraph-properties fo:break-before="page"/><style:text-properties fo:language="en" fo:country="US"/></style:style><style:style style:name="P19" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="0099263e"/></style:style><style:style style:name="P20" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00c4e0a9"/></style:style><style:style style:name="P21" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00cae438"/></style:style><style:style style:name="P22" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00d49860"/></style:style><style:style style:name="P23" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00d78e37"/></style:style><style:style style:name="P24" style:family="paragraph" style:parent-style-name="Contents_20_1"><style:paragraph-properties><style:tab-stops><style:tab-stop style:position="5.2126in" style:type="right" style:leader-style="dotted" style:leader-text="."/></style:tab-stops></style:paragraph-properties></style:style><style:style style:name="P25" style:family="paragraph" style:parent-style-name="Contents_20_2"><style:paragraph-properties><style:tab-stops><style:tab-stop style:position="5.0161in" style:type="right" style:leader-style="dotted" style:leader-text="."/></style:tab-stops></style:paragraph-properties></style:style><style:style style:name="P26" style:family="paragraph" style:parent-style-name="Contents_20_Heading"><style:paragraph-properties fo:break-before="page"/></style:style><style:style style:name="P27" style:family="paragraph" style:parent-style-name="Contents_20_1"><style:paragraph-properties><style:tab-stops><style:tab-stop style:position="5.2126in" style:type="right" style:leader-style="dotted" style:leader-text="."/></style:tab-stops></style:paragraph-properties></style:style><style:style style:name="P28" style:family="paragraph" style:parent-style-name="Contents_20_2"><style:paragraph-properties><style:tab-stops><style:tab-stop style:position="5.0161in" style:type="right" style:leader-style="dotted" style:leader-text="."/></style:tab-stops></style:paragraph-properties></style:style><style:style style:name="P29" style:family="paragraph" style:parent-style-name="Contents_20_Heading"><style:paragraph-properties fo:break-before="page"/></style:style><style:style style:name="P30" style:family="paragraph" style:parent-style-name="Heading_20_1"><style:text-properties fo:language="en" fo:country="US"/></style:style><style:style style:name="P31" style:family="paragraph" style:parent-style-name="Heading_20_1"><style:paragraph-properties fo:break-before="page"/><style:text-properties fo:language="en" fo:country="US"/></style:style><style:style style:name="P32" style:family="paragraph" style:parent-style-name="Heading_20_2"><style:text-properties fo:language="en" fo:country="US"/></style:style><style:style style:name="P33" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US"/></style:style><style:style style:name="P34" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00ea19be"/></style:style><style:style style:name="P35" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00edde6d"/></style:style><style:style style:name="T1" style:family="text"><style:text-properties officeooo:rsid="002867a0"/></style:style><style:style style:name="T2" style:family="text"><style:text-properties officeooo:rsid="003941f7"/></style:style><style:style style:name="T3" style:family="text"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="006e0a29"/></style:style><style:style style:name="T4" style:family="text"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="00e6305c"/></style:style><style:style style:name="T5" style:family="text"><style:text-properties officeooo:rsid="003e1eeb"/></style:style><style:style style:name="T6" style:family="text"><style:text-properties officeooo:rsid="0042e914"/></style:style><style:style style:name="T7" style:family="text"><style:text-properties officeooo:rsid="004c56b5"/></style:style><style:style style:name="T8" style:family="text"><style:text-properties officeooo:rsid="004e218b"/></style:style><style:style style:name="T9" style:family="text"><style:text-properties officeooo:rsid="005fa15f"/></style:style><style:style style:name="T10" style:family="text"><style:text-properties officeooo:rsid="00ac6e4e"/></style:style><style:style style:name="T11" style:family="text"><style:text-properties officeooo:rsid="00be8bdc"/></style:style><style:style style:name="T12" style:family="text"><style:text-properties officeooo:rsid="00c4873f"/></style:style><style:style style:name="T13" style:family="text"><style:text-properties officeooo:rsid="00dc3064"/></style:style><style:style style:name="T14" style:family="text"><style:text-properties officeooo:rsid="00de182b"/></style:style><style:style style:name="T15" style:family="text"><style:text-properties officeooo:rsid="00dea8bf"/></style:style><style:style style:name="T16" style:family="text"><style:text-properties officeooo:rsid="00f2a227"/></style:style><style:style style:name="fr1" style:family="graphic" style:parent-style-name="Frame"><style:graphic-properties fo:margin-left="0in" fo:margin-right="0in" fo:margin-top="0in" fo:margin-bottom="0in" style:wrap="dynamic" style:number-wrapped-paragraphs="no-limit" style:vertical-pos="top" style:vertical-rel="baseline" style:horizontal-pos="center" style:horizontal-rel="paragraph" fo:padding="0in" fo:border="none" loext:rel-width-rel="paragraph"/></style:style><style:style style:name="fr2" style:family="graphic" style:parent-style-name="Frame"><style:graphic-properties fo:margin-left="0in" fo:margin-right="0in" fo:margin-top="0in" fo:margin-bottom="0in" style:wrap="dynamic" style:number-wrapped-paragraphs="no-limit" style:vertical-pos="top" style:vertical-rel="baseline" style:horizontal-pos="center" style:horizontal-rel="paragraph" fo:padding="0in" fo:border="none"/></style:style><style:style style:name="fr3" style:family="graphic" style:parent-style-name="Graphics"><style:graphic-properties fo:margin-left="0in" fo:margin-right="0in" fo:margin-top="0in" fo:margin-bottom="0in" style:run-through="foreground" style:wrap="none" style:vertical-pos="top" style:vertical-rel="paragraph-content" style:horizontal-pos="center" style:horizontal-rel="paragraph-content" fo:padding="0in" fo:border="none" style:shadow="none" draw:shadow-opacity="100%" style:mirror="none" fo:clip="rect(0in, 0in, 0in, 0in)" draw:luminance="0%" draw:contrast="0%" draw:red="0%" draw:green="0%" draw:blue="0%" draw:gamma="100%" draw:color-inversion="false" draw:image-opacity="100%" draw:color-mode="standard" loext:rel-width-rel="paragraph"/></style:style><style:style style:name="fr4" style:family="graphic" style:parent-style-name="Graphics"><style:graphic-properties fo:margin-left="0in" fo:margin-right="0in" fo:margin-top="0in" fo:margin-bottom="0in" style:run-through="foreground" style:wrap="none" style:vertical-pos="top" style:vertical-rel="paragraph-content" style:horizontal-pos="center" style:horizontal-rel="paragraph-content" fo:padding="0in" fo:border="none" style:shadow="none" draw:shadow-opacity="100%" style:mirror="none" fo:clip="rect(0in, 0in, 0in, 0in)" draw:luminance="0%" draw:contrast="0%" draw:red="0%" draw:green="0%" draw:blue="0%" draw:gamma="100%" draw:color-inversion="false" draw:image-opacity="100%" draw:color-mode="standard" loext:rel-width-rel="paragraph"/></style:style><style:style style:name="Sect1" style:family="section"><style:section-properties style:editable="false"><style:columns fo:column-count="1" fo:column-gap="0in"/></style:section-properties></style:style></office:automatic-styles><office:body><office:text text:use-soft-page-breaks="true"><text:sequence-decls><text:sequence-decl text:display-outline-level="0" text:name="Illustration"/><text:sequence-decl text:display-outline-level="0" text:name="Table"/><text:sequence-decl text:display-outline-level="0" text:name="Text"/><text:sequence-decl text:display-outline-level="0" text:name="Drawing"/><text:sequence-decl text:display-outline-level="0" text:name="Figure"/></text:sequence-decls><text:h text:style-name="P30" text:outline-level="1"><text:bookmark-start text:name="__RefHeading___Toc552_3689700921"/><text:span text:style-name="T7">1 </text:span>Time Series Prediction by Artificial Neural Networks and Differential Evolution in Distributed Environment<text:bookmark-end text:name="__RefHeading___Toc552_3689700921"/></text:h><text:p text:style-name="P4"/><text:p text:style-name="P4">Models for time series prediction using Artificial Neural Networks trained with Differential Evolution in a distributed computational environment have become popular in recent decades. Time series prediction is a complex task that demands the development of more effective and faster algorithms. Artificial Neural Networks are used as a base and trained with historical data. One of the main problems is how to select an accurate Artificial Neural Network training algorithm. There are two general approaches - exact numeric and heuristic optimization methods. When a suitable heuristic is applied, training can be done in a distributed computational environment. In this case, there is a much faster and more realistic output, which helps to achieve better predictions.</text:p><text:p text:style-name="P4"/><text:h text:style-name="P32" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc554_3689700921"/><text:span text:style-name="T8">1.1 </text:span>Introduction<text:bookmark-end text:name="__RefHeading___Toc554_3689700921"/></text:h><text:p text:style-name="P4"/><text:p text:style-name="P6">Time series prediction is the process of forecasting future values in a series of data based on their known values in the past. Decision-makers <text:soft-page-break/>hold a significant responsibility in shaping an effective investment strategy. Investing involves taking acceptable risks with the expectation of a certain profit. The critical aspect of investment lies in balancing the risks taken with the anticipated profits. The primary trading activity revolves around exchanging currencies in the currency market, commonly known as the foreign exchange market or FOREX. Currencies are highly volatile and subject to frequent price changes. When engaging in FOREX trading, decision-makers must make three crucial decisions:</text:p><text:p text:style-name="P6">1. Predict whether the price will rise or fall;</text:p><text:p text:style-name="P6">2. Determine the appropriate volume to buy or sell;</text:p><text:p text:style-name="P6">3. Decide how long to maintain the open position.</text:p><text:p text:style-name="P6"/><text:p text:style-name="P6">Although these decisions may seem straightforward, accurately estimating the price-changing direction is challenging due to many influencing factors. The order volume is directly tied to the level of risk assumed. While high-volume orders can lead to substantial profits when the price-changing direction is well-estimated, they can also result in significant losses. The duration of holding an open position is crucial for maximizing profits or minimizing losses. Financial forecasting is paramount for traders in the currency market, mainly due to the market&apos;s high price dynamics.</text:p><text:p text:style-name="P4"/><text:p text:style-name="P5">The development of effective and reliable financial predictions is a demanding and complex task. Therefore, this field is highly relevant for <text:soft-page-break/>developing and utilizing self-organizing and self-learning systems for prediction. The application of Artificial Neural Networks for predicting time series in the field of the economy has been considered by various authors, such as Dunis [1], Giles [2], and Moody [3].</text:p><text:p text:style-name="P5"/><text:p text:style-name="P5">A widely-used method employs so-called Feed Forward Neural Networks (Haykin [4]). While Feed Forward Neural Networks types of networks are very effective, they suffer from a fundamental flaw: they lack short-term memory. This problem could be avoided by using Recurrent Neural Networks. However, Recurrent Neural Networks, on the other hand, present difficulties due to their challenge in employing precise gradient methods during the learning phase (Werbos [5]). A possible solution is a combined approach for training Artificial Neural Networks using evolutionary algorithms, as <text:span text:style-name="T11">(</text:span>Yao [6]<text:span text:style-name="T11">)</text:span> and several other authors proposed.</text:p><text:p text:style-name="P5"/><text:p text:style-name="P5">Evolutionary algorithms show significantly better results for optimum search in complex multidimensional spaces with many local optima, in which case gradient-based methods will likely get stuck (Holland [7]). In this paper, we present a model of a self-learning system for predicting time series based on ANN and DE in a distributed environment.</text:p><text:p text:style-name="P4"/><text:h text:style-name="P32" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc556_3689700921"/><text:soft-page-break/><text:span text:style-name="T8">1.2 </text:span>Problem <text:span text:style-name="T2">D</text:span>e<text:span text:style-name="T2">fi</text:span>nition<text:bookmark-end text:name="__RefHeading___Toc556_3689700921"/></text:h><text:p text:style-name="P4"/><text:p text:style-name="P9">Several specific aspects come into play in the realm of time series prediction. Firstly, a series of values is tightly intertwined with the temporal axis, forming time-value pairs. Values within a time series exhibit interdependence; they are not isolated entities. Instead, these values are intricately linked with newer values stemming from preceding ones. For instance, consider the exchange rate between the EUR and USD. It is exceedingly uncommon for this rate to undergo dramatic shifts over two consecutive trading days. Instead, the rate displays a gradual and continuous evolution. This characteristic rate serves as a depiction of the trading dynamics between Europe and the USA.</text:p><text:p text:style-name="P9"/><text:p text:style-name="P9">The challenge in predicting financial time series lies in leveraging historical price values to construct a prediction model, subsequently employing this model to forecast future price values. If the model proves accurate, decision-makers can harness the projected values to mitigate investment risks.</text:p><text:p text:style-name="P4"/><text:h text:style-name="P32" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc472_3627904809"/>1.3 <text:span text:style-name="T9">Machine Learning</text:span> Tools<text:bookmark-end text:name="__RefHeading___Toc472_3627904809"/></text:h><text:p text:style-name="P4"/><text:p text:style-name="P10"><text:soft-page-break/>Artificial Neural Networks are mathematical models inspired by natural neural networks. They consist of artificial neurons connected by a series of links. Information is input into the network and then propagated through internal layers (neurons) to produce an output. ANNs are designed to be self-adaptive systems capable of altering their internal structure based on external or internal information encountered during the learning process. They represent intricate relationships between inputs and outputs (functional relations) or identify patterns within a dataset (data mining).</text:p><text:p text:style-name="P10"/><text:p text:style-name="P10">The concept of ANNs draws inspiration from biological central nervous systems and their components, such as neurons, axons, dendrites, and synapses. An ANN comprises a network of uncomplicated processing elements that collectively manifest complex global behaviors via interconnections between processing units and their parameters. In practical terms, utilizing ANNs involves employing algorithms to adjust connections&apos; strength (weights), thereby achieving the desired signal flow.</text:p><text:p text:style-name="P10"/><text:p text:style-name="P10">ANNs resemble natural neural networks in that the processing elements execute network operations collectively and simultaneously. In modern computing, the ANN approach is often combined with non-adaptive techniques to yield improved practical outcomes. The primary advantage of employing ANNs is their capacity to deduce a function from observed <text:soft-page-break/>examples. This proves particularly advantageous when tackling problems where the intricacy of the studied process (or its accompanying data) renders manual function derivation exceedingly challenging.</text:p><text:p text:style-name="P4"/><text:p text:style-name="P11">Differential Evolution is a population-based meta-heuristic optimization method rooted in evolution. DE was developed by Kenneth Price and Rainer Storn [8] and drew inspiration from classical genetic algorithms (GAs). However, meta-heuristics like DE do not guarantee the discovery of an optimal solution. DE finds application in exploring complex, high-dimensional search spaces. While it finds primary use in continuous problems involving real numbers, its applicability to discrete problems is limited. Notably, DE does not mandate differentiability, a requirement in classical optimization techniques such as gradient descent.</text:p><text:p text:style-name="P11"/><text:p text:style-name="P11">Much like classical GAs, DE employs a population of potential solutions. It generates fresh candidates by melding existing ones using crossover, mutation, and selection rules. The best candidates are retained based on objective function evaluation, obviating the need for gradient usage.</text:p><text:p text:style-name="P11"/><text:p text:style-name="P11">A key distinction arises in the mutation operator in comparing DE with GA. DE&apos;s mutation relies on the calculation of difference vectors, rendering it considerably more potent than GA&apos;s mutation approach, which adjusts individual values within the mutated chromosome. One drawback of the <text:soft-page-break/>difference vector approach lies in its difficulty in effecting minor changes to discrete values.</text:p><text:p text:style-name="P4"/><text:h text:style-name="P32" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc474_3627904809"/>1.4 <text:span text:style-name="T9">Forecasting </text:span>Organization<text:bookmark-end text:name="__RefHeading___Toc474_3627904809"/></text:h><text:p text:style-name="P4"/><text:p text:style-name="P4">The forecasting model is built upon a standard Artificial Neural Network and employs a combined training approach involving Differential Evolution and back-propagation. The ANN&apos;s topology is a research subject and is parameterized on a remote server. Neuronal connections can take three forms: strictly forward, forward-backward, and fully connected (establishing connections with all other neurons, including themselves). A linear summation function and a sigmoid activation function characterize each neuron.</text:p><text:p text:style-name="P4"/><text:p text:style-name="P4"><text:soft-page-break/><draw:frame draw:style-name="fr2" draw:name="Frame1" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="0"><draw:text-box fo:min-height="2.6201in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image1" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="2.6201in" style:rel-height="scale" draw:z-index="1"><draw:image xlink:href="Pictures/1000000000000A1E00000549EAF7DA525E2C6E7F.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure0" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">1</text:sequence>: Computers organization</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P4"/><text:p text:style-name="P13">In Differential Evolution, a variant of genetic algorithms, each chromosome represents a set of weights for a specific topology of an Artificial Neural Network. As depicted in Figure 1, the overall DE population is showcased on a remote server. Individual computational units (client machines) load subsets of the global DE population (as illustrated in step 1 of Figure 3) and engage in localized DE-BP mixed ANN training (depicted in step 2 of Figure 3). At regular intervals, these local machines establish connections with the remote server. During these connections, the local populations are updated, and the outcomes of local computations are reported (as shown in step 3 of Figure 3).</text:p><text:p text:style-name="P13"/><text:p text:style-name="P13"><text:soft-page-break/>Due to DE&apos;s remarkable parallelism, no theoretical constraint exists on the number of computational units that can be employed. However, the remote server serves as a bottleneck in the system due to technical limitations. The server&apos;s capacity determines the maximum number of simultaneously connected clients. Notably, each client is not required to maintain a constant connection with the remote server. Consequently, this technical limitation can be easily surmounted. Each computational unit can perform calculations for weeks or even months before necessitating a connection to the remote server.</text:p><text:p text:style-name="P4"/><text:p text:style-name="P4"><draw:frame draw:style-name="fr2" draw:name="Frame2" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="2"><draw:text-box fo:min-height="3.1661in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image2" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="3.1661in" style:rel-height="scale" draw:z-index="3"><draw:image xlink:href="Pictures/1000000000000948000005DC201979DAD9828B19.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure1" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">2</text:sequence>: ANN training <text:span text:style-name="T3">with</text:span> DE-BP</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P4"/><text:p text:style-name="P14"><text:soft-page-break/>As depicted in Figure 2, the DE-based training process comprises five fundamental steps:</text:p><text:p text:style-name="P14">1. Loading a set of weights (a chromosome) from DE into the ANN.</text:p><text:p text:style-name="P14">2. Loading training examples into the ANN.</text:p><text:p text:style-name="P14">3. Calculating prediction values.</text:p><text:p text:style-name="P14">4. Computing the total prediction error (older data carry less impact on the calculated error).</text:p><text:p text:style-name="P14">5. Estimating chromosome fitness.</text:p><text:p text:style-name="P14"/><text:p text:style-name="P14">Every set of weights (chromosome) is introduced into the ANN structure. Subsequently, each input value undergoes a feed-forward process. The resultant output value is then juxtaposed with the anticipated value, and the disparity between them contributes to the overall error associated with this specific set of weights. This calculated total error serves as the fitness value for the DE population. The primary objective of DE optimization is to minimize the total error of the ANN. Due to the real-time nature of the training process (where new data continually emerges over time), the pursuit of total error minimization remains ongoing.</text:p><text:p text:style-name="P4"/><text:p text:style-name="P4"><text:soft-page-break/><draw:frame draw:style-name="fr2" draw:name="Frame3" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="4"><draw:text-box fo:min-height="2.8555in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image3" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="2.8555in" style:rel-height="scale" draw:z-index="5"><draw:image xlink:href="Pictures/1000000000000882000004D8087FEE1B151F6F0D.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure2" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">3</text:sequence>: Computation on a local machine</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P4"/><text:p text:style-name="P15">All computations are carried out locally, as depicted in Figure 3. Local computations encompass the Differential Evolution (DE) training loop, involving crossover and mutation operations. These operations are elaborated upon in Figures 4 and 5, respectively. Parent chromosomes are designated for crossover via the Genetic Algorithm (GA) selection rule. While various selection rules exist, the current model randomly selects two parents, determining the surviving parent based on a survival percentage.</text:p><text:p text:style-name="P15"/><text:p text:style-name="P15">The crossover operation is considered disruptive to the Artificial Neural Network (ANN) training process, and consequently, it can be regulated as <text:soft-page-break/>a parameter. Subsequently, the mutation phase is executed. The conventional DE mutation involves the summation of mutated chromosomes with a weighted difference vector derived from the discrepancy between two other randomly selected chromosomes and then multiplied by a weight coefficient. This mutation operator is a distinctive advantage of DE when juxtaposed with GA.</text:p><text:p text:style-name="P4"/><text:p text:style-name="P4"><text:soft-page-break/><draw:frame draw:style-name="fr2" draw:name="Frame4" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="6"><draw:text-box fo:min-height="4.7043in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image4" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="4.7043in" style:rel-height="scale" draw:z-index="7"><draw:image xlink:href="Pictures/100000000000070B0000069B6411A92D23C25DC2.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure3" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">4</text:sequence>: Crossover</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P4"/><text:p text:style-name="P4"><text:soft-page-break/><draw:frame draw:style-name="fr2" draw:name="Frame5" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="8"><draw:text-box fo:min-height="4.2898in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image5" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="4.2898in" style:rel-height="scale" draw:z-index="9"><draw:image xlink:href="Pictures/1000000000000918000007C78D825628614DB685.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure4" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">5</text:sequence>: Mutation</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P4"/><text:p text:style-name="P16">Local copies of ANNs are distributed to each computational node for parallel training. Differential Evolution and Backpropagation techniques are applied on the local machines, with constant synchronization with the remote server. When DE is used for training the ANN, the issue of slow learning rates can be mitigated by transitioning to BP training. This process involves eliminating all recurrent connections, as illustrated in Figure 6.</text:p><text:p text:style-name="P16"><text:soft-page-break/></text:p><text:p text:style-name="P16"><draw:frame draw:style-name="fr2" draw:name="Frame6" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="10"><draw:text-box fo:min-height="2.4547in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image6" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="2.4547in" style:rel-height="scale" draw:z-index="11"><draw:image xlink:href="Pictures/100000000000062500000302B66BC73B6A9ADA13.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure5" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">6</text:sequence>: Switching ANN topology for BP training</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P16"/><text:p text:style-name="P16">The rate of learning convergence serves as a valuable indicator for determining when to switch between DE and BP and vice versa. Furthermore, it is worth noting that BP can be conceptualized as a specific case of Genetic Algorithm (GA) mutation.</text:p><text:p text:style-name="P4"/><text:h text:style-name="P31" text:outline-level="1"><text:bookmark-start text:name="__RefHeading___Toc351_966847004"/>2 Distributed Evolutionary Computing Migration Strategy by Accidental Node Involvement<text:bookmark-end text:name="__RefHeading___Toc351_966847004"/></text:h><text:p text:style-name="P4"/><text:p text:style-name="P17">Distributed evolutionary algorithms are implemented across heterogeneous computing nodes. In a distributed environment, it is common for these nodes to vary in terms of operating systems and hardware configurations. Such an environment presents significant challenges, particularly concerning network latency. Specific evolutionary optimization algorithms lend themselves well to distributed computing implementation due to their high level of parallel scalability. Typically, only the fitness function calculations are distributed synchronously or asynchronously.</text:p><text:p text:style-name="P17"/><text:p text:style-name="P17">In the former scenario, the population is solely hosted on the primary node. In the latter scenario, each node maintains a portion of the distributed population, a configuration known as the island model. Another prevalent approach relies on shared memory, granting each computing node access to the entire population - a model known as the fine-grained model. Various other models represent hybridizations of these basic approaches.</text:p><text:p text:style-name="P17"><text:soft-page-break/></text:p><text:p text:style-name="P17">Within the island model, a pivotal parameter pertains to the migration strategy. The most commonly employed node topology is the ring topology, where each node periodically forwards its best-performing individual to the subsequent node in the ring. However, exploring a hybrid model incorporating star topology and involving neighboring nodes could yield improvements.</text:p><text:p text:style-name="P4"/><text:h text:style-name="P32" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc979_966847004"/>2.1 Introduction<text:bookmark-end text:name="__RefHeading___Toc979_966847004"/></text:h><text:p text:style-name="P4"/><text:p text:style-name="P4">Evolutionary Algorithms (EAs) represent efficient search methods grounded in natural selection and recombination principles. They have found successful applications in solving problems across various domains, including business, engineering, and science (Goldberg [9], Pappa [10]). By harnessing the power of EAs, viable solutions can be discovered within a reasonable time-frame. For more minor problems, a single computing node can suffice. However, as problems increase in size or complexity, the time required to find suitable solutions also escalates.</text:p><text:p text:style-name="P4"/><text:p text:style-name="P4">Given the burgeoning popularity of parallel computing, employing it has emerged as a promising strategy to expedite EAs. Certain distributed evolutionary algorithms (DEAs) operate with a singular population, while <text:soft-page-break/>others divide the population into multiple, relatively independent sub-populations. A comprehensive classification can be found in existing literature (Adamidis [11], Gordon [12], Lin [13]). In distributed computing, the migration strategy is pivotal in implementing DEAs.</text:p><text:p text:style-name="P4"/><text:p text:style-name="P4">In cases where EA&apos;s population is distributed among numerous computing nodes, each node possesses a portion of the population referred to as a sub-population. In such instances, local recombination and fitness function evaluations are conducted. To enhance optimization convergence, the computing nodes exchange individuals, a process is known as migration. Several parameters govern this migration process, shaping a strategy for information interchange in DEAs.</text:p><text:p text:style-name="P4"/><text:p text:style-name="P4">The primary parameter pertains to traveler selection, determining the approach for choosing an individual from the sub-population to be dispatched. In most scenarios, the optimal individual is chosen. The second crucial parameter is migration frequency, dictating how often selected individuals traverse between sub-populations. This parameter is specific to the problem and is typically adjusted through experimental means. The third and most pivotal parameter concerns the migration destination, signifying how nodes are structured in terms of topology and how each traverses within this topology.</text:p><text:p text:style-name="P4"/><text:p text:style-name="P4"><text:soft-page-break/>One option for this parameter is individual broadcasting, wherein each node receives a copy of selected travelers from other nodes. An alternative is a ring topology, where each traveler migrates to the adjacent node. A grid topology is also viable (Spiessens [14], Kruger [15]), with each node having four neighbors. Numerous other topologies exist, including hierarchical, 3D-based, hybrid, and more.</text:p><text:p text:style-name="P4"/><text:p text:style-name="P4">A distribution strategy grounded in the participation of incident nodes (where volunteers are expected to join the project and contribute computing power) holds the potential to address these challenges. Computing nodes are structured in a star topology, and the island model (Tanese [16], Uchida [17]) is applied to DEAs.</text:p><text:p text:style-name="P4"/><text:h text:style-name="P32" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc772_951591551"/>2.2 Accidental Node Involvement<text:bookmark-end text:name="__RefHeading___Toc772_951591551"/></text:h><text:p text:style-name="P4"/><text:p text:style-name="P19">The goal of optimization is to adjust the weights of an Artificial Neural Network in order to achieve improved forecasting. Training an ANN with Evolutionary Algorithms (EAs) can be highly computationally intensive, especially when dealing with ANNs with more than 400 weights, depending on their topology. Due to the time-consuming nature of EA-based ANN training, the computing nodes should operate relatively autonomously.</text:p><text:p text:style-name="P19"><text:soft-page-break/></text:p><text:p text:style-name="P19">The distributed system is structured in a star topology, comprising a lightweight central node (server) and heavily loaded remote computing nodes (clients). In the context of Differential Evolution Algorithm (DEA) implementation, the island model is suitable. A global EA population is situated in the central node, while numerous local EA populations are distributed among the remote computing nodes. Each remote computing node can join or leave the system at any moment, functioning asynchronously (Figure 7). Upon a new client joining, the central node transmits a subset of the global population. Subsequently, communication between the central and remote nodes ceases, allowing the remote node to evolve the local EA population using a sequential EA approach.</text:p><text:p text:style-name="P4"/><text:p text:style-name="P4"><text:soft-page-break/><draw:frame draw:style-name="fr2" draw:name="Frame7" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="12"><draw:text-box fo:min-height="4.9866in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image7" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="4.9866in" style:rel-height="scale" draw:z-index="13"><draw:image xlink:href="Pictures/10000000000004150000040FC03E2C163005CE4E.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure6" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">7</text:sequence>: Accidental node involvement</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P4"/><text:p text:style-name="P4">The communication between the remote node and the central node is reinitiated only if a better solution is found on the remote node, which must then be reported to the central node. Through this organization of the calculation process, each remote node can operate for hours, weeks, <text:soft-page-break/>or even months before sending any information to the central node. In practical terms, a failure of the central node will not impact the performance of the remote nodes. Even in a central node failure, the remote nodes will transmit their results once the central node becomes available.</text:p><text:p text:style-name="P4"/><text:p text:style-name="P4">The distribution of individuals occurs exclusively during the process of remote node joining. Local best-found solutions are relayed to the central node, migrating to the subsequent remote node upon incorporation. Due to the sluggish nature of EA-based training for ANNs, this distribution strategy proves highly efficient. Additionally, in financial time series forecasting domains, the training set undergoes constant fluctuations due to the incessant influx of new data. Consequently, the objective function, aimed at optimizing the total error of ANNs, remains in a state of perpetual flux. Hence, the application of continuous ANN training becomes imperative.</text:p><text:p text:style-name="P4"/><text:h text:style-name="P32" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc632_1783862520"/>2.3 Distribution Parameterization<text:bookmark-end text:name="__RefHeading___Toc632_1783862520"/></text:h><text:p text:style-name="P4"/><text:p text:style-name="P4">Distribution is an essential operation responsible for the seamless exchange of individuals among the nodes within the DEA framework. When considering distribution, a set of parameters becomes relevant, <text:soft-page-break/>including the distribution gap, distribution rate, selection/replacement, topology, and heterogeneity. Specific parameters can be applied in the context of the Distribution Strategy by Accidental Node Involvement, while others lack reasonable significance.</text:p><text:p text:style-name="P4"/><text:p text:style-name="P4">Defining a parameter such as the distribution gap becomes intricate in the context of Accidental Node Involvement. The literature outlines two prevalent approaches. The first one involves regularly measuring the distribution gap after several steps. The second one is probabilistic, occurring in each generation with a certain probability (Gorges-Schleuter [18], Munetomo [19], Voigt [20]). The distribution gap aligns more closely with the probabilistic model. As elucidated in the preceding sections, distribution occurs only once when the computing node joins the system. From this perspective, the distribution gap follows an exponential distribution in a probabilistic manner. The model avoids the issues of ineffectiveness or super-individual problems since distribution is relatively infrequent.</text:p><text:p text:style-name="P4"/><text:p text:style-name="P4">The distribution rate represents a parameter specific to the problem, determining the number of individuals that will traverse among the local populations. Typically, it is denoted as a population percentage or absolute value. Various recommendations exist regarding the estimation of this parameter, although the prevailing approach is experimental, as <text:soft-page-break/>evidenced by studies such as <text:span text:style-name="T10">(</text:span>Tanese [16], Belding [21], and Mejia-Olvera [22]<text:span text:style-name="T10">)</text:span>. In the proposed model, this parameter is defined as a percentage (a fraction) of the global population (i.e., the population residing on the central node). When expressed in absolute terms, it equates to the magnitude of the local population (i.e., the population size on the remote node).</text:p><text:p text:style-name="P4"/><text:p text:style-name="P4">There are two main approaches for selecting migrants: the first involves choosing the most exceptional individuals, and the second entails selecting individuals randomly. Of course, it is worth noting that numerous alternative selection methods can be employed, similar to those utilized in genetic algorithms (Baker [23], Lim [24]). In the proposed model, the selection process occurs at the central node, and random selection is utilized. The quantity of migrants selected corresponds to the size of the remote sub-population. Notably, there is no replacement procedure since distribution occurs only once, from the central to the remote node. Based on the node joining process, several identified individuals can be directed in the opposite direction (remote to central). However, these individuals will then participate in other remote sub-populations.</text:p><text:p text:style-name="P4"/><text:p text:style-name="P4">DEAs are categorized into two standard models: the stepping-stone and island models. This categorization hinges on whether individuals have the <text:soft-page-break/>freedom to migrate to any local population or are restricted to migrating only to geographically nearby islands. Numerous studies have endeavored to determine the optimal topology for a DEA, with the ring and hyper-cube topologies emerging as the most favored choices in many instances (Adamidis [11], Gordon [12], Lin [13], Mejia-Olvera [22]). In most cases, issues such as parallelization and scalability arise with fully connected and centralized topologies due to their tight connectivity. The proposed model&apos;s most suitable topology is a star configuration featuring a centralized node and relatively independent remote computing nodes. EA-based ANN training is a time-intensive process, often leading to extended periods during which remote computing nodes operate without communication with the central node. Consequently, the central node poses minimal risk to the distributed system. Disruption of the connection with the central node does not affect the local optimization process. The central node&apos;s failure only impacts new nodes attempting to join the system. Despite these drawbacks, the proposed model exhibits exceptional scalability due to the lightweight nature of the central node.</text:p><text:p text:style-name="P4"/><text:p text:style-name="P4">Accidental Node Involvement is ideally suited for heterogeneity. On each remote computing node, a distinct optimization algorithm can be employed. This approach allows for a much-improved balance between exploration and exploitation, a well-established trade-off decision in evolutionary algorithms. The relevance of this parameter varies <text:soft-page-break/>depending on the problem. For instance, in the context of the distribution of weights in artificial neural networks, it can be seamlessly implemented within a distributed system due to the availability of highly effective gradient-based training algorithms.</text:p><text:p text:style-name="P4"/><text:h text:style-name="P31" text:outline-level="1"><text:bookmark-start text:name="__RefHeading___Toc634_1783862520"/>3 Modifications of Artificial Neural Networks<text:bookmark-end text:name="__RefHeading___Toc634_1783862520"/></text:h><text:p text:style-name="P4"/><text:p text:style-name="P4">Classical artificial neural networks have been studied and utilized in various industries for decades. While they have demonstrated successful applications in specific tasks, their performance could be better in others. Despite being extensively researched, there remain aspects within them that can be modified to achieve greater efficiency. The key aspect of artificial neural networks is their remarkable efficiency once they are trained; however, the training process is frequently characterized by slowness and inefficiency.</text:p><text:p text:style-name="P4"/><text:p text:style-name="P4">In this section, various ideas for modifying different components within artificial neural networks will be presented. These ideas offer opportunities to enhance efficiency and reduce training time.</text:p><text:p text:style-name="P4"/><text:h text:style-name="Heading_20_2" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc718_1268615346"/>3.1 Alternative Activation Function Derivative<text:bookmark-end text:name="__RefHeading___Toc718_1268615346"/></text:h><text:p text:style-name="P4"/><text:p text:style-name="P20">Classic artificial neural networks can be conceptualized as directed weighted graphs. These networks operate in two modes  training and performance. The training process involves seeking appropriate values for <text:soft-page-break/>the weights in the graph (Keremedchiev [25], Tomov [26], Zankinski [27]) to enable the network to optimally correlate information from the input with the information at the output. In classical three-layer networks, information is propagated from the input to the output, with each node receiving signals from the nodes in the preceding layer.</text:p><text:p text:style-name="P20"/><text:p text:style-name="P20">These signals are derived using a summing function, most commonly linear (involving the multiplication of the signal by the weight assigned to the connection between two nodes). The cumulative input signals are then directed to a threshold function that determines the activation level of each node.</text:p><text:p text:style-name="P4"/><text:p text:style-name="P4"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame8" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="5.4083in" style:rel-height="scale-min" draw:z-index="14"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image8" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="5.0201in" style:rel-height="scale" draw:z-index="15"><draw:image xlink:href="Pictures/10000000000001F4000001F476EB77554A5ADDF3.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure7" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">8</text:sequence>: Decaying sine wave activation function</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P4"/><text:p text:style-name="P4"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame11" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="5.4083in" style:rel-height="scale-min" draw:z-index="20"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image11" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="5.0201in" style:rel-height="scale" draw:z-index="21"><draw:image xlink:href="Pictures/10000000000001F4000001F4E5D0BC5BEE7C2055.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure8" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">9</text:sequence>: Exponent regulated sin<text:span text:style-name="T4">e</text:span> activation function</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P4"/><text:p text:style-name="P21">Multiple activation functions are commonly used in practice (Karlik [28]). In this context, we consider a decaying function with a periodic character (as shown in Figure 8 <text:span text:style-name="T13">and Figure 9</text:span>). The periodic component is an effect of the sine component. When employing the correct training methods, <text:soft-page-break/>such as the error backpropagation method, the derivative of the activation function becomes of primary importance. The first derivative&apos;s values directly determine the extent to which the weights will change during the training process.</text:p><text:p text:style-name="P21"/><text:p text:style-name="P21"><draw:frame draw:style-name="fr1" draw:name="Frame9" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="5.4083in" style:rel-height="scale-min" draw:z-index="16"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image9" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="5.0201in" style:rel-height="scale" draw:z-index="17"><draw:image xlink:href="Pictures/10000000000001F4000001F41B49D716BE89478B.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure9" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">10</text:sequence>: First derivative of decaying sine wave activation function</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P21"><text:soft-page-break/></text:p><text:p text:style-name="P21"><draw:frame draw:style-name="fr1" draw:name="Frame12" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="5.4083in" style:rel-height="scale-min" draw:z-index="22"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image12" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="5.0201in" style:rel-height="scale" draw:z-index="23"><draw:image xlink:href="Pictures/10000000000001F4000001F4F14C431A42B0C8A1.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure10" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">11</text:sequence>: First derivative of exponent regulated sine activation function</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P21"/><text:p text:style-name="P21">In situations where the first derivative also exhibits a periodic component (as depicted in Figure <text:span text:style-name="T14">10</text:span> <text:span text:style-name="T14">and Figure 11</text:span>), it becomes possible to replace this first derivative mechanically. Such a substitution is feasible due to the <text:soft-page-break/>organizational structure of software libraries designed for working with artificial neural networks.</text:p><text:p text:style-name="P4"/><text:p text:style-name="P22">Each neuron within the artificial neural network determines its activity level based on a normalization function called an activation function. It is crucial to standardize the output of each neuron to a specific interval, typically ranging between zero and one or, alternatively, between minus one and plus one. This necessity arises due to the varying numbers of neurons across different layers of artificial neural networks. Without normalization, the signals transmitted to the subsequent layer would be disproportionate and uneven.</text:p><text:p text:style-name="P22"/><text:p text:style-name="P22">The activation function<text:span text:style-name="T15">s</text:span> depicted in Figure 8 <text:span text:style-name="T15">and Figure 9</text:span> <text:span text:style-name="T15">have</text:span> been selected to emulate the natural saturation processes found in nature. When the sum of inputs is positive, the neuron generates a positive output value. Conversely, in the presence of a negative input sum, the neuron yields a negative output value. Simultaneously, if the input signals are excessively intense, encompassing both negative and positive values, the simulated saturation process prevents the neuron from emitting a signal.</text:p><text:p text:style-name="P4"/><text:p text:style-name="P4"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame10" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="5.7134in" style:rel-height="scale-min" draw:z-index="18"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image10" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="5.0201in" style:rel-height="scale" draw:z-index="19"><draw:image xlink:href="Pictures/10000000000001F4000001F412CC8C5452BC8AAC.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure11" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">12</text:sequence>: First derivative alternative of decaying sine wave activation function</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P23"/><text:p text:style-name="P23">This systematic activation mechanism enables the neurons within a given layer to evenly distribute their responsibilities, thereby fostering a more balanced representation of information throughout the network.</text:p><text:p text:style-name="P4"><text:soft-page-break/></text:p><text:p text:style-name="P4">When the activation function includes a periodic component, it is also reflected in its first derivative. On one hand, the periodic aspect of the first derivative is noticeable; however, two distinct discontinuities are also clearly evident (see Figure <text:span text:style-name="T15">12</text:span>). As a result of these complexities, the convergence of the backpropagation learning algorithm becomes slower. An elegant approach to expedite the process involves replacing the first derivative with a function that closely follows the same form yet lacks both the periodic component and breakpoints (refer to Figure 10). Not only does this alternative derivative exhibit more appropriate mathematical properties but it is also computed more quickly than the original derivative.</text:p><text:p text:style-name="P4"/><text:h text:style-name="Heading_20_2" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc1666_1268615346"/>3.2 Long Short Term Memory in Multilayer Perceptron Pair<text:bookmark-end text:name="__RefHeading___Toc1666_1268615346"/></text:h><text:p text:style-name="P4"/><text:p text:style-name="P34">In artificial neural networks&apos; most commonly used case, such as the multilayer perceptron, signals are transmitted from the input to the output. Each neuron&apos;s input information is derived from the outputs of the preceding neurons. The external impact received through this process is then subjected to a summation function. The prevailing summation function is typically the linear function, where the output signals of <text:soft-page-break/>neurons are multiplied by the connection weights. However, it is essential to note that functions other than the linear function can also be applied. The outcome of the summation function is subsequently channeled into a normalization function, which determines the neuron&apos;s activation level. The literature presents a variety of proposed activation functions, some well-established, while others are not as recognized.</text:p><text:p text:style-name="P34"/><text:p text:style-name="P34">In the classical multilayer perceptron, links between neurons exist solely from the input to the output. Signals cannot propagate from the output to the input. In the backward pass, only the neurons&apos; errors are propagated. Consequently, in an artificial neural network with such a topology, there is no capacity to retain past information circulating within the network. Due to this limitation, multilayer perceptrons are not particularly well-suited for forecasting tasks, particularly in the field of financial time series forecasting. Recurrent links were introduced to address the absence of memory in multilayer perceptrons, as seen in the Jordan and Elman networks. This innovation allows for the retention of historical information within the network.</text:p><text:p text:style-name="P4"/><text:p text:style-name="P4"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame13" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="5.8583in" style:rel-height="scale-min" draw:z-index="24"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr3" draw:name="Image13" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="5.4701in" style:rel-height="scale" draw:z-index="25"><draw:image xlink:href="Pictures/10000001000003E600000441CA747C47ADF55E41.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure12" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">13</text:sequence>: A pair of multilayer perceptrons</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P4"/><text:p text:style-name="P35">Memory can be realized using a paired network of two multilayer perceptrons, as depicted in Figure 13. In this arrangement, MLP1 takes as input the historical values of the time series combined with the output of <text:soft-page-break/>MLP2. The anticipated outcome is generated at the output layer of MLP1. Additionally, the output of MLP1 serves as an input for MLP2, followed by the propagation of signals within MLP2. This process endows MLP2 with the role of a network memory component. In this implementation, both multilayer perceptrons undergo training using backpropagation to minimize errors.</text:p><text:p text:style-name="P35"/><text:p text:style-name="P35">Prior to entering MLP1, the time series values undergo normalization. The time series is segregated into two segments: the past frame (lag) and the future frame (lead), each with its specific conditional division.</text:p><text:p text:style-name="P4"/><text:h text:style-name="Heading_20_2" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc1317_2021269790"/>3.3 Self-Growing Multilayer Perceptron <text:span text:style-name="T16">f</text:span>or Time Series Forecasting<text:bookmark-end text:name="__RefHeading___Toc1317_2021269790"/></text:h><text:p text:style-name="P4"/><text:p text:style-name="P4"/><text:p text:style-name="P4"/><text:p text:style-name="P4"/><text:bibliography text:style-name="Sect1" text:name="Bibliography1"><text:bibliography-source><text:index-title-template text:style-name="Bibliography_20_Heading">Bibliography</text:index-title-template><text:bibliography-entry-template text:bibliography-type="article" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="book" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="booklet" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="conference" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="custom1" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="custom2" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="custom3" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="custom4" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="custom5" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="email" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="inbook" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="incollection" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="inproceedings" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="journal" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="manual" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="mastersthesis" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="misc" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="url"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="phdthesis" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="proceedings" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="techreport" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="unpublished" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="www" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template></text:bibliography-source><text:index-body><text:index-title text:style-name="Sect1" text:name="Bibliography1_Head"><text:p text:style-name="P18">Bibliography</text:p></text:index-title></text:index-body></text:bibliography><text:p text:style-name="P4"><text:span text:style-name="T5">[1] </text:span>Dunis, C.L. Williams, M., Modelling and trading the eur/usd exchange rate: Do neural network models perform better? Derivatives Use, Trading and Regulation, 8(3), pp. 211-239 (2002)</text:p><text:p text:style-name="P7">[<text:span text:style-name="T6">2</text:span>] Giles, C.L., Lawrence, S. Tsoi, A.C., Noisy time series prediction using a recurrent neural network and grammatical inference. Machine Learning, 44(1/2), pp. 161-183 (2001)</text:p><text:p text:style-name="P8">[3] Moody, J.E., Economic forecasting: Challenges and neural network solutions. Proceedings of the International Symposium on Artificial Neural Networks, Hsinchu, Taiwan (1995)</text:p><text:p text:style-name="P8">[4] Haykin, S., Neural Networks, A Comprehensive Foundation. Prentice-Hall, Inc., 2nd edition (1999)</text:p><text:p text:style-name="P8">[5] Werbos, P., Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78(10), pp. 1550-1560 (1990)</text:p><text:p text:style-name="P8">[6] Yao, X., Evolving artificial neural networks. Proc of the IEEE, 87(9), pp. 1423-1447 (1999)</text:p><text:p text:style-name="P8">[7] Holland, J., Adaptation In Natural and Artificial Systems. The University of Michigan Press (1975)</text:p><text:p text:style-name="P12">[8] Storn, R., Price, K. Differential evolution - a simple and efficient heuristic for global optimization over continuous spaces. Journal of Global Optimization 11, 341-359 (1997)</text:p><text:p text:style-name="P8"><text:soft-page-break/>[9] Goldberg, D.E.: Genetic and evolutionary algorithms come of age. Commun. ACM 37(3), 113-119 (1994)</text:p><text:p text:style-name="P8">[10] Pappa, G., Ochoa, G., Hyde, M., Freitas, A., Woodward, J., Swan, J.: Contrasting meta-learning and hyper-heuristic research: the role of evolutionary algorithms. Genet. Program. Evolvable Mach. 15(1), 335 (2014)</text:p><text:p text:style-name="P8">[11] Adamidis, P.: Review of parallel genetic algorithms bibliography. Technical Report version 1, Aristotle University of Thessaloniki, Thessaloniki, Greece (1994)</text:p><text:p text:style-name="P8">[12] Gordon, V.S., Whitley, D.: Serial and parallel genetic algorithms as function optimizers. In: Forrest S. (ed.) Proceedings of the Fifth International Conference on Genetic Algorithms, pp. 177183. Morgan Kaufmann, San Mateo (1993)</text:p><text:p text:style-name="P8">[13] Lin, S.-C., Punch, W., Goodman, E.: Coarse-grain parallel genetic algorithms - categorization and new approach. In: Sixth IEEE Symposium on Parallel and Distributed Processing, IEEE Computer Society Press, Los Alamitos, CA (1994)</text:p><text:p text:style-name="P8">[14] Spiessens, P., Manderick, B.: A massively parallel genetic algorithm. In: Belew, R.K., Booker, L.B. (eds.) Proceedings of the 4th International Conference on Genetic Algorithms, pp. 279286. Morgan Kaufmann, San Francisco (1991)</text:p><text:p text:style-name="P8">[15] Kruger, F., Wagner, D., Collet, P.: Massively parallel generational GA on GPGPU applied to power load profiles determination. In: Legrand, P., <text:soft-page-break/>Corsini, M.-M., Hao, J.-K., Monmarche, N., Lutton, E., Schoenauer, M. (eds.) EA 2013. LNCS, vol. 8752, pp. 227239. Springer, Heidelberg (2014)</text:p><text:p text:style-name="P8">[16] Tanese, R.: Distributed genetic algorithms. In: Schaffer, J.D. (ed.) Proceedings of the 3rd International Conference on Genetic Algorithms, pp. 434439. Morgan Kaufmann, San Francisco (1989)</text:p><text:p text:style-name="P8">[17] Uchida, T., Matsuzawa, T., Inoguchi, Y.: The influence of elitism strategy on migration intervals of a distributed genetic algorithm. In: Proceedings in Adaptation, Learning and Optimization, vol. 2, pp. 363374 (2015)</text:p><text:p text:style-name="P8">[18] Gorges-Schleuter, M.: ASPARAGOS An asynchronous parallel genetic optimisation strategy. In: Schaffer, J.D. (ed.) Proceedings of the 3rd ICGA, pp. 422427. Morgan Kaufmann, San Francisco (1989)</text:p><text:p text:style-name="P8">[19] Munetomo, M., Takai, Y., Sato, Y.: An efficient migration scheme for subpopulation-based asynchronously parallel GAs. Technical Report HIER-IS- 9301, Hokkaido University (1993)</text:p><text:p text:style-name="P8">[20] Voigt, H.M., Santibanez-Koref, I., Born, J.: Hierarchically structured distributed genetic algorithms. In: Manner, R., Manderick, B. (eds.) Proceedings of the International Conference Parallel Problem Solving from Nature, vol. 2, pp. 155164. North-Holland, Amsterdam (1992)</text:p><text:p text:style-name="P8">[21] Belding, T.C.: The distributed genetic algorithm revisited. In: Eshelman, L.J. (ed.) Proceedings of the 6th International Conference on GAs, pp. 122129. Morgan Kaufmann, San Francisco (1995)</text:p><text:p text:style-name="P8"><text:soft-page-break/>[22] Mejia-Olvera, M., Cantu-Paz, E.: DGENESIS-software for the execution of distributed genetic algorithms. In: Proceedings of the XX Conferencia Latinoamericana de Informatica, pp. 935946. Monterrey, Mexico (1994)</text:p><text:p text:style-name="P8">[23] Baker, J.E.: Reducing bias and inefficiency in the selection algorithm. In: Grefenstette, J.J. (ed.) Proceedings of the Second International Conference on Genetic Algorithms, pp. 1421. Lawrence Erlbaum Associates Publishers, Hillsdale (1987)</text:p><text:p text:style-name="P8">[24] Lim, T.Y.: Structured population genetic algorithms: a literature survey. Artif. Intell. Rev. 41(3), 385399 (2014)</text:p><text:p text:style-name="P8"><text:span text:style-name="T12">[</text:span>25<text:span text:style-name="T12">]</text:span> Keremedchiev, D., Barova, M., Tomov, P., Mobile application as distributed computing system for artificial neural networks training used in perfect information games, Proceedings of 16th International scientific conference UNITECH16, Gabrovo, vol. 2, pp.389-393 (2016)</text:p><text:p text:style-name="P8"><text:span text:style-name="T12">[</text:span>26<text:span text:style-name="T12">]</text:span> Tomov, P., Monov, V., Artificial Neural Networks and Differential Evolution Used for Time Series Forecasting in Distributed Environment, Proceedings of International conference AUTOMATICS AND INFORMATICS, ISSN 1313-1850, pp. 129-132, Sofia (2016)</text:p><text:p text:style-name="P8"><text:span text:style-name="T12">[</text:span>27<text:span text:style-name="T12">]</text:span> Zankinski, I., Stoilov, T., Effect of the Neuron Permutation Problem on Training Artificial Neural Networks with Genetic Algorithms in Distributed Computing, Proceedings of XXIV International Symposium Management of Energy, Industrial and Environmental Systems, ISSN 1313-2237, Bankya, pp. 53-55 (2016)</text:p><text:p text:style-name="P8"><text:soft-page-break/>[28] Karlik, B., Vehbi, A., Performance Analysis of Various Activation Functions in Generalized MLP Architectures of Neural Networks. International Journal of Artificial Intelligence And Expert Systems (IJAE), vol. 1, no. 4, pp. 111-122 (2011)</text:p><text:p text:style-name="P8"/><text:p text:style-name="P8"/><text:p text:style-name="P8"/><text:p text:style-name="P8"/><text:p text:style-name="P8"/><text:table-of-content text:style-name="Sect1" text:name="Table of Contents1"><text:table-of-content-source text:outline-level="10"><text:index-title-template text:style-name="Contents_20_Heading">Table of Contents</text:index-title-template><text:table-of-content-entry-template text:outline-level="1" text:style-name="Contents_20_1"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="2" text:style-name="Contents_20_2"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="3" text:style-name="Contents_20_3"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="4" text:style-name="Contents_20_4"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="5" text:style-name="Contents_20_5"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="6" text:style-name="Contents_20_6"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="7" text:style-name="Contents_20_7"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="8" text:style-name="Contents_20_8"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="9" text:style-name="Contents_20_9"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="10" text:style-name="Contents_20_10"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template></text:table-of-content-source><text:index-body><text:index-title text:style-name="Sect1" text:name="Table of Contents1_Head"><text:p text:style-name="P26">Table of Contents</text:p></text:index-title><text:p text:style-name="P27"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc552_3689700921" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">1 Time Series Prediction by Artificial Neural Networks and Differential Evolution in Distributed Environment<text:tab/>1</text:a></text:p><text:p text:style-name="P28"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc554_3689700921" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">1.1 Introduction<text:tab/>1</text:a></text:p><text:p text:style-name="P28"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc556_3689700921" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">1.2 Problem Definition<text:tab/>4</text:a></text:p><text:p text:style-name="P28"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc472_3627904809" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">1.3 Machine Learning Tools<text:tab/>4</text:a></text:p><text:p text:style-name="P28"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc474_3627904809" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">1.4 Forecasting Organization<text:tab/>7</text:a></text:p><text:p text:style-name="P27"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc351_966847004" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">2 Distributed Evolutionary Computing Migration Strategy by Accidental Node Involvement<text:tab/>16</text:a></text:p><text:p text:style-name="P28"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc979_966847004" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">2.1 Introduction<text:tab/>17</text:a></text:p><text:p text:style-name="P28"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc772_951591551" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">2.2 Accidental Node Involvement<text:tab/>19</text:a></text:p><text:p text:style-name="P28"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc632_1783862520" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">2.3 Distribution Parameterization<text:tab/>22</text:a></text:p><text:p text:style-name="P27"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc634_1783862520" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">3 Modifications of Artificial Neural Networks<text:tab/>27</text:a></text:p><text:p text:style-name="P28"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc718_1268615346" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">3.1 Alternative Activation Function Derivative<text:tab/>27</text:a></text:p><text:p text:style-name="P28"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc1666_1268615346" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">3.2 Long Short Term Memory in Multilayer Perceptron Pair<text:tab/>35</text:a></text:p><text:p text:style-name="P28"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc1317_2021269790" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">3.3 Self-Growing Multilayer Perceptron for Time Series Forecasting<text:tab/>38</text:a></text:p></text:index-body></text:table-of-content><text:p text:style-name="P4"/></office:text></office:body></office:document-content>