<?xml version="1.0" encoding="UTF-8"?>
<office:document-content xmlns:office="urn:oasis:names:tc:opendocument:xmlns:office:1.0" xmlns:ooo="http://openoffice.org/2004/office" xmlns:fo="urn:oasis:names:tc:opendocument:xmlns:xsl-fo-compatible:1.0" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:meta="urn:oasis:names:tc:opendocument:xmlns:meta:1.0" xmlns:style="urn:oasis:names:tc:opendocument:xmlns:style:1.0" xmlns:text="urn:oasis:names:tc:opendocument:xmlns:text:1.0" xmlns:rpt="http://openoffice.org/2005/report" xmlns:draw="urn:oasis:names:tc:opendocument:xmlns:drawing:1.0" xmlns:dr3d="urn:oasis:names:tc:opendocument:xmlns:dr3d:1.0" xmlns:svg="urn:oasis:names:tc:opendocument:xmlns:svg-compatible:1.0" xmlns:chart="urn:oasis:names:tc:opendocument:xmlns:chart:1.0" xmlns:table="urn:oasis:names:tc:opendocument:xmlns:table:1.0" xmlns:number="urn:oasis:names:tc:opendocument:xmlns:datastyle:1.0" xmlns:ooow="http://openoffice.org/2004/writer" xmlns:oooc="http://openoffice.org/2004/calc" xmlns:of="urn:oasis:names:tc:opendocument:xmlns:of:1.2" xmlns:xforms="http://www.w3.org/2002/xforms" xmlns:tableooo="http://openoffice.org/2009/table" xmlns:calcext="urn:org:documentfoundation:names:experimental:calc:xmlns:calcext:1.0" xmlns:drawooo="http://openoffice.org/2010/draw" xmlns:xhtml="http://www.w3.org/1999/xhtml" xmlns:loext="urn:org:documentfoundation:names:experimental:office:xmlns:loext:1.0" xmlns:field="urn:openoffice:names:experimental:ooo-ms-interop:xmlns:field:1.0" xmlns:math="http://www.w3.org/1998/Math/MathML" xmlns:form="urn:oasis:names:tc:opendocument:xmlns:form:1.0" xmlns:script="urn:oasis:names:tc:opendocument:xmlns:script:1.0" xmlns:formx="urn:openoffice:names:experimental:ooxml-odf-interop:xmlns:form:1.0" xmlns:dom="http://www.w3.org/2001/xml-events" xmlns:xsd="http://www.w3.org/2001/XMLSchema" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:grddl="http://www.w3.org/2003/g/data-view#" xmlns:css3t="http://www.w3.org/TR/css3-text/" xmlns:officeooo="http://openoffice.org/2009/office" office:version="1.3"><office:scripts/><office:font-face-decls><style:font-face style:name="Arial Unicode MS" svg:font-family="&apos;Arial Unicode MS&apos;" style:font-family-generic="swiss"/><style:font-face style:name="Arial Unicode MS1" svg:font-family="&apos;Arial Unicode MS&apos;" style:font-family-generic="system" style:font-pitch="variable"/><style:font-face style:name="Calibri" svg:font-family="Calibri" style:font-adornments="Regular" style:font-family-generic="swiss" style:font-pitch="variable"/><style:font-face style:name="Liberation Sans" svg:font-family="&apos;Liberation Sans&apos;" style:font-family-generic="swiss" style:font-pitch="variable"/><style:font-face style:name="Liberation Serif" svg:font-family="&apos;Liberation Serif&apos;" style:font-family-generic="roman" style:font-pitch="variable"/><style:font-face style:name="PingFang SC" svg:font-family="&apos;PingFang SC&apos;" style:font-family-generic="system" style:font-pitch="variable"/><style:font-face style:name="Songti SC" svg:font-family="&apos;Songti SC&apos;" style:font-family-generic="system" style:font-pitch="variable"/></office:font-face-decls><office:automatic-styles><style:style style:name="P1" style:family="paragraph" style:parent-style-name="Header"><loext:graphic-properties draw:fill-gradient-name="gradient" draw:fill-hatch-name="hatch"/><style:paragraph-properties fo:line-height="100%" fo:text-align="end" style:justify-single-word="false"/></style:style><style:style style:name="P2" style:family="paragraph" style:parent-style-name="Footer"><style:paragraph-properties fo:text-align="end" style:justify-single-word="false"/><style:text-properties fo:language="en" fo:country="US"/></style:style><style:style style:name="P3" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US"/></style:style><style:style style:name="P4" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00358a2a"/></style:style><style:style style:name="P5" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="003682c7"/></style:style><style:style style:name="P6" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="00415f3c" officeooo:paragraph-rsid="00415f3c"/></style:style><style:style style:name="P7" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="00460fe0" officeooo:paragraph-rsid="00415f3c"/></style:style><style:style style:name="P8" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="004c56b5"/></style:style><style:style style:name="P9" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00560699"/></style:style><style:style style:name="P10" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="0056f98d"/></style:style><style:style style:name="P11" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="005815c8" officeooo:paragraph-rsid="005815c8"/></style:style><style:style style:name="P12" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00682738"/></style:style><style:style style:name="P13" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="006c7d80"/></style:style><style:style style:name="P14" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00748eff"/></style:style><style:style style:name="P15" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="007c82d7"/></style:style><style:style style:name="P16" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="0080c64c"/></style:style><style:style style:name="P17" style:family="paragraph" style:parent-style-name="Bibliography_20_Heading"><style:paragraph-properties fo:break-before="page"/><style:text-properties fo:language="en" fo:country="US"/></style:style><style:style style:name="P18" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="0099263e"/></style:style><style:style style:name="P19" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00c4e0a9"/></style:style><style:style style:name="P20" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00cae438"/></style:style><style:style style:name="P21" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00d49860"/></style:style><style:style style:name="P22" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00d78e37"/></style:style><style:style style:name="P23" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00ea19be"/></style:style><style:style style:name="P24" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00edde6d"/></style:style><style:style style:name="P25" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00f9b976"/></style:style><style:style style:name="P26" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00fe3b76"/></style:style><style:style style:name="P27" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="01167a7d"/></style:style><style:style style:name="P28" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="0116d99f"/></style:style><style:style style:name="P29" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="01175038"/></style:style><style:style style:name="P30" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="012aa61e"/></style:style><style:style style:name="P31" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="012ce3ba"/></style:style><style:style style:name="P32" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="0136f81f"/></style:style><style:style style:name="P33" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="0138eb2b"/></style:style><style:style style:name="P34" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="015abe49"/></style:style><style:style style:name="P35" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="015b7635"/></style:style><style:style style:name="P36" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="0160d280"/></style:style><style:style style:name="P37" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="01653046"/></style:style><style:style style:name="P38" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="016814b9"/></style:style><style:style style:name="P39" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="016c08e8"/></style:style><style:style style:name="P40" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="016eae89"/></style:style><style:style style:name="P41" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="01770657"/></style:style><style:style style:name="P42" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="017859be"/></style:style><style:style style:name="P43" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="017ecdd3"/></style:style><style:style style:name="P44" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="01815991"/></style:style><style:style style:name="P45" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="0182e956"/></style:style><style:style style:name="P46" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="01877feb"/></style:style><style:style style:name="P47" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="018d5398"/></style:style><style:style style:name="P48" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="01910a95"/></style:style><style:style style:name="P49" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="0198aecd"/></style:style><style:style style:name="P50" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="019bc2f4"/></style:style><style:style style:name="P51" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013aa37e" officeooo:paragraph-rsid="013aa37e"/></style:style><style:style style:name="P52" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013aa37e" officeooo:paragraph-rsid="01a5eedd"/></style:style><style:style style:name="P53" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013aa37e" officeooo:paragraph-rsid="01ac9ab9"/></style:style><style:style style:name="P54" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013aa37e" officeooo:paragraph-rsid="01b026af"/></style:style><style:style style:name="P55" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013aa37e" officeooo:paragraph-rsid="01bce2b4"/></style:style><style:style style:name="P56" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013aa37e" officeooo:paragraph-rsid="01bf1da2"/></style:style><style:style style:name="P57" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013acf53" officeooo:paragraph-rsid="013acf53"/></style:style><style:style style:name="P58" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013acf53" officeooo:paragraph-rsid="01c8fc2d"/></style:style><style:style style:name="P59" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013acf53" officeooo:paragraph-rsid="01d0f7a7"/></style:style><style:style style:name="P60" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013acf53" officeooo:paragraph-rsid="01d8c538"/></style:style><style:style style:name="P61" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013acf53" officeooo:paragraph-rsid="01dd4369"/></style:style><style:style style:name="P62" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013acf53" officeooo:paragraph-rsid="01de2caa"/></style:style><style:style style:name="P63" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013acf53" officeooo:paragraph-rsid="01e40f6b"/></style:style><style:style style:name="P64" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013acf53" officeooo:paragraph-rsid="01e63662"/></style:style><style:style style:name="P65" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013acf53" officeooo:paragraph-rsid="01e97979"/></style:style><style:style style:name="P66" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013acf53" officeooo:paragraph-rsid="01f31542"/></style:style><style:style style:name="P67" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013be91f" officeooo:paragraph-rsid="013be91f"/></style:style><style:style style:name="P68" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013be91f" officeooo:paragraph-rsid="021afc44"/></style:style><style:style style:name="P69" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013be91f" officeooo:paragraph-rsid="022068f9"/></style:style><style:style style:name="P70" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013be91f" officeooo:paragraph-rsid="0221011e"/></style:style><style:style style:name="P71" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013be91f" officeooo:paragraph-rsid="02225a0f"/></style:style><style:style style:name="P72" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013be91f" officeooo:paragraph-rsid="02249119"/></style:style><style:style style:name="P73" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013dae88" officeooo:paragraph-rsid="013dae88"/></style:style><style:style style:name="P74" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013dae88" officeooo:paragraph-rsid="023bc001"/></style:style><style:style style:name="P75" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013dae88" officeooo:paragraph-rsid="024367a7"/></style:style><style:style style:name="P76" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013dae88" officeooo:paragraph-rsid="024612f1"/></style:style><style:style style:name="P77" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013dae88" officeooo:paragraph-rsid="0246316b"/></style:style><style:style style:name="P78" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013dae88" officeooo:paragraph-rsid="02592c48"/></style:style><style:style style:name="P79" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013dae88" officeooo:paragraph-rsid="0261efab"/></style:style><style:style style:name="P80" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013dae88" officeooo:paragraph-rsid="0265a707"/></style:style><style:style style:name="P81" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="01493454"/></style:style><style:style style:name="P82" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="014e86f1"/></style:style><style:style style:name="P83" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="01509d9a"/></style:style><style:style style:name="P84" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="01f9003f"/></style:style><style:style style:name="P85" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="02012a0c"/></style:style><style:style style:name="P86" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="0204acb6"/></style:style><style:style style:name="P87" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="02061b40"/></style:style><style:style style:name="P88" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="020644ee"/></style:style><style:style style:name="P89" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="021648e6" officeooo:paragraph-rsid="021648e6"/></style:style><style:style style:name="P90" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="02284f24"/></style:style><style:style style:name="P91" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="022ace96"/></style:style><style:style style:name="P92" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="022dc32a"/></style:style><style:style style:name="P93" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="025dc49f" officeooo:paragraph-rsid="0261efab"/></style:style><style:style style:name="P94" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="025eca87" officeooo:paragraph-rsid="0261efab"/></style:style><style:style style:name="P95" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="026d7ed3"/></style:style><style:style style:name="P96" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="0277c589"/></style:style><style:style style:name="P97" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="027b81f7"/></style:style><style:style style:name="P98" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="027e06e1"/></style:style><style:style style:name="P99" style:family="paragraph" style:parent-style-name="Contents_20_1"><style:paragraph-properties><style:tab-stops><style:tab-stop style:position="5.2126in" style:type="right" style:leader-style="dotted" style:leader-text="."/></style:tab-stops></style:paragraph-properties></style:style><style:style style:name="P100" style:family="paragraph" style:parent-style-name="Contents_20_2"><style:paragraph-properties><style:tab-stops><style:tab-stop style:position="5.0161in" style:type="right" style:leader-style="dotted" style:leader-text="."/></style:tab-stops></style:paragraph-properties></style:style><style:style style:name="P101" style:family="paragraph" style:parent-style-name="Contents_20_1"><style:paragraph-properties><style:tab-stops><style:tab-stop style:position="5.2126in" style:type="right" style:leader-style="dotted" style:leader-text="."/></style:tab-stops></style:paragraph-properties></style:style><style:style style:name="P102" style:family="paragraph" style:parent-style-name="Contents_20_2"><style:paragraph-properties><style:tab-stops><style:tab-stop style:position="5.0161in" style:type="right" style:leader-style="dotted" style:leader-text="."/></style:tab-stops></style:paragraph-properties></style:style><style:style style:name="P103" style:family="paragraph" style:parent-style-name="Contents_20_Heading"><style:paragraph-properties fo:break-before="page"/><style:text-properties fo:language="en" fo:country="US"/></style:style><style:style style:name="P104" style:family="paragraph" style:parent-style-name="Contents_20_Heading"><style:paragraph-properties fo:break-before="page"/></style:style><style:style style:name="P105" style:family="paragraph" style:parent-style-name="Header"><loext:graphic-properties draw:fill-gradient-name="gradient" draw:fill-hatch-name="hatch"/><style:paragraph-properties fo:line-height="100%"/><style:text-properties officeooo:rsid="0022a018" officeooo:paragraph-rsid="00242fc2"/></style:style><style:style style:name="P106" style:family="paragraph" style:parent-style-name="Heading_20_1"><style:text-properties fo:language="en" fo:country="US"/></style:style><style:style style:name="P107" style:family="paragraph" style:parent-style-name="Heading_20_1"><style:paragraph-properties fo:break-before="page"/><style:text-properties fo:language="en" fo:country="US"/></style:style><style:style style:name="P108" style:family="paragraph" style:parent-style-name="Heading_20_2"><style:text-properties fo:language="en" fo:country="US"/></style:style><style:style style:name="P109" style:family="paragraph" style:parent-style-name="Heading_20_2"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="02819591"/></style:style><style:style style:name="P110" style:family="paragraph" style:parent-style-name="Heading_20_2"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="02844867"/></style:style><style:style style:name="P111" style:family="paragraph" style:parent-style-name="Heading_20_2"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="0289281e"/></style:style><style:style style:name="P112" style:family="paragraph" style:parent-style-name="Heading_20_2"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="028e49d5"/></style:style><style:style style:name="P113" style:family="paragraph" style:parent-style-name="Heading_20_2"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="02901c32"/></style:style><style:style style:name="P114" style:family="paragraph" style:parent-style-name="Heading_20_2"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="0290d6d6"/></style:style><style:style style:name="P115" style:family="paragraph" style:parent-style-name="Heading_20_2"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="029310aa"/></style:style><style:style style:name="P116" style:family="paragraph" style:parent-style-name="Heading_20_2"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="0293b0c3"/></style:style><style:style style:name="P117" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US"/></style:style><style:style style:name="P118" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="02805736"/></style:style><style:style style:name="P119" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="02819591"/></style:style><style:style style:name="P120" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="02826148"/></style:style><style:style style:name="P121" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="02844867"/></style:style><style:style style:name="P122" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="0286729a"/></style:style><style:style style:name="P123" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="02874acf" officeooo:paragraph-rsid="02874acf"/></style:style><style:style style:name="P124" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="02874acf" officeooo:paragraph-rsid="029618b5"/></style:style><style:style style:name="P125" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="0289281e"/></style:style><style:style style:name="P126" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="028b7c8d"/></style:style><style:style style:name="P127" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="028e49d5"/></style:style><style:style style:name="P128" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="028f7454"/></style:style><style:style style:name="P129" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="02901c32"/></style:style><style:style style:name="P130" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="029066e4"/></style:style><style:style style:name="P131" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="0290d6d6"/></style:style><style:style style:name="P132" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="029179e5"/></style:style><style:style style:name="P133" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="029310aa"/></style:style><style:style style:name="P134" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="029329bd"/></style:style><style:style style:name="P135" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="0293b0c3"/></style:style><style:style style:name="P136" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="0293e73a"/></style:style><style:style style:name="T1" style:family="text"><style:text-properties officeooo:rsid="002867a0"/></style:style><style:style style:name="T2" style:family="text"><style:text-properties officeooo:rsid="003941f7"/></style:style><style:style style:name="T3" style:family="text"><style:text-properties fo:language="en" fo:country="US"/></style:style><style:style style:name="T4" style:family="text"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="006e0a29"/></style:style><style:style style:name="T5" style:family="text"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="00e6305c"/></style:style><style:style style:name="T6" style:family="text"><style:text-properties officeooo:rsid="003e1eeb"/></style:style><style:style style:name="T7" style:family="text"><style:text-properties officeooo:rsid="0042e914"/></style:style><style:style style:name="T8" style:family="text"><style:text-properties officeooo:rsid="004c56b5"/></style:style><style:style style:name="T9" style:family="text"><style:text-properties officeooo:rsid="004e218b"/></style:style><style:style style:name="T10" style:family="text"><style:text-properties officeooo:rsid="005fa15f"/></style:style><style:style style:name="T11" style:family="text"><style:text-properties officeooo:rsid="00ac6e4e"/></style:style><style:style style:name="T12" style:family="text"><style:text-properties officeooo:rsid="00be8bdc"/></style:style><style:style style:name="T13" style:family="text"><style:text-properties officeooo:rsid="00c4873f"/></style:style><style:style style:name="T14" style:family="text"><style:text-properties officeooo:rsid="00dc3064"/></style:style><style:style style:name="T15" style:family="text"><style:text-properties officeooo:rsid="00de182b"/></style:style><style:style style:name="T16" style:family="text"><style:text-properties officeooo:rsid="00dea8bf"/></style:style><style:style style:name="T17" style:family="text"><style:text-properties officeooo:rsid="00f2a227"/></style:style><style:style style:name="T18" style:family="text"><style:text-properties officeooo:rsid="01a2d07a"/></style:style><style:style style:name="T19" style:family="text"><style:text-properties officeooo:rsid="01c53a48"/></style:style><style:style style:name="T20" style:family="text"><style:text-properties officeooo:rsid="01f2e180"/></style:style><style:style style:name="T21" style:family="text"><style:text-properties officeooo:rsid="01f362e9"/></style:style><style:style style:name="T22" style:family="text"><style:text-properties officeooo:rsid="01f3a5ad"/></style:style><style:style style:name="T23" style:family="text"><style:text-properties officeooo:rsid="01fa0a06"/></style:style><style:style style:name="T24" style:family="text"><style:text-properties officeooo:rsid="02061b40"/></style:style><style:style style:name="T25" style:family="text"><style:text-properties fo:font-size="10pt" fo:font-style="italic" style:font-style-asian="italic" style:font-style-complex="italic"/></style:style><style:style style:name="T26" style:family="text"><style:text-properties fo:font-size="10pt" fo:font-style="italic" style:font-size-asian="10pt" style:font-style-asian="italic" style:font-size-complex="10pt" style:font-style-complex="italic"/></style:style><style:style style:name="T27" style:family="text"><style:text-properties fo:font-style="italic" style:font-style-asian="italic" style:font-style-complex="italic"/></style:style><style:style style:name="T28" style:family="text"><style:text-properties officeooo:rsid="02136c0c"/></style:style><style:style style:name="T29" style:family="text"><style:text-properties officeooo:rsid="021758c2"/></style:style><style:style style:name="T30" style:family="text"><style:text-properties officeooo:rsid="0227eaad"/></style:style><style:style style:name="T31" style:family="text"><style:text-properties style:text-position="sub 58%" fo:font-style="italic" style:font-style-asian="italic" style:font-style-complex="italic"/></style:style><style:style style:name="T32" style:family="text"><style:text-properties style:text-position="sub 58%" fo:font-size="10pt" fo:font-style="italic" style:font-size-asian="10pt" style:font-style-asian="italic" style:font-size-complex="10pt" style:font-style-complex="italic"/></style:style><style:style style:name="T33" style:family="text"><style:text-properties officeooo:rsid="0232bfac"/></style:style><style:style style:name="T34" style:family="text"><style:text-properties officeooo:rsid="0239fc2b"/></style:style><style:style style:name="T35" style:family="text"><style:text-properties officeooo:rsid="023e47ac"/></style:style><style:style style:name="T36" style:family="text"><style:text-properties officeooo:rsid="0251981b"/></style:style><style:style style:name="T37" style:family="text"><style:text-properties officeooo:rsid="026aefe0"/></style:style><style:style style:name="T38" style:family="text"><style:text-properties officeooo:rsid="026b903c"/></style:style><style:style style:name="T39" style:family="text"><style:text-properties officeooo:rsid="023eef5b"/></style:style><style:style style:name="T40" style:family="text"><style:text-properties officeooo:rsid="02365360"/></style:style><style:style style:name="T41" style:family="text"><style:text-properties officeooo:rsid="02848c9b"/></style:style><style:style style:name="fr1" style:family="graphic" style:parent-style-name="Frame"><style:graphic-properties fo:margin-left="0in" fo:margin-right="0in" fo:margin-top="0in" fo:margin-bottom="0in" style:wrap="dynamic" style:number-wrapped-paragraphs="no-limit" style:vertical-pos="top" style:vertical-rel="baseline" style:horizontal-pos="center" style:horizontal-rel="paragraph" fo:padding="0in" fo:border="none"/></style:style><style:style style:name="fr2" style:family="graphic" style:parent-style-name="Frame"><style:graphic-properties fo:margin-left="0in" fo:margin-right="0in" fo:margin-top="0in" fo:margin-bottom="0in" style:wrap="dynamic" style:number-wrapped-paragraphs="no-limit" style:vertical-pos="top" style:vertical-rel="baseline" style:horizontal-pos="center" style:horizontal-rel="paragraph" fo:padding="0in" fo:border="none" loext:rel-width-rel="paragraph"/></style:style><style:style style:name="fr3" style:family="graphic" style:parent-style-name="Frame"><style:graphic-properties fo:margin-left="0in" fo:margin-right="0in" fo:margin-top="0in" fo:margin-bottom="0in" style:wrap="dynamic" style:number-wrapped-paragraphs="no-limit" style:vertical-pos="middle" style:vertical-rel="baseline" style:horizontal-pos="from-left" style:horizontal-rel="paragraph" fo:padding="0in" fo:border="none" loext:rel-width-rel="paragraph"/></style:style><style:style style:name="fr4" style:family="graphic" style:parent-style-name="Graphics"><style:graphic-properties fo:margin-left="0in" fo:margin-right="0in" fo:margin-top="0in" fo:margin-bottom="0in" style:run-through="foreground" style:wrap="none" style:vertical-pos="top" style:vertical-rel="paragraph-content" style:horizontal-pos="center" style:horizontal-rel="paragraph-content" fo:padding="0in" fo:border="none" style:shadow="none" draw:shadow-opacity="100%" style:mirror="none" fo:clip="rect(0in, 0in, 0in, 0in)" draw:luminance="0%" draw:contrast="0%" draw:red="0%" draw:green="0%" draw:blue="0%" draw:gamma="100%" draw:color-inversion="false" draw:image-opacity="100%" draw:color-mode="standard" loext:rel-width-rel="paragraph"/></style:style><style:style style:name="fr5" style:family="graphic" style:parent-style-name="Graphics"><style:graphic-properties fo:margin-left="0in" fo:margin-right="0in" fo:margin-top="0in" fo:margin-bottom="0in" style:run-through="foreground" style:wrap="none" style:vertical-pos="top" style:vertical-rel="baseline" style:horizontal-pos="center" style:horizontal-rel="paragraph-content" fo:padding="0in" fo:border="none" style:shadow="none" draw:shadow-opacity="100%" style:mirror="none" fo:clip="rect(0in, 0in, 0in, 0in)" draw:luminance="0%" draw:contrast="0%" draw:red="0%" draw:green="0%" draw:blue="0%" draw:gamma="100%" draw:color-inversion="false" draw:image-opacity="100%" draw:color-mode="standard" loext:rel-width-rel="paragraph"/></style:style><style:style style:name="Sect1" style:family="section"><style:section-properties style:editable="false"><style:columns fo:column-count="1" fo:column-gap="0in"/></style:section-properties></style:style></office:automatic-styles><office:body><office:text text:use-soft-page-breaks="true"><text:sequence-decls><text:sequence-decl text:display-outline-level="0" text:name="Illustration"/><text:sequence-decl text:display-outline-level="0" text:name="Table"/><text:sequence-decl text:display-outline-level="0" text:name="Text"/><text:sequence-decl text:display-outline-level="0" text:name="Drawing"/><text:sequence-decl text:display-outline-level="0" text:name="Figure"/></text:sequence-decls><text:h text:style-name="P106" text:outline-level="1"><text:bookmark-start text:name="__RefHeading___Toc552_3689700921"/><text:span text:style-name="T8">1 </text:span>Time Series Prediction by Artificial Neural Networks and Differential Evolution in Distributed Environment<text:bookmark-end text:name="__RefHeading___Toc552_3689700921"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P3">Models for time series prediction using Artificial Neural Networks trained with Differential Evolution in a distributed computational environment have become popular in recent decades. Time series prediction is a complex task that demands the development of more effective and faster algorithms. Artificial Neural Networks are used as a base and trained with historical data. One of the main problems is how to select an accurate Artificial Neural Network training algorithm. There are two general approaches - exact numeric and heuristic optimization methods. When a suitable heuristic is applied, training can be done in a distributed computational environment. In this case, there is a much faster and more realistic output, which helps to achieve better predictions.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc554_3689700921"/><text:span text:style-name="T9">1.1 </text:span>Introduction<text:bookmark-end text:name="__RefHeading___Toc554_3689700921"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P5">Time series prediction is the process of forecasting future values in a series of data based on their known values in the past. Decision-makers <text:soft-page-break/>hold a significant responsibility in shaping an effective investment strategy. Investing involves taking acceptable risks with the expectation of a certain profit. The critical aspect of investment lies in balancing the risks taken with the anticipated profits. The primary trading activity revolves around exchanging currencies in the currency market, commonly known as the foreign exchange market or FOREX. Currencies are highly volatile and subject to frequent price changes. When engaging in FOREX trading, decision-makers must make three crucial decisions:</text:p><text:p text:style-name="P5">1. Predict whether the price will rise or fall;</text:p><text:p text:style-name="P5">2. Determine the appropriate volume to buy or sell;</text:p><text:p text:style-name="P5">3. Decide how long to maintain the open position.</text:p><text:p text:style-name="P5"/><text:p text:style-name="P5">Although these decisions may seem straightforward, accurately estimating the price-changing direction is challenging due to many influencing factors. The order volume is directly tied to the level of risk assumed. While high-volume orders can lead to substantial profits when the price-changing direction is well-estimated, they can also result in significant losses. The duration of holding an open position is crucial for maximizing profits or minimizing losses. Financial forecasting is paramount for traders in the currency market, mainly due to the market&apos;s high price dynamics.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P4">The development of effective and reliable financial predictions is a demanding and complex task. Therefore, this field is highly relevant for <text:soft-page-break/>developing and utilizing self-organizing and self-learning systems for prediction. The application of Artificial Neural Networks for predicting time series in the field of the economy has been considered by various authors, such as <text:span text:style-name="T37">(</text:span>Dunis [1], Giles [2], Moody [3]<text:span text:style-name="T37">)</text:span>.</text:p><text:p text:style-name="P4"/><text:p text:style-name="P4">A widely-used method employs so-called Feed Forward Neural Networks (Haykin [4]). While Feed Forward Neural Networks types of networks are very effective, they suffer from a fundamental flaw: they lack short-term memory. This problem could be avoided by using Recurrent Neural Networks. However, Recurrent Neural Networks, on the other hand, present difficulties due to their challenge in employing precise gradient methods during the learning phase (Werbos [5]). A possible solution is a combined approach for training Artificial Neural Networks using evolutionary algorithms, as <text:span text:style-name="T12">(</text:span>Yao [6]<text:span text:style-name="T12">)</text:span> and several other authors proposed.</text:p><text:p text:style-name="P4"/><text:p text:style-name="P4">Evolutionary algorithms show significantly better results for optimum search in complex multidimensional spaces with many local optima, in which case gradient-based methods will likely get stuck (Holland [7]). In this paper, we present a model of a self-learning system for predicting time series based on ANN and DE in a distributed environment.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc556_3689700921"/><text:soft-page-break/><text:span text:style-name="T9">1.2 </text:span>Problem <text:span text:style-name="T2">D</text:span>e<text:span text:style-name="T2">fi</text:span>nition<text:bookmark-end text:name="__RefHeading___Toc556_3689700921"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P8">Several specific aspects come into play in the realm of time series prediction. Firstly, a series of values is tightly intertwined with the temporal axis, forming time-value pairs. Values within a time series exhibit interdependence; they are not isolated entities. Instead, these values are intricately linked with newer values stemming from preceding ones. For instance, consider the exchange rate between the EUR and USD. It is exceedingly uncommon for this rate to undergo dramatic shifts over two consecutive trading days. Instead, the rate displays a gradual and continuous evolution. This characteristic rate serves as a depiction of the trading dynamics between Europe and the USA.</text:p><text:p text:style-name="P8"/><text:p text:style-name="P8">The challenge in predicting financial time series lies in leveraging historical price values to construct a prediction model, subsequently employing this model to forecast future price values. If the model proves accurate, decision-makers can harness the projected values to mitigate investment risks.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc472_3627904809"/>1.3 <text:span text:style-name="T10">Machine Learning</text:span> Tools<text:bookmark-end text:name="__RefHeading___Toc472_3627904809"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P9"><text:soft-page-break/>Artificial Neural Networks are mathematical models inspired by natural neural networks. They consist of artificial neurons connected by a series of links. Information is input into the network and then propagated through internal layers (neurons) to produce an output. ANNs are designed to be self-adaptive systems capable of altering their internal structure based on external or internal information encountered during the learning process. They represent intricate relationships between inputs and outputs (functional relations) or identify patterns within a dataset (data mining).</text:p><text:p text:style-name="P9"/><text:p text:style-name="P9">The concept of ANNs draws inspiration from biological central nervous systems and their components, such as neurons, axons, dendrites, and synapses. An ANN comprises a network of uncomplicated processing elements that collectively manifest complex global behaviors via interconnections between processing units and their parameters. In practical terms, utilizing ANNs involves employing algorithms to adjust connections&apos; strength (weights), thereby achieving the desired signal flow.</text:p><text:p text:style-name="P9"/><text:p text:style-name="P9">ANNs resemble natural neural networks in that the processing elements execute network operations collectively and simultaneously. In modern computing, the ANN approach is often combined with non-adaptive techniques to yield improved practical outcomes. The primary advantage of employing ANNs is their capacity to deduce a function from observed <text:soft-page-break/>examples. This proves particularly advantageous when tackling problems where the intricacy of the studied process (or its accompanying data) renders manual function derivation exceedingly challenging.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P10">Differential Evolution is a population-based meta-heuristic optimization method rooted in evolution. DE was developed by <text:span text:style-name="T38">(</text:span>Kenneth Price and Rainer Storn [8]<text:span text:style-name="T38">)</text:span> and drew inspiration from classical genetic algorithms (GAs). However, meta-heuristics like DE do not guarantee the discovery of an optimal solution. DE finds application in exploring complex, high-dimensional search spaces. While it finds primary use in continuous problems involving real numbers, its applicability to discrete problems is limited. Notably, DE does not mandate differentiability, a requirement in classical optimization techniques such as gradient descent.</text:p><text:p text:style-name="P10"/><text:p text:style-name="P10">Much like classical GAs, DE employs a population of potential solutions. It generates fresh candidates by melding existing ones using crossover, mutation, and selection rules. The best candidates are retained based on objective function evaluation, obviating the need for gradient usage.</text:p><text:p text:style-name="P10"/><text:p text:style-name="P10">A key distinction arises in the mutation operator in comparing DE with GA. DE&apos;s mutation relies on the calculation of difference vectors, rendering it considerably more potent than GA&apos;s mutation approach, which adjusts individual values within the mutated chromosome. One drawback of the <text:soft-page-break/>difference vector approach lies in its difficulty in effecting minor changes to discrete values.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc474_3627904809"/>1.4 <text:span text:style-name="T10">Forecasting </text:span>Organization<text:bookmark-end text:name="__RefHeading___Toc474_3627904809"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P3">The forecasting model is built upon a standard Artificial Neural Network and employs a combined training approach involving Differential Evolution and back-propagation. The ANN&apos;s topology is a research subject and is parameterized on a remote server. Neuronal connections can take three forms: strictly forward, forward-backward, and fully connected (establishing connections with all other neurons, including themselves). A linear summation function and a sigmoid activation function characterize each neuron.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame1" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="0"><draw:text-box fo:min-height="2.6201in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image1" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="2.6201in" style:rel-height="scale" draw:z-index="1"><draw:image xlink:href="Pictures/1000000000000A1E00000549EAF7DA525E2C6E7F.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure0" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">1</text:sequence>: Computers organization</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P3"/><text:p text:style-name="P12">In Differential Evolution, a variant of genetic algorithms, each chromosome represents a set of weights for a specific topology of an Artificial Neural Network. As depicted in Figure 1, the overall DE population is showcased on a remote server. Individual computational units (client machines) load subsets of the global DE population (as illustrated in step 1 of Figure 3) and engage in localized DE-BP mixed ANN training (depicted in step 2 of Figure 3). At regular intervals, these local machines establish connections with the remote server. During these connections, the local populations are updated, and the outcomes of local computations are reported (as shown in step 3 of Figure 3).</text:p><text:p text:style-name="P12"/><text:p text:style-name="P12"><text:soft-page-break/>Due to DE&apos;s remarkable parallelism, no theoretical constraint exists on the number of computational units that can be employed. However, the remote server serves as a bottleneck in the system due to technical limitations. The server&apos;s capacity determines the maximum number of simultaneously connected clients. Notably, each client is not required to maintain a constant connection with the remote server. Consequently, this technical limitation can be easily surmounted. Each computational unit can perform calculations for weeks or even months before necessitating a connection to the remote server.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3"><draw:frame draw:style-name="fr1" draw:name="Frame2" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="2"><draw:text-box fo:min-height="3.1661in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image2" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="3.1661in" style:rel-height="scale" draw:z-index="3"><draw:image xlink:href="Pictures/1000000000000948000005DC201979DAD9828B19.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure1" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">2</text:sequence>: ANN training <text:span text:style-name="T4">with</text:span> DE-BP</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P3"/><text:p text:style-name="P13"><text:soft-page-break/>As depicted in Figure 2, the DE-based training process comprises five fundamental steps:</text:p><text:p text:style-name="P13">1. Loading a set of weights (a chromosome) from DE into the ANN.</text:p><text:p text:style-name="P13">2. Loading training examples into the ANN.</text:p><text:p text:style-name="P13">3. Calculating prediction values.</text:p><text:p text:style-name="P13">4. Computing the total prediction error (older data carry less impact on the calculated error).</text:p><text:p text:style-name="P13">5. Estimating chromosome fitness.</text:p><text:p text:style-name="P13"/><text:p text:style-name="P13">Every set of weights (chromosome) is introduced into the ANN structure. Subsequently, each input value undergoes a feed-forward process. The resultant output value is then juxtaposed with the anticipated value, and the disparity between them contributes to the overall error associated with this specific set of weights. This calculated total error serves as the fitness value for the DE population. The primary objective of DE optimization is to minimize the total error of the ANN. Due to the real-time nature of the training process (where new data continually emerges over time), the pursuit of total error minimization remains ongoing.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame3" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="4"><draw:text-box fo:min-height="2.8555in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image3" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="2.8555in" style:rel-height="scale" draw:z-index="5"><draw:image xlink:href="Pictures/1000000000000882000004D8087FEE1B151F6F0D.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure2" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">3</text:sequence>: Computation on a local machine</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P3"/><text:p text:style-name="P14">All computations are carried out locally, as depicted in Figure 3. Local computations encompass the Differential Evolution (DE) training loop, involving crossover and mutation operations. These operations are elaborated upon in Figures 4 and 5, respectively. Parent chromosomes are designated for crossover via the Genetic Algorithm (GA) selection rule. While various selection rules exist, the current model randomly selects two parents, determining the surviving parent based on a survival percentage.</text:p><text:p text:style-name="P14"/><text:p text:style-name="P14">The crossover operation is considered disruptive to the Artificial Neural Network (ANN) training process, and consequently, it can be regulated as <text:soft-page-break/>a parameter. Subsequently, the mutation phase is executed. The conventional DE mutation involves the summation of mutated chromosomes with a weighted difference vector derived from the discrepancy between two other randomly selected chromosomes and then multiplied by a weight coefficient. This mutation operator is a distinctive advantage of DE when juxtaposed with GA.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame4" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="6"><draw:text-box fo:min-height="4.7043in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image4" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="4.7043in" style:rel-height="scale" draw:z-index="7"><draw:image xlink:href="Pictures/100000000000070B0000069B6411A92D23C25DC2.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure3" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">4</text:sequence>: Crossover</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P3"/><text:p text:style-name="P3"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame5" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="8"><draw:text-box fo:min-height="4.2898in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image5" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="4.2898in" style:rel-height="scale" draw:z-index="9"><draw:image xlink:href="Pictures/1000000000000918000007C78D825628614DB685.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure4" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">5</text:sequence>: Mutation</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P3"/><text:p text:style-name="P15">Local copies of ANNs are distributed to each computational node for parallel training. Differential Evolution and Backpropagation techniques are applied on the local machines, with constant synchronization with the remote server. When DE is used for training the ANN, the issue of slow learning rates can be mitigated by transitioning to BP training. This process involves eliminating all recurrent connections, as illustrated in Figure 6.</text:p><text:p text:style-name="P15"><text:soft-page-break/></text:p><text:p text:style-name="P15"><draw:frame draw:style-name="fr1" draw:name="Frame6" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="10"><draw:text-box fo:min-height="2.4547in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image6" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="2.4547in" style:rel-height="scale" draw:z-index="11"><draw:image xlink:href="Pictures/100000000000062500000302B66BC73B6A9ADA13.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure5" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">6</text:sequence>: Switching ANN topology for BP training</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P15"/><text:p text:style-name="P15">The rate of learning convergence serves as a valuable indicator for determining when to switch between DE and BP and vice versa. Furthermore, it is worth noting that BP can be conceptualized as a specific case of Genetic Algorithm (GA) mutation.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc7828_527140494"/>Summary<text:bookmark-end text:name="__RefHeading___Toc7828_527140494"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P118"><text:span text:style-name="T41">C</text:span>hapter <text:span text:style-name="T41">1</text:span> focuses on employing Artificial Neural Networks in predicting time series within a distributed computational environment, emphasizing the challenges in accurate prediction and the application of Differential Evolution for more efficient training. The chapter delves into the <text:soft-page-break/>complexities of financial forecasting, particularly in the currency market, highlighting the intricate decisions traders face and the significance of accurate predictions in mitigating risks.</text:p><text:p text:style-name="P118"/><text:p text:style-name="P118">It starts with an overview of the challenges and nuances in time series prediction, specifically in financial markets where decisions depend on forecasting price changes, determining trade volumes, and managing the duration of open positions. The chapter stresses the importance of effective prediction models in such volatile markets.</text:p><text:p text:style-name="P118"/><text:p text:style-name="P118">Machine learning tools, particularly ANNs and DE, are introduced as pivotal elements in constructing these predictive models. ANNs are discussed in detail, highlighting their adaptability and ability to discern patterns in datasets, drawing parallels with biological neural networks. DE, a population-based optimization method inspired by evolution, is presented as a powerful tool for navigating complex search spaces without the constraints of differentiability.</text:p><text:p text:style-name="P118"/><text:p text:style-name="P118">The chapter details the organization of the forecasting model, which integrates ANNs and DE in a distributed environment. It outlines the steps involved in this model, from initializing the weights in the ANN to calculating prediction errors and estimating chromosome fitness using DE. <text:soft-page-break/>The parallelism of DE is emphasized, showcasing its potential in leveraging multiple computational units for training.</text:p><text:p text:style-name="P118"/><text:p text:style-name="P118">The training process is described, encompassing steps such as crossover, mutation, and synchronization with the remote server. The chapter also addresses the issue of slow learning rates and introduces the transition between DE and Backpropagation as a solution, emphasizing the adaptive nature of the learning process.</text:p><text:p text:style-name="P118"/><text:p text:style-name="P118">Overall, the chapter comprehensively explores utilizing ANNs and DE in a distributed environment for time series prediction, elucidating their roles, methodologies, and the intricate interplay between these techniques in constructing robust predictive models.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P107" text:outline-level="1"><text:bookmark-start text:name="__RefHeading___Toc351_966847004"/>2 Distributed Evolutionary Computing Migration Strategy by Accidental Node Involvement<text:bookmark-end text:name="__RefHeading___Toc351_966847004"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P16">Distributed evolutionary algorithms are implemented across heterogeneous computing nodes. In a distributed environment, it is common for these nodes to vary in terms of operating systems and hardware configurations. Such an environment presents significant challenges, particularly concerning network latency. Specific evolutionary optimization algorithms lend themselves well to distributed computing implementation due to their high level of parallel scalability. Typically, only the fitness function calculations are distributed synchronously or asynchronously.</text:p><text:p text:style-name="P16"/><text:p text:style-name="P16">In the former scenario, the population is solely hosted on the primary node. In the latter scenario, each node maintains a portion of the distributed population, a configuration known as the island model. Another prevalent approach relies on shared memory, granting each computing node access to the entire population - a model known as the fine-grained model. Various other models represent hybridizations of these basic approaches.</text:p><text:p text:style-name="P16"><text:soft-page-break/></text:p><text:p text:style-name="P16">Within the island model, a pivotal parameter pertains to the migration strategy. The most commonly employed node topology is the ring topology, where each node periodically forwards its best-performing individual to the subsequent node in the ring. However, exploring a hybrid model incorporating star topology and involving neighboring nodes could yield improvements.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc979_966847004"/>2.1 Introduction<text:bookmark-end text:name="__RefHeading___Toc979_966847004"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P3">Evolutionary Algorithms (EAs) represent efficient search methods grounded in natural selection and recombination principles. They have found successful applications in solving problems across various domains, including business, engineering, and science (Goldberg [9], Pappa [10]). By harnessing the power of EAs, viable solutions can be discovered within a reasonable time-frame. For more minor problems, a single computing node can suffice. However, as problems increase in size or complexity, the time required to find suitable solutions also escalates.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3">Given the burgeoning popularity of parallel computing, employing it has emerged as a promising strategy to expedite EAs. Certain distributed evolutionary algorithms (DEAs) operate with a singular population, while <text:soft-page-break/>others divide the population into multiple, relatively independent sub-populations. A comprehensive classification can be found in existing literature (Adamidis [11], Gordon [12], Lin [13]). In distributed computing, the migration strategy is pivotal in implementing DEAs.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3">In cases where EA&apos;s population is distributed among numerous computing nodes, each node possesses a portion of the population referred to as a sub-population. In such instances, local recombination and fitness function evaluations are conducted. To enhance optimization convergence, the computing nodes exchange individuals, a process is known as migration. Several parameters govern this migration process, shaping a strategy for information interchange in DEAs.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3">The primary parameter pertains to traveler selection, determining the approach for choosing an individual from the sub-population to be dispatched. In most scenarios, the optimal individual is chosen. The second crucial parameter is migration frequency, dictating how often selected individuals traverse between sub-populations. This parameter is specific to the problem and is typically adjusted through experimental means. The third and most pivotal parameter concerns the migration destination, signifying how nodes are structured in terms of topology and how each traverses within this topology.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3"><text:soft-page-break/>One option for this parameter is individual broadcasting, wherein each node receives a copy of selected travelers from other nodes. An alternative is a ring topology, where each traveler migrates to the adjacent node. A grid topology is also viable (Spiessens [14], Kruger [15]), with each node having four neighbors. Numerous other topologies exist, including hierarchical, 3D-based, hybrid, and more.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3">A distribution strategy grounded in the participation of incident nodes (where volunteers are expected to join the project and contribute computing power) holds the potential to address these challenges. Computing nodes are structured in a star topology, and the island model (Tanese [16], Uchida [17]) is applied to DEAs.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc772_951591551"/>2.2 Accidental Node Involvement<text:bookmark-end text:name="__RefHeading___Toc772_951591551"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P18">The goal of optimization is to adjust the weights of an Artificial Neural Network in order to achieve improved forecasting. Training an ANN with Evolutionary Algorithms (EAs) can be highly computationally intensive, especially when dealing with ANNs with more than 400 weights, depending on their topology. Due to the time-consuming nature of EA-based ANN training, the computing nodes should operate relatively autonomously.</text:p><text:p text:style-name="P18"><text:soft-page-break/></text:p><text:p text:style-name="P18">The distributed system is structured in a star topology, comprising a lightweight central node (server) and heavily loaded remote computing nodes (clients). In the context of Differential Evolution Algorithm (DEA) implementation, the island model is suitable. A global EA population is situated in the central node, while numerous local EA populations are distributed among the remote computing nodes. Each remote computing node can join or leave the system at any moment, functioning asynchronously (Figure 7). Upon a new client joining, the central node transmits a subset of the global population. Subsequently, communication between the central and remote nodes ceases, allowing the remote node to evolve the local EA population using a sequential EA approach.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame7" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="12"><draw:text-box fo:min-height="4.9866in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image7" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="4.9866in" style:rel-height="scale" draw:z-index="13"><draw:image xlink:href="Pictures/10000000000004150000040FC03E2C163005CE4E.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure6" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">7</text:sequence>: Accidental node involvement</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P3"/><text:p text:style-name="P3">The communication between the remote node and the central node is reinitiated only if a better solution is found on the remote node, which must then be reported to the central node. Through this organization of the calculation process, each remote node can operate for hours, weeks, <text:soft-page-break/>or even months before sending any information to the central node. In practical terms, a failure of the central node will not impact the performance of the remote nodes. Even in a central node failure, the remote nodes will transmit their results once the central node becomes available.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3">The distribution of individuals occurs exclusively during the process of remote node joining. Local best-found solutions are relayed to the central node, migrating to the subsequent remote node upon incorporation. Due to the sluggish nature of EA-based training for ANNs, this distribution strategy proves highly efficient. Additionally, in financial time series forecasting domains, the training set undergoes constant fluctuations due to the incessant influx of new data. Consequently, the objective function, aimed at optimizing the total error of ANNs, remains in a state of perpetual flux. Hence, the application of continuous ANN training becomes imperative.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc632_1783862520"/>2.3 Distribution Parameterization<text:bookmark-end text:name="__RefHeading___Toc632_1783862520"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P3">Distribution is an essential operation responsible for the seamless exchange of individuals among the nodes within the DEA framework. When considering distribution, a set of parameters becomes relevant, <text:soft-page-break/>including the distribution gap, distribution rate, selection/replacement, topology, and heterogeneity. Specific parameters can be applied in the context of the Distribution Strategy by Accidental Node Involvement, while others lack reasonable significance.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3">Defining a parameter such as the distribution gap becomes intricate in the context of Accidental Node Involvement. The literature outlines two prevalent approaches. The first one involves regularly measuring the distribution gap after several steps. The second one is probabilistic, occurring in each generation with a certain probability (Gorges-Schleuter [18], Munetomo [19], Voigt [20]). The distribution gap aligns more closely with the probabilistic model. As elucidated in the preceding sections, distribution occurs only once when the computing node joins the system. From this perspective, the distribution gap follows an exponential distribution in a probabilistic manner. The model avoids the issues of ineffectiveness or super-individual problems since distribution is relatively infrequent.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3">The distribution rate represents a parameter specific to the problem, determining the number of individuals that will traverse among the local populations. Typically, it is denoted as a population percentage or absolute value. Various recommendations exist regarding the estimation of this parameter, although the prevailing approach is experimental, as <text:soft-page-break/>evidenced by studies such as <text:span text:style-name="T11">(</text:span>Tanese [16], Belding [21], and Mejia-Olvera [22]<text:span text:style-name="T11">)</text:span>. In the proposed model, this parameter is defined as a percentage (a fraction) of the global population (i.e., the population residing on the central node). When expressed in absolute terms, it equates to the magnitude of the local population (i.e., the population size on the remote node).</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3">There are two main approaches for selecting migrants: the first involves choosing the most exceptional individuals, and the second entails selecting individuals randomly. Of course, it is worth noting that numerous alternative selection methods can be employed, similar to those utilized in genetic algorithms (Baker [23], Lim [24]). In the proposed model, the selection process occurs at the central node, and random selection is utilized. The quantity of migrants selected corresponds to the size of the remote sub-population. Notably, there is no replacement procedure since distribution occurs only once, from the central to the remote node. Based on the node joining process, several identified individuals can be directed in the opposite direction (remote to central). However, these individuals will then participate in other remote sub-populations.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3">DEAs are categorized into two standard models: the stepping-stone and island models. This categorization hinges on whether individuals have the <text:soft-page-break/>freedom to migrate to any local population or are restricted to migrating only to geographically nearby islands. Numerous studies have endeavored to determine the optimal topology for a DEA, with the ring and hyper-cube topologies emerging as the most favored choices in many instances (Adamidis [11], Gordon [12], Lin [13], Mejia-Olvera [22]). In most cases, issues such as parallelization and scalability arise with fully connected and centralized topologies due to their tight connectivity. The proposed model&apos;s most suitable topology is a star configuration featuring a centralized node and relatively independent remote computing nodes. EA-based ANN training is a time-intensive process, often leading to extended periods during which remote computing nodes operate without communication with the central node. Consequently, the central node poses minimal risk to the distributed system. Disruption of the connection with the central node does not affect the local optimization process. The central node&apos;s failure only impacts new nodes attempting to join the system. Despite these drawbacks, the proposed model exhibits exceptional scalability due to the lightweight nature of the central node.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3">Accidental Node Involvement is ideally suited for heterogeneity. On each remote computing node, a distinct optimization algorithm can be employed. This approach allows for a much-improved balance between exploration and exploitation, a well-established trade-off decision in evolutionary algorithms. The relevance of this parameter varies <text:soft-page-break/>depending on the problem. For instance, in the context of the distribution of weights in artificial neural networks, it can be seamlessly implemented within a distributed system due to the availability of highly effective gradient-based training algorithms.</text:p><text:p text:style-name="P119"/><text:h text:style-name="P109" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc7830_527140494"/>Summary<text:bookmark-end text:name="__RefHeading___Toc7830_527140494"/></text:h><text:p text:style-name="P119"/><text:p text:style-name="P120">Chapter 2 delves into the complexities and strategies involved in Distributed Evolutionary Computing, mainly focusing on migration strategies through Accidental Node Involvement. It explores the challenges of heterogeneous computing nodes and varying network latencies, presenting different models for distributing populations in evolutionary algorithms.</text:p><text:p text:style-name="P120"/><text:p text:style-name="P120">The chapter begins by highlighting the significance of Evolutionary Algorithms in problem-solving across domains and the shift toward parallel computing to expedite these algorithms. It delineates the division of populations in distributed environments and the critical role of migration strategies in Distributed Evolutionary Algorithms.</text:p><text:p text:style-name="P120"/><text:p text:style-name="P120">Within the context of the island model, a critical parameter explored is the migration strategy, emphasizing the impact of topology on node <text:soft-page-break/>communication and individual migration. The chapter proposes a hybrid model employing a star topology, where nodes operate autonomously, joined by a lightweight central node.</text:p><text:p text:style-name="P120"/><text:p text:style-name="P120">The section on Accidental Node Involvement presents a practical scenario employing a Differential Evolution Algorithm to adjust weights in Artificial Neural Networks. It details a star topology structure with a central node and remote computing nodes, highlighting these nodes&apos; independence and asynchronous operation. Communication between central and remote nodes occurs selectively, optimizing computational efficiency and resilience in case of central node failures.</text:p><text:p text:style-name="P120"/><text:p text:style-name="P120">The chapter further elucidates parameters relevant to distribution strategies within Accidental Node Involvement. It explores the distribution gap, rate, selection methods, topology choices, and heterogeneity in a DEA context. Notably, it emphasizes the infrequent distribution due to the probabilistic nature of node joining, optimizing computational resources while addressing ineffectiveness issues.</text:p><text:p text:style-name="P120"/><text:p text:style-name="P120">The proposed model advocates for a star topology due to its scalability and minimal impact of central node disruptions on remote node operations. It also suggests leveraging heterogeneity across nodes to <text:soft-page-break/>balance exploration and exploitation, which is crucial in evolutionary algorithms.</text:p><text:p text:style-name="P120"/><text:p text:style-name="P120">Overall, the chapter offers a comprehensive understanding of the challenges and strategies involved in distributing populations in DEAs, emphasizing the effectiveness of the Accidental Node Involvement model in addressing these complexities.</text:p><text:p text:style-name="P119"/><text:h text:style-name="P107" text:outline-level="1"><text:bookmark-start text:name="__RefHeading___Toc634_1783862520"/>3 Modifications of Artificial Neural Networks<text:bookmark-end text:name="__RefHeading___Toc634_1783862520"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P3">Classical artificial neural networks have been studied and utilized in various industries for decades. While they have demonstrated successful applications in specific tasks, their performance could be better in others. Despite being extensively researched, there remain aspects within them that can be modified to achieve greater efficiency. The key aspect of artificial neural networks is their remarkable efficiency once they are trained; however, the training process is frequently characterized by slowness and inefficiency.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3">In this section, various ideas for modifying different components within artificial neural networks will be presented. These ideas offer opportunities to enhance efficiency and reduce training time.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc718_1268615346"/>3.1 Alternative Activation Function Derivative<text:bookmark-end text:name="__RefHeading___Toc718_1268615346"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P19">Classic artificial neural networks can be conceptualized as directed weighted graphs. These networks operate in two modes – training and performance. The training process involves seeking appropriate values for <text:soft-page-break/>the weights in the graph (Keremedchiev [25], Tomov [26], Zankinski [27]) to enable the network to optimally correlate information from the input with the information at the output. In classical three-layer networks, information is propagated from the input to the output, with each node receiving signals from the nodes in the preceding layer.</text:p><text:p text:style-name="P19"/><text:p text:style-name="P19">These signals are derived using a summing function, most commonly linear (involving the multiplication of the signal by the weight assigned to the connection between two nodes). The cumulative input signals are then directed to a threshold function that determines the activation level of each node.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3"><text:soft-page-break/><draw:frame draw:style-name="fr2" draw:name="Frame8" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="5.4083in" style:rel-height="scale-min" draw:z-index="14"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image8" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="5.0201in" style:rel-height="scale" draw:z-index="15"><draw:image xlink:href="Pictures/10000000000001F4000001F476EB77554A5ADDF3.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure7" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">8</text:sequence>: Decaying sine wave activation function</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P3"/><text:p text:style-name="P3"><text:soft-page-break/><draw:frame draw:style-name="fr2" draw:name="Frame11" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="5.4083in" style:rel-height="scale-min" draw:z-index="20"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image11" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="5.0201in" style:rel-height="scale" draw:z-index="21"><draw:image xlink:href="Pictures/10000000000001F4000001F4E5D0BC5BEE7C2055.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure8" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">9</text:sequence>: Exponent regulated sin<text:span text:style-name="T5">e</text:span> activation function</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P3"/><text:p text:style-name="P20">Multiple activation functions are commonly used in practice (Karlik [28]). In this context, we consider a decaying function with a periodic character (as shown in Figure 8 <text:span text:style-name="T14">and Figure 9</text:span>). The periodic component is an effect of the sine component. When employing the correct training methods, <text:soft-page-break/>such as the error backpropagation method, the derivative of the activation function becomes of primary importance. The first derivative&apos;s values directly determine the extent to which the weights will change during the training process.</text:p><text:p text:style-name="P20"/><text:p text:style-name="P20"><draw:frame draw:style-name="fr2" draw:name="Frame9" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="5.4083in" style:rel-height="scale-min" draw:z-index="16"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image9" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="5.0201in" style:rel-height="scale" draw:z-index="17"><draw:image xlink:href="Pictures/10000000000001F4000001F41B49D716BE89478B.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure9" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">10</text:sequence>: First derivative of decaying sine wave activation function</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P20"><text:soft-page-break/></text:p><text:p text:style-name="P20"><draw:frame draw:style-name="fr2" draw:name="Frame12" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="5.4083in" style:rel-height="scale-min" draw:z-index="22"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image12" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="5.0201in" style:rel-height="scale" draw:z-index="23"><draw:image xlink:href="Pictures/10000000000001F4000001F4F14C431A42B0C8A1.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure10" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">11</text:sequence>: First derivative of exponent regulated sine activation function</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P20"/><text:p text:style-name="P20">In situations where the first derivative also exhibits a periodic component (as depicted in Figure <text:span text:style-name="T15">10</text:span> <text:span text:style-name="T15">and Figure 11</text:span>), it becomes possible to replace this first derivative mechanically. Such a substitution is feasible due to the <text:soft-page-break/>organizational structure of software libraries designed for working with artificial neural networks.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P21">Each neuron within the artificial neural network determines its activity level based on a normalization function called an activation function. It is crucial to standardize the output of each neuron to a specific interval, typically ranging between zero and one or, alternatively, between minus one and plus one. This necessity arises due to the varying numbers of neurons across different layers of artificial neural networks. Without normalization, the signals transmitted to the subsequent layer would be disproportionate and uneven.</text:p><text:p text:style-name="P21"/><text:p text:style-name="P21">The activation function<text:span text:style-name="T16">s</text:span> depicted in Figure 8 <text:span text:style-name="T16">and Figure 9</text:span> <text:span text:style-name="T16">have</text:span> been selected to emulate the natural saturation processes found in nature. When the sum of inputs is positive, the neuron generates a positive output value. Conversely, in the presence of a negative input sum, the neuron yields a negative output value. Simultaneously, if the input signals are excessively intense, encompassing both negative and positive values, the simulated saturation process prevents the neuron from emitting a signal.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3"><text:soft-page-break/><draw:frame draw:style-name="fr2" draw:name="Frame10" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="5.7134in" style:rel-height="scale-min" draw:z-index="18"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image10" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="5.0201in" style:rel-height="scale" draw:z-index="19"><draw:image xlink:href="Pictures/10000000000001F4000001F412CC8C5452BC8AAC.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure11" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">12</text:sequence>: First derivative alternative of decaying sine wave activation function</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P22"/><text:p text:style-name="P22">This systematic activation mechanism enables the neurons within a given layer to evenly distribute their responsibilities, thereby fostering a more balanced representation of information throughout the network.</text:p><text:p text:style-name="P3"><text:soft-page-break/></text:p><text:p text:style-name="P3">When the activation function includes a periodic component, it is also reflected in its first derivative. On one hand, the periodic aspect of the first derivative is noticeable; however, two distinct discontinuities are also clearly evident (see Figure <text:span text:style-name="T16">12</text:span>). As a result of these complexities, the convergence of the backpropagation learning algorithm becomes slower. An elegant approach to expedite the process involves replacing the first derivative with a function that closely follows the same form yet lacks both the periodic component and breakpoints (refer to Figure 10). Not only does this alternative derivative exhibit more appropriate mathematical properties but it is also computed more quickly than the original derivative.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc1666_1268615346"/>3.2 Long Short Term Memory in Multilayer Perceptron Pair<text:bookmark-end text:name="__RefHeading___Toc1666_1268615346"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P23">In artificial neural networks&apos; most commonly used case, such as the multilayer perceptron, signals are transmitted from the input to the output. Each neuron&apos;s input information is derived from the outputs of the preceding neurons. The external impact received through this process is then subjected to a summation function. The prevailing summation function is typically the linear function, where the output signals of <text:soft-page-break/>neurons are multiplied by the connection weights. However, it is essential to note that functions other than the linear function can also be applied. The outcome of the summation function is subsequently channeled into a normalization function, which determines the neuron&apos;s activation level. The literature presents a variety of proposed activation functions, some well-established, while others are not as recognized.</text:p><text:p text:style-name="P23"/><text:p text:style-name="P23">In the classical multilayer perceptron, links between neurons exist solely from the input to the output. Signals cannot propagate from the output to the input. In the backward pass, only the neurons&apos; errors are propagated. Consequently, in an artificial neural network with such a topology, there is no capacity to retain past information circulating within the network. Due to this limitation, multilayer perceptrons are not particularly well-suited for forecasting tasks, particularly in the field of financial time series forecasting. Recurrent links were introduced to address the absence of memory in multilayer perceptrons, as seen in the Jordan and Elman networks. This innovation allows for the retention of historical information within the network.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3"><text:soft-page-break/><draw:frame draw:style-name="fr2" draw:name="Frame13" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="5.8583in" style:rel-height="scale-min" draw:z-index="24"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image13" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="5.4701in" style:rel-height="scale" draw:z-index="25"><draw:image xlink:href="Pictures/10000001000003E600000441CA747C47ADF55E41.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure12" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">13</text:sequence>: A pair of multilayer perceptrons</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P3"/><text:p text:style-name="P24">Memory can be realized using a paired network of two multilayer perceptrons, as depicted in Figure 13. In this arrangement, MLP1 takes as input the historical values of the time series combined with the output of <text:soft-page-break/>MLP2. The anticipated outcome is generated at the output layer of MLP1. Additionally, the output of MLP1 serves as an input for MLP2, followed by the propagation of signals within MLP2. This process endows MLP2 with the role of a network memory component. In this implementation, both multilayer perceptrons undergo training using backpropagation to minimize errors.</text:p><text:p text:style-name="P24"/><text:p text:style-name="P24">Prior to entering MLP1, the time series values undergo normalization. The time series is segregated into two segments: the past frame (lag) and the future frame (lead), each with its specific conditional division.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc1317_2021269790"/>3.3 Self-Growing Multilayer Perceptron <text:span text:style-name="T17">f</text:span>or Time Series Forecasting<text:bookmark-end text:name="__RefHeading___Toc1317_2021269790"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P25">Numerous techniques are applied in time series forecasting (Atanasova [29]). Artificial neural networks are one technique that is successfully applied for such forecasting. Time series consist of values measured over time while maintaining a strict order of the measurements (time-value pairs). The measurement interval is usually fixed, but variations are also possible. The fundamental concept behind time series is that values are not independent across time; they are interconnected in that future values depend on past values. A forecasting problem is defined as <text:soft-page-break/>predicting future values based on past values. To achieve successful forecasting, the construction of a prediction model is necessary. Artificial neural networks stand out as proven models in time series forecasting.</text:p><text:p text:style-name="P25"/><text:p text:style-name="P25">Initially, artificial neural networks drew inspiration from biological neural systems and first appeared in the mid-20th century. The most commonly used artificial neural networks are oriented weighted graphs, with the nodes referred to as neurons. The connections between neurons carry weights, which form the core of the information presented in the network. Artificial neural networks operate in two standard modes: training and operation.</text:p><text:p text:style-name="P25"/><text:p text:style-name="P25">The training mode is executed as an optimization task involving the modification of weights in the network to enable the best learning of training patterns. Over the last four decades, numerous training algorithms have been developed, but the most popular one remains the backpropagation of the error. Backpropagation of the error is a precise numerical method and the preferred training approach in this study. The idea revolves around minimizing the total neural network error across all training examples processed. The gradient of the total error determines the direction and magnitude of the weight updates. The organization of links between neurons follows a typical pattern in artificial neural network topology. Various topologies have been extensively researched in the <text:soft-page-break/>literature, including generalized nets (Tashev [30]) and deep-learning neural networks. When dealing with noisy time series data, input information can be filtered using techniques such as a Kalman filter (Alexandrov [31]).</text:p><text:p text:style-name="P25"/><text:p text:style-name="P25">The critical concept in deep learning neural networks departs from the traditional approach, wherein instead of using hidden layers, the number of nodes in both the input and hidden layers increases during neural network training. Expanding the input layer is motivated by each new measurement expanding the time series. The training objective is to increase the input layer size to match the complete time series size.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P26">Conditionally, the time series is divided into the past and the future. The values supplied to the input of the artificial neural network are called lags, and they constitute a subset of the closest past values to the future values. The values obtained in the output of the artificial neural network are predictions, which are then compared with a subset of the future values called leads. A multilayer perceptron is employed as the base artificial neural network for the proposed model. It consists of input, hidden, and output layers.</text:p><text:p text:style-name="P26"/><text:p text:style-name="P26">In the proposed model, a set of artificial neural sub-networks is utilized, and these sub-networks are integrated into a comprehensive artificial <text:soft-page-break/>neural network. The smallest artificial neural sub-network features a 1-1-1 topology (Figure 14-left). The network is trained with examples containing only a single value in the input. The model&apos;s objective is to forecast only one value ahead of time, which is why all sub-networks yield only a single output. Figure 14-left displays just 3 intermediary training examples. All 29 input values are provided as training instances for resilient backpropagation training. Training halts once a specific epsilon level for total neural network error change is reached.</text:p><text:p text:style-name="P26"/><text:p text:style-name="P26"><draw:frame draw:style-name="fr3" draw:name="Frame14" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="3.5429in" style:rel-height="scale-min" draw:z-index="26"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image14" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="2.85in" style:rel-height="scale" draw:z-index="27"><draw:image xlink:href="Pictures/1000000000000826000004A31930735E10CAF8DB.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure13" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">14</text:sequence>: Training of artificial neural sub-networks with 1-1-1 topology (left) and 2-1-1 topology (right)</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P26"/><text:p text:style-name="P26"><text:soft-page-break/>After training the 1-1-1 topology, the weight values from the first sub-network are transferred to the second sub-network with a 2-1-1 topology (Figure 14-right). It is evident that one weight will not be transferred since it is absent in the first sub-network. This particular weight retains its value from the prior training of the largest sub-network. The time series is restructured to provide two input values and anticipate one forecasted value in the output. For the second sub-network, there exist 28 input examples, and a single output is anticipated. The training process mirrors that of the first sub-network - utilizing resilient backpropagation training. Like the first sub-network, training concludes upon reaching a specific epsilon level for total neural network error change.</text:p><text:p text:style-name="P26"/><text:p text:style-name="P26">The third sub-network adopts a 3-2-1 topology. The hidden layer&apos;s size is determined automatically by an incremental pruning algorithm. Figure 15-left exhibits two neurons in the hidden layer, though this is merely illustrative; the algorithm estimates the actual hidden layer size. The training algorithm and stopping criteria are consistent with the previous sub-networks.</text:p><text:p text:style-name="P26"/><text:p text:style-name="P26"><text:soft-page-break/><draw:frame draw:style-name="fr2" draw:name="Frame15" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="3.5429in" style:rel-height="scale-min" draw:z-index="28"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image15" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="2.85in" style:rel-height="scale" draw:z-index="29"><draw:image xlink:href="Pictures/1000000000000827000004A3100690A9A715E8CD.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure14" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">15</text:sequence>: Training of artificial neural sub-networks with 3-2-1 topology (left) and 4-2-1 topology (right)</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P26"/><text:p text:style-name="P26">The fourth sub-network employs a 4-2-1 topology, and once again, the hidden layer&apos;s size is illustrative (Figure 15-right). An incremental pruning algorithm estimates the true hidden layer size. Training instances are reduced by one compared to the previous sub-network due to the increased input size by one. Training and stopping criteria align with those of the prior sub-networks. Figures 14 and 15 exclusively present the initial 4 sub-networks. In the model implementation, a multitude of additional sub-networks are involved. Sub-network topologies evolve by adding a single neuron to the input layer and adapting the hidden layer size through an incremental pruning algorithm. The ultimate aim is to attain <text:soft-page-break/>an n-m-1 topology (Figure 16), encompassing all known time series values. Numerous connections between the input and the hidden layers in Figure 16 are omitted for clarity, but both layers are fully interconnected in the model implementation.</text:p><text:p text:style-name="P26"/><text:p text:style-name="P26"><draw:frame draw:style-name="fr1" draw:name="Frame16" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="30"><draw:text-box fo:min-height="1.8626in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image16" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="1.8626in" style:rel-height="scale" draw:z-index="31"><draw:image xlink:href="Pictures/1000000000000714000002A1E531DFDE666FE3D4.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure15" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">16</text:sequence>: Training of artificial neural sub-network with n-m-1 topology as certain links between the input and hidden layers are omitted for better visualization</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P26"/><text:p text:style-name="P26">Following the training of the largest sub-network, the hierarchical process regresses back to the smallest sub-network. Weight values from the largest sub-network corresponding to the links in the smaller sub-network are extracted and integrated into the smallest sub-network. Similarly, weight values are sourced from the largest sub-network for other sub-networks, combined with the weights from the previous, smaller sub-network. For instance, the sub-network with a 4-2-1 topology borrows <text:soft-page-break/>some of its weights from the 3-2-1 sub-networks, but links absent in the smaller sub-network are taken from the largest sub-network.</text:p><text:p text:style-name="P26"/><text:p text:style-name="P26">The proposed model&apos;s fundamental concept is the incremental training of increasingly sized artificial neural networks. This training approach is inspired by natural neural systems, wherein biological cells proliferate and form connections. A common issue in artificial neural network training is network size. The training process is expedited by segmenting the largest network into smaller ones. It is widely recognized in the field of time series forecasting that the earliest measurements exert minimal influence on the forecast. The proposed model accommodates this reality by incorporating the oldest measurements in the largest sub-network, although their impact on the final forecast remains relatively minor. The model boasts a heightened degree of self-adaptation; as new values in the time series emerge, the artificial neural network&apos;s size expands, synchronizing the training and operational phases.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc2431_612792312"/>3.4 Permutations in Graph Structure<text:bookmark-end text:name="__RefHeading___Toc2431_612792312"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P29">The most commonly used approach to training a multilayer perceptron is the exact numerical method with error backpropagation. Exact numerical methods are most often gradients, which impose certain restrictions on <text:soft-page-break/>the type of activation function. Some of the limitations of exact numerical methods are overcome by using evolutionary heuristics for global optimization. Using heuristics for global optimization also provides an additional opportunity to train artificial neural networks in a distributed environment with multiple computing machines.</text:p><text:p text:style-name="P29"/><text:p text:style-name="P29">A multilayer perceptron consists of neurons (graph nodes) and connections (edges in the weight graph). Neurons are organized into layers, each connected by a weighted link to every neuron in the next layer (see Figure 17). Information from the external environment enters the input layer, passes through the inner (there may be more than one) layers, and leaves the neural network through the output layer. In addition to input, output, and ordinary neurons, the multilayer perceptron has one additional bias neuron, which constantly emits a single value and has no input edges.</text:p><text:p text:style-name="P29"/><text:p text:style-name="P29"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame17" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="32"><draw:text-box fo:min-height="4.0126in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr5" draw:name="Image17" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="3.9598in" style:rel-height="scale" draw:z-index="33"><draw:image xlink:href="Pictures/100000010000023F0000020A1B3CFD7EF9D20CAF.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure16" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">17</text:sequence>: Three-layer network</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P29"/><text:p text:style-name="P29">Being the most common and one of the most effective algorithms for training artificial neural networks, error backpropagation has the primary task of calculating values for the weights in the network so that the network successfully learns the functional dependence between the input and output data. Error backpropagation is an exact gradient optimization method. It consists of two main phases – forward pass and backward pass. During the forward calculation, signals are passed from the network&apos;s input to its output. The error made during the forward pass is <text:soft-page-break/>determined at the network&apos;s output. The reverse pass follows based on the calculated error, where the error is propagated back through the layers. This backpropagation allows the calculation of the fractional error committed by each neuron. Using the calculated partial error, a decision is made regarding how much the weights connecting the neuron to the neurons of the previous layer should be adjusted.</text:p><text:p text:style-name="P28"/><text:p text:style-name="P28">The main task in training artificial neural networks using the error backpropagation method is to arrive at weight values in the network so that the network incurs minimal total error when given training and test examples. A primary aspiration of scientists working in the field of artificial neural networks is to search for algorithms, approaches, and methods that can accelerate the process of training artificial neural networks to the maximum extent.</text:p><text:p text:style-name="P28"/><text:p text:style-name="P28">Building upon the research and results published by (Zankinski [27]), the present study builds on the idea by modifying the error backpropagation algorithm. The modification involves deactivating a randomly chosen neuron during the weight correction process (see Figures 18-20) in the backward pass of the error backpropagation algorithm.</text:p><text:p text:style-name="P28"/><text:p text:style-name="P28"><text:soft-page-break/>In weight correction, the randomly selected neuron does not participate in the backpass. This allows some weights in the network to remain unchanged until the backward pass of the next epoch is executed.</text:p><text:p text:style-name="P28"/><text:p text:style-name="P28">It is essential to note that all neurons participate in the forward pass and contribute to the output signals of the network, including the randomly chosen one that does not participate in the backpropagation process.</text:p><text:p text:style-name="P27"/><text:p text:style-name="P27"><text:soft-page-break/><draw:frame draw:style-name="fr2" draw:name="Frame18" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="4.9382in" style:rel-height="scale-min" draw:z-index="34"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image18" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="4.55in" style:rel-height="scale" draw:z-index="35"><draw:image xlink:href="Pictures/100000010000023F0000020A4AA275F3F12CAC9F.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure17" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">18</text:sequence>: First learning cycle</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P27"/><text:p text:style-name="P27"><text:soft-page-break/><draw:frame draw:style-name="fr2" draw:name="Frame19" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="4.9382in" style:rel-height="scale-min" draw:z-index="36"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image19" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="4.55in" style:rel-height="scale" draw:z-index="37"><draw:image xlink:href="Pictures/100000010000023F0000020AC658451223671A88.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure18" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">19</text:sequence>: Second learning cycle</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P27"/><text:p text:style-name="P27"><text:soft-page-break/><draw:frame draw:style-name="fr2" draw:name="Frame20" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="4.9382in" style:rel-height="scale-min" draw:z-index="38"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image20" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="4.55in" style:rel-height="scale" draw:z-index="39"><draw:image xlink:href="Pictures/100000010000023F0000020AF4B42A21486DCAE3.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure19" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">20</text:sequence>: Third learning cycle</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P27"/><text:p text:style-name="P27">The foundation of this proposal lies in the fact that in natural nervous systems, some nerve cells die or become overloaded with signals.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P30">As proposed by (Zankinski [27]), the algorithm for permuting neurons leads to another idea for permuting artificial neural network weights. Neuron permutation involves a more extensive scope than permuting two <text:soft-page-break/>individual weights, rendering this approach more favorable. The selection of which weights to swap is made randomly. Notably, the training algorithm remains unchanged compared to the classic backpropagation method.</text:p><text:p text:style-name="P30"/><text:p text:style-name="P30">The process of selecting neurons for weight swapping is executed randomly but with consideration to preventing neurons within the same layer from self-selecting. The permutation is not applied continuously; instead, it follows a probabilistic rate set as a constant value.</text:p><text:p text:style-name="P30"/><text:p text:style-name="P30">Weights are swapped for a single training cycle, after which they are restored to their original positions. The weight swapping occurs briefly since prolonged swapping would impede the backpropagation procedure, hindering convergence. This type of weight permutation can be likened to introducing a training noise. A crucial modification involves swapping weights with minimal differences. If the disparity between swapped weights is substantial, the algorithm becomes more detrimental to the training process than beneficial.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P31">Backpropagation is an exact numerical method. However, it has the disadvantage of becoming trapped in local optima. Numerous attempts have been made to utilize stochastic or hybrid training algorithms to address this limitation.</text:p><text:p text:style-name="P31"><text:soft-page-break/></text:p><text:p text:style-name="P31">Another idea, originating from the proposed neuron permutation by (Zankinski [27]), involves the permutation of activation functions as a modification of the backpropagation training algorithm used for multilayer perceptrons.</text:p><text:p text:style-name="P31"/><text:p text:style-name="P31"><draw:frame draw:style-name="fr2" draw:name="Frame21" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="3.4382in" style:rel-height="scale-min" draw:z-index="40"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image21" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="2.4402in" style:rel-height="scale" draw:z-index="41"><draw:image xlink:href="Pictures/100000010000064A0000030E180B89FD985739EE.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure20" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">21</text:sequence>: Components of an artificial neuron ( https://commons.wikimedia.org/wiki/File:ArtificialNeuronModel_english.png )</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P31"/><text:p text:style-name="P31">Each neuron in an artificial neural network has weights attached to it, a transfer function (usually a sum of the weighted input signals), and an activation function (Figure 21).</text:p><text:p text:style-name="P31"/><text:p text:style-name="P31"><text:soft-page-break/><draw:frame draw:style-name="fr2" draw:name="Frame22" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="3.9075in" style:rel-height="scale-min" draw:z-index="42"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image22" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="2.9098in" style:rel-height="scale" draw:z-index="43"><draw:image xlink:href="Pictures/1000000100000335000001DCDB7BC7E6AA5645AD.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure21" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">22</text:sequence>: Multilayer perceptron ( https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q_o8pu8aPRGKo2Bq5KvzCQ.png )</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P31"/><text:p text:style-name="P31">The neurons within the topology of the multilayer perceptron network are organized in layers (Figure 22). At the same time, the input layer is responsible for receiving inputs, neurons in the hidden layer relay signals within the network. The transfer function involves collecting input signals through weighted multiplication. The most commonly used transfer function is summation. After signal collection, normalization is applied to emit a single signal from the neuron. This normalization is achieved using an activation function. Although various mathematical functions can serve as activations, the most prevalent options are hyperbolic tangent and <text:soft-page-break/>sigmoid functions. The activation function plays a significant role as the number of input links to a single neuron can vary from a few to hundreds.</text:p><text:p text:style-name="P31"/><text:p text:style-name="P31">The weights themselves constitute another crucial factor in neural network training. In classical multilayer perceptrons, weights are unconstrained real numbers. This implies that large negative and positive values can influence the regular functioning of artificial neural networks. The issue of summation involving a wide range of multiplications between input signals and weights can lead to uneven participation of different neurons in the network. These challenges are addressed through normalization using an appropriate activation function.</text:p><text:p text:style-name="P31"/><text:p text:style-name="P31">The proposed concept involves employing a network with an initial topology of three layers and using hyperbolic tangent as the activation function for each neuron. Several neurons are randomly selected, and their activation functions are switched from hyperbolic tangent to the sigmoid function for a single training cycle. This approach achieves the permutation of the activation functions.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P32">The permutation of weights can be expanded beyond a single artificial neural network by employing two parallel multilayer perceptrons. Each perceptron consists of three layers: input, hidden, and output. The primary multilayer perceptron utilizes the hyperbolic tangent activation <text:soft-page-break/>function, while the secondary one employs the sigmoid function. In each training cycle, with a specified probability, randomly chosen weights from the secondary multilayer perceptron are duplicated into the primary multilayer perceptron. Both artificial neural networks share an identical topology, ensuring a direct correspondence of weights between the primary and secondary networks.</text:p><text:p text:style-name="P32"/><text:p text:style-name="P32">Both networks undergo training using a back-propagation training procedure and are provided with identical input-output training examples. The similarity between the shapes of the sigmoid function and the hyperbolic tangent introduces additional noise to the primary network, which aids in escaping local optima. This enhancement facilitates improved convergence during the training phase.</text:p><text:p text:style-name="P121"/><text:h text:style-name="P110" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc7832_527140494"/>Summary<text:bookmark-end text:name="__RefHeading___Toc7832_527140494"/></text:h><text:p text:style-name="P121"/><text:p text:style-name="P122">Chapter 3 delves into four significant modifications to enhance artificial neural networks. </text:p><text:p text:style-name="P122"/><text:p text:style-name="P122">Explores the replacement of the activation function&apos;s derivative. It proposes using a function that resembles the original but lacks periodicity <text:soft-page-break/>and breakpoints, aiming to expedite training by improving mathematical properties. </text:p><text:p text:style-name="P122"/><text:p text:style-name="P122">Introduces a paired network concept where one perceptron retains historical information while another forecasts outcomes. This setup addresses the limitations of traditional multilayer perceptrons in retaining past information, particularly in forecasting tasks.</text:p><text:p text:style-name="P122"/><text:p text:style-name="P122">Proposes a unique approach to neural network training by incrementally expanding the network size to match the increasing time series length. This self-growing mechanism adapts to new data, facilitating more effective forecasting.</text:p><text:p text:style-name="P122"/><text:p text:style-name="P122">Explores modifications to the error backpropagation algorithm. It suggests deactivating random neurons during weight correction and swapping weights or activation functions within and between neural networks to introduce training noise and avoid local optima.</text:p><text:p text:style-name="P122"/><text:p text:style-name="P122">These modifications address specific challenges in neural network training, from improving mathematical properties to enhancing memory retention and adapting network size dynamically for better performance in time series forecasting. Each modification offers a unique perspective to optimize artificial neural networks for various applications and tasks.</text:p><text:p text:style-name="P121"><text:soft-page-break/></text:p><text:h text:style-name="P107" text:outline-level="1"><text:bookmark-start text:name="__RefHeading___Toc2345_1821873452"/>4 Evolutionary Algorithms in Games Combinatorial Problems<text:bookmark-end text:name="__RefHeading___Toc2345_1821873452"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P81">Frequently, games present tasks characterized by combinatorial complexity. This inherent quality renders them captivating to individuals of all ages, sparking both intrigue and intellectual engagement. Beyond their recreational appeal, games have spurred the development of a field of study known as game theory. This discipline finds applications in a diverse array of domains, including economics, military strategy, the entertainment industry, and various other facets of life.</text:p><text:p text:style-name="P81"/><text:p text:style-name="P81">Challenges imbued with combinatorial complexity often necessitate exploring solutions through exhaustive search, dynamic optimization techniques, and, most notably, the employment of heuristics. These heuristic approaches become particularly invaluable when dealing with expansive search spaces, aiding the quest to unravel complex problems.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc3265_968957226"/>4.1 Solving Combinatorial Puzzles with Parallel Evolutionary Algorithms<text:bookmark-end text:name="__RefHeading___Toc3265_968957226"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P82"><text:soft-page-break/>Rubik&apos;s Cube, a brain-teasing puzzle, was conceived and introduced to the world by Erno Rubik in the 1970s. Since its inception, it has captured the fascination of puzzle enthusiasts globally. The original Rubik&apos;s Cube comprises 3×3×3 cubical segments, each adorned with stickers in six distinct colors on the exposed sides. These six planes (3×3×1) can be manipulated through rotations of 90, 180, 270, or 360 degrees relative to other parts of the puzzle. In its pristine state, all sides of the cube display a single color.</text:p><text:p text:style-name="P82"/><text:p text:style-name="P82">The challenge arises when the puzzle is scrambled through a series of random rotations of the (3×3×1) sides, creating a staggering number of possible combinations. Due to its sheer complexity, restoring the cube to its original state becomes a formidable combinatorial optimization task. To put this into perspective, there are approximately 4.3252×10^19 combinations, a figure elucidated by (Korf [32]), and every one of these combinations can be reached from any initial configuration. The ultimate objective is to arrive at a sequence of moves that aligns all sub-cubes based on their colors, effectively solving the puzzle. According to Korf&apos;s estimation, these resolution sequences typically range from 50 to 100 moves when the cube is thoroughly scrambled.</text:p><text:p text:style-name="P82"/><text:p text:style-name="P82">When confronted with an optimization problem that involves a sequence of commands, evolutionary algorithms emerge as ideal candidates for <text:soft-page-break/>finding optimal or sub-optimal solutions. In this context, we explore the application of parallel genetic algorithms to tackle the Rubik&apos;s Cube challenge. Moreover, we can enhance the evaluation function by incorporating the Hausdorff distance component, further refining the search for optimal solutions.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P83">Genetic algorithms are global optimization strategies inspired by the theories of biological evolution. The role of genetic algorithms and genetic programming within the realm of meta-heuristics is well illustrated in Figure 23.</text:p><text:p text:style-name="P83"/><text:p text:style-name="P83"><text:soft-page-break/><draw:frame draw:style-name="fr2" draw:name="Frame23" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="5.9126in" style:rel-height="scale-min" draw:z-index="44"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image23" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="4.6098in" style:rel-height="scale" draw:z-index="45"><draw:image xlink:href="Pictures/1000000100000276000002430991FFA5714E2F9F.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure22" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">23</text:sequence>: Euler diagram depicting the various classifications of metaheuristics ( https://upload.wikimedia.org/wikipedia/commons/c/c3/Metaheuristics_classification.svg )</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P83"/><text:p text:style-name="P83">In this approach, solutions to a specific problem are represented as vectors of values within the solution space. These selected solution <text:soft-page-break/>vectors collectively constitute the algorithm&apos;s population. The most common method for establishing the initial population is generating random vectors. Each new generation emerges in the population following the recombination of selected individuals. In genetic algorithms, recombination is achieved through two consecutive operators: crossover and mutation. The application of the selection operator determines the selection of individuals for the recombination process. It is pretty common to apply the elitism rule during the selection process. Elitism ensures that a certain percentage of the best-found solutions persist until the very end of the optimization process. A stopping criterion is required since genetic algorithm-based optimization is an iterative process. The most commonly used stopping criterion is a predetermined number of generations for the genetic algorithm.</text:p><text:p text:style-name="P83"/><text:p text:style-name="P83">Genetic algorithms serve as the foundation for genetic programming in this research. In this context, each element of the solution vector represents an operation applied to the state of the Rubik’s cube. An ordered sequence of such instructions constitutes an algorithmic program. Genetic algorithms are particularly suitable for implementation in parallel or distributed computing environments because there is no direct intermediate relationship between individuals within a given population. The genetic algorithm&apos;s population can be easily divided into numerous sub-populations, distributed across multiple processors/cores, <text:soft-page-break/>or even heterogeneous computers within a cluster. The preferred approach involves partitioning the global population. However, in cases where only the fitness value calculation is time-consuming, the population is retained on the central processor/computer, with only fitness value calculations being dispatched to other contributing processors/computers.</text:p><text:p text:style-name="P33"/><text:p text:style-name="P33">When a sub-population distribution calculation scheme is selected, a strategy for individual migration should be implemented. Migration between different islands is necessary for the best-found solutions to be available in some or all sub-populations. When the implementation of the calculation is organized as a donated distributed computing project and a new remote contributing computer is included, a fresh subset of the global population can be provided. With such a strategy, solutions are much more thoroughly investigated, resulting in improved space exploration.</text:p><text:p text:style-name="P33"/><text:p text:style-name="P35">The core of the optimization code revolves around the representation of the Rubik’s Cube in computer memory. For this concept, the cube is depicted using six two-dimensional arrays, one for each side, each with dimensions of 3x3. These arrays&apos; values are integer numbers corresponding to the cube&apos;s colors. While more advanced digital <text:soft-page-break/>representations exist, as suggested by (Korf [32]), this approach proves to be more practical from an algorithmic perspective.</text:p><text:p text:style-name="P35"/><text:p text:style-name="P35">Data structures constitute the first aspect of the modeling process, while the second aspect involves the algorithmic operations carried out on these data structures. Since the cube has six sides, the minimum number of operations needed for manipulating the cube is six. These operations are denoted by six capital letters, as proposed by (Randall [33]):</text:p><text:p text:style-name="P35">-T (Top) - 90° clockwise rotation of the top side.</text:p><text:p text:style-name="P35">-L (Left) - 90° clockwise rotation of the left side.</text:p><text:p text:style-name="P35">-B (Back) - 90° clockwise rotation of the back side.</text:p><text:p text:style-name="P35">-R (Right) - 90° clockwise rotation of the right side.</text:p><text:p text:style-name="P35">-F (Front) - 90° clockwise rotation of the front side.</text:p><text:p text:style-name="P35">-D (Down) - 90° clockwise rotation of the down side.</text:p><text:p text:style-name="P35"/><text:p text:style-name="P35">This set of six operations forms the minimal, fully functional grammar of the Rubik&apos;s Cube. Extended grammars are also possible by including counterclockwise operators (+T, +L, +B, +R, +F, +D, -T, -L, -B, -R, -F, -D).</text:p><text:p text:style-name="P35"/><text:p text:style-name="P35">The next level of grammar extension involves the addition of the number of turns (+1T, +2T, +3T, +1L, +2L, +3L, +1B, +2B, +3B, +1R, +2R, +3R, +1F, +2F, +3F, +1D, +2D, +3D, -1T, -2T, -3T, -1L, -2L, -3L, -1B, -2B, -3B, -1R, -2R, -3R, -1F, -2F, -3F, -1D, -2D, -3D).</text:p><text:p text:style-name="P34"><text:soft-page-break/></text:p><text:p text:style-name="P34">With these proposed ideas for a formal Rubik’s Cube grammar, the preferred approach is to represent genetic algorithm individuals as formal grammar sentences with variable lengths. Each letter can appear at any position and can be repeated multiple times within the chromosome. As mentioned in (Korf [32]), the expected average length of the chromosomes can range between 50 and 100.</text:p><text:p text:style-name="P34"/><text:p text:style-name="P34">A single cut point is selected for population crossover, although other options, as suggested by (Poli [34]), are also applicable. As a mutation operator, random changes to a single instruction are chosen. Randomly selected parents are used for selection, but the elitism rule is applied. The instructions encoded within the individual are applied to a scrambled cube to evaluate the newly created individuals. Subsequently, the cube&apos;s state is compared to the target state (a solved cube). For each pair of cube sides, the Euclidean distance is calculated. Following this, the maximum of the minimum distances is determined based on the rules of the Hausdorff distance. The resulting fitness value is positive because the Euclidean distance is calculated using positive integers (mapping the cube&apos;s colors to integers), and the Hausdorff distance is a calculation of the maximum of the minimums. A smaller fitness value indicates a better solution to the puzzle.</text:p><text:p text:style-name="P33"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc3267_968957226"/><text:soft-page-break/>4.2 Virtual Reels in Slot Machines<text:bookmark-end text:name="__RefHeading___Toc3267_968957226"/></text:h><text:p text:style-name="P33"/><text:p text:style-name="P36">In gambling, slot machines are one of the most prevalent forms of betting entertainment. In today&apos;s digital age, these machines come in two primary formats: standalone electronic devices and web applications accessible over the Internet. Modern slot machines, while preserving the classic charm, have undergone a transformation. They now operate with virtual reels that spin and halt at randomly selected positions, with only segments of these virtual reels visible on the screen.</text:p><text:p text:style-name="P36"/><text:p text:style-name="P36">The key to winning lies in forming specific combinations based on preset patterns, with each symbol carrying its own weight in the process. The rarest symbols hold the promise of the highest payouts, while the more common ones yield comparatively more minor rewards. Achieving the proper arrangement of symbols on the virtual reels is a discrete, combinatorial challenge, intending to meet predetermined statistical parameters like return to player and volatility.</text:p><text:p text:style-name="P36"/><text:p text:style-name="P36">Given the intricate nature of this task, meta-heuristic algorithms have found extensive application in optimizing these arrangements. These algorithms ensure slot machines deliver thrilling and potentially lucrative gaming experiences that keep players returning for more.</text:p><text:p text:style-name="P33"/><text:p text:style-name="P37"><text:soft-page-break/>The primary focus of this proposition revolves around the distribution of symbols on the machine reels. Each reel contains ordered symbols, typically represented with numerical values in mathematical models. The sequence in which these symbols appear on the reel follows a discrete probability distribution. Different winning combinations can emerge on the game screen based on this distribution. Mathematicians responsible for designing slot machine reels have the task of selecting a discrete probability distribution that yields a specific RTP value. RTP is determined by dividing the money won by the lost and multiplying the result by one hundred. The RTP value holds significant importance for slot machine vendors as it is a crucial gambling parameter subject to government regulation. The RTP value carries mathematical significance as an expected value. While this discussion excludes volatility, another notable parameter in slot machine games, it is worth mentioning that it is less frequently considered during government regulatory processes. The proposed approach for optimizing the discrete distribution of symbols could prove beneficial for mathematicians developing new slot gambling games.</text:p><text:p text:style-name="P37"/><text:p text:style-name="P37">Slot machines operate on the fundamental concept of spinning reels. Initially, slot reels were mechanical, and players manually pulled the game handle to initiate the spin. Today, most slot machines employ computerized game reels that exist solely in a virtual realm, with stops <text:soft-page-break/>being selected by a Random Number Generator (RNG) (Brysbaert [35]), which provides more information. In most games, five independent reels are presented, although there are variations with three or more than five reels. When the push button is pressed, the reels begin to spin, with each one coming to a stop sequentially. The player&apos;s winnings are determined based on combinations of symbols displayed on the screen. Each game features its unique paytable, typically accessible to the player on a separate screen. Certain symbols appear more frequently on the reels than others. Less common symbols lead to winning combinations less frequently, resulting in larger payouts for the player.</text:p><text:p text:style-name="P37"/><text:p text:style-name="P37">The primary characteristic defining each slot machine is its RTP percentage. This parameter is calculated by taking the ratio of winnings to losses and multiplying it by one hundred. RTP percentages can range from 80% in Las Vegas to as high as 98% in some EU member states. In the United Kingdom, fruit machines are legally mandated to pay out a minimum percentage within a short time-frame (Parke [36]), which offers further insights). Typically, RTP percentages exceed 90%. Achieving the desired RTP necessitates collaboration between mathematicians and game designers to strategically populate game reels with appropriate symbols according to a discrete distribution (Osesa [37]), providing additional information.</text:p><text:p text:style-name="P33"/><text:p text:style-name="P38"><text:soft-page-break/>Genetic algorithms, as search heuristics inspired by the process of natural selection (Eiben [38], Ting [39]), can be efficiently employed in problem-solving. They are routinely used to generate points (candidate solutions) within the solution space. By applying techniques such as inheritance (crossover), mutation, and selection, these generated points can move closer to the optima. GAs are also classified as population-based algorithms because each point in the solution space represents an individual within the GA population. Each individual possesses a set of properties that are subject to mutation and modification, usually through crossover. The traditional representation of these properties is binary, using a sequence of 0s and 1s, although other encodings, such as binary trees, are also available.</text:p><text:p text:style-name="P38"/><text:p text:style-name="P38">The optimization process typically begins with a randomly generated population of individuals but is subject to implementation details. This optimization process is iterative, with each iteration referred to as a generation. For each individual within a generation, a fitness value is calculated. This fitness value typically corresponds to the objective function under optimization. The fittest individuals within the population are selected according to a specified selection rule and are then recombined through crossover and/or mutation to form a new generation. This new generation is subsequently used in the next iteration of the algorithm. Algorithm termination is usually triggered by reaching a <text:soft-page-break/>maximum number of generations or attaining the desired level of fitness value.</text:p><text:p text:style-name="P38"/><text:p text:style-name="P38">To run GAs successfully, two key components need to be provided:</text:p><text:p text:style-name="P38">1. Genetic representation of the solution space (solution domain).</text:p><text:p text:style-name="P38">2. An appropriate fitness function for evaluating the solution domain.</text:p><text:p text:style-name="P38"/><text:p text:style-name="P38">Once these two conditions are met, GAs can proceed with population initialization and iterative population improvement by repetitively applying selection, crossover, mutation, and individual evaluation.</text:p><text:p text:style-name="P33"/><text:p text:style-name="P39">In the optimization model, each individual consists of slot machine symbols represented as numbers distributed on the reels. Each symbol on the reels corresponds to a single integer number, and both the symbol and its position on the reel hold significance. The solution domain is finite and discrete, with each position on each reel being a single integer number from a list of possible game symbols.</text:p><text:p text:style-name="P39"/><text:p text:style-name="P39">Population initialization is typically achieved through random generation. However, the proposed model employs an initial reels configuration to initiate the population. This initialization process involves introducing random noise to the initial reels configuration. The population size varies <text:soft-page-break/>and is determined through experimental estimation, ranging from several individuals to hundreds or even thousands.</text:p><text:p text:style-name="P39"/><text:p text:style-name="P39">During the selection process, individuals are chosen based on their fitness values. Some selection methods favor the best individuals, while others prefer a random population subset. The fitness function is problem-specific and is defined over the genetic representation as a measure of the quality of the represented solution. The fitness function calculates the absolute difference between the desired RTP and the obtained RTP in the proposed model. To calculate the obtained RTP, Monte-Carlo simulation is utilized to estimate the behavior of the slot machine in either 100,000 or 1,000,000 separate runs. The elitism rule is also applied to ensure that the best individual survives between generations.</text:p><text:p text:style-name="P39"/><text:p text:style-name="P39">A pair of parent individuals are selected from the chosen population subset for the crossover operation. A single-point cut is employed to recombine attributes of the first and second parent to create a child individual. Further research is needed to determine if using more than two individuals as parents is advantageous. Following the crossover, mutation is applied to the child by randomly selecting a symbol and replacing it with another randomly selected symbol.</text:p><text:p text:style-name="P39"/><text:p text:style-name="P39"><text:soft-page-break/>Termination criteria include a maximum number of generations and manual observation/termination of the process. The final solution, discovered through genetic algorithms, is an integer vector. For instance, in the case of a slot game with 5 reels (displayed on the screen as 5 columns and 3 rows), each reel having 63 symbols, the GA&apos;s final solution would be an integer vector with 5x63 values.</text:p><text:p text:style-name="P33"/><text:p text:style-name="P40">In 1984, Inge Telnaes received a patent for a device titled &quot;Electronic Gaming Device Utilizing a Random Number Generator for Selecting the Reel Stop Positions&quot;, US Patent 4448419 (Inge [40]). Slot machines are the most popular casino gambling method, constituting approximately 70 percent of the average US casino income (Cooper [41]). The size of this market has led to the development of more advanced optimization approaches in game design, with one of the options being Discrete Differential Evolution. Such a discrete distribution of virtual reels can be achieved through discrete optimization, adhering to specified constraints such as desired RTP, prize equalization, and symbol diversity. Symbol diversity can be easily calculated without simulation based on these three criteria, while RTP and prize equalization require Monte Carlo simulations to be incorporated into the DDE cost function.</text:p><text:p text:style-name="P40"/><text:p text:style-name="P40">A multi-criteria cost function is transformed into a single criterion through linear transformation, with coefficients assigned to each criterion. The <text:soft-page-break/>decision maker selects coefficients according to their personal preferences. In this idea proposition, score 1 was assigned to symbol diversity, 100 to RTP, and 10 to prize equalization. These numbers were chosen in accordance with the relative importance of each criterion. Symbol diversity pertains to the arrangement of symbols of the same kind next to each other within a single reel. Achieving symbol diversity is straightforward through simple swaps of symbols that are in an inappropriate order. Slot machine reels are treated as individuals in DDE optimization, allowing for control over symbol diversity even before the Monte Carlo simulation. This cost function is preferred for this criterion to facilitate the exploration of the solution space.</text:p><text:p text:style-name="P33"/><text:p text:style-name="P42">Differential Evolution is one of the stochastic optimization algorithms. It addresses the following search problem: Minimizing an objective function, which represents a mapping from a parameter vector &apos;x&apos; in an n-dimensional real-value space to a one-dimensional real-value space. DE encompasses self-organization for mutation, crossover, and selection, but its strategy parameters are chosen empirically (Price [42]).</text:p><text:p text:style-name="P42"/><text:p text:style-name="P42">DE exhibits similarities to traditional evolutionary algorithms; however, it does not utilize binary encoding like a simple genetic algorithm (Goldberg [43]), and it does not rely on a probability density function to self-adapt its parameters, as is the case in Evolution Strategy (Hans-Paul [44]). DE <text:soft-page-break/>distinguishes itself through its mutation process, executed based on the distribution of solutions within the population. Consequently, search directions and potential step sizes are contingent on the selected individuals&apos; positions to compute the mutation values. The most widely recognized model is referred to as DE/rand/1/bin, where DE stands for Differential Evolution, &quot;rand&quot; signifies that individuals chosen to calculate the mutation values are selected randomly, &quot;1&quot; denotes the number of pairs of solutions chosen, and &quot;bin&quot; indicates the use of binomial recombination (Mezura-Montes [45]).</text:p><text:p text:style-name="P41"/><text:p text:style-name="P41">Like other Evolutionary Algorithms, DE cannot handle constrained optimization. In this context, a strict constraint stipulates that slot machine reels must adhere to specific criteria, which depend on the game rules. For instance, the reels must exclusively consist of valid symbols. This constraint is manually ensured after each new individual&apos;s reproduction, wherein a randomly selected valid symbol replaces each invalid symbol. This correction can be viewed as an addition to the mutation operation.</text:p><text:p text:style-name="P41"/><text:p text:style-name="P41">In this case, DDE is applied for goal optimization. The original DE is modified to compute the weighted difference vector using discrete values. Instead of a conventional difference, a normalized discrete difference vector is employed. This difference vector comprises three common values: minus one, zero, and plus one.</text:p><text:p text:style-name="P33"><text:soft-page-break/></text:p><text:p text:style-name="P43">DDE individuals are represented as 2D arrays of symbols, referred to as reels. All symbols are represented as integer numbers. Each DDE individual is a point in the solution space, a discrete finite space. To be considered valid, each reel should contain only integer numbers from the available symbols. In many game designs, symbols are numbered from 3 to 12. Symbols from 0 to 2 are typically reserved for special symbols known as wilds, while symbols between 14 and 16 are typically reserved for special symbols called scatters. For simplicity&apos;s sake, the slot machine&apos;s model is assumed not to have any wild or scatter symbols.</text:p><text:p text:style-name="P43"/><text:p text:style-name="P43">The model does not include free spins but a simple bonus game simulating a bingo game. There is a bonus prize for completing a bingo line and another for achieving bingo.</text:p><text:p text:style-name="P43"/><text:p text:style-name="P43">Population initialization is carried out by manually constructing reels. Some of the individuals are shuffled initially to introduce population diversity. The population size is determined experimentally and may range from a few individuals to hundreds or even thousands.</text:p><text:p text:style-name="P43"/><text:p text:style-name="P43">The first step of DDE optimization involves selecting a target vector, a base vector, and two other vectors to be used for the weighted difference vector. All four vectors are chosen randomly, a slight deviation from the <text:soft-page-break/>original DE algorithm. A discrete difference vector is calculated in the second step, with the only valid values minus one, zero, and plus one. The third step involves mutation, adding the difference vector to the base vector. Any invalid symbol numbers (in this case, 2 or 13) are randomly replaced with valid ones (ranging from 3 to 12). The fourth step entails a crossover operation between the base vector and the mutated vector, employing binomial crossover. The final fifth step is associated with calculating the fitness value and deciding which vector to retain for the next generation, either the target vector or the newly recombined one.</text:p><text:p text:style-name="P43"/><text:p text:style-name="P43">This constitutes a multi-criteria problem. Assigning weights to the three criteria are used as linear equations, effectively converting the problem into a single-criteria one. The decision maker is responsible for selecting the coefficients for each criterion. This research assigns a weight of 1 to symbol diversity, 100 to target RTP, and 10 to prize equalization. Monte-Carlo simulation is employed for estimating target RTP and prize equalization. Symbol diversity is directly calculated from the reels. To enhance accuracy in Monte-Carlo simulations, 10 sets of 1,000,000 separate slot game runs are executed.</text:p><text:p text:style-name="P43"/><text:p text:style-name="P43">The maximum number of recombinations is utilized as an optimization termination criterion. Manual observation and termination of the process are also possible. The final solution, obtained through DDE, is represented <text:soft-page-break/>as an integer matrix. This matrix can be directly applied as slot machine reel strips. For instance, if a slot game features 5 reels (visible on the screen as 5 columns and 3 rows), and each reel consists of 63 symbols, the final DDE solution would be an integer matrix with dimensions of 5x63 values.</text:p><text:p text:style-name="P33"/><text:p text:style-name="P44">In almost all cases, gambling games are mathematically unfair, meaning long-term players incur losses against the operator. This loss rate is quantified by the Return to Player (RTP) percentage, which typically ranges between 90% and 98% in many countries where gambling is legalized. However, exceptions exist, such as Nevada, where the RTP can be considerably lower.</text:p><text:p text:style-name="P44"/><text:p text:style-name="P44">The RTP carries significant statistical significance. For instance, if a player wagers $100 on a game with a 95% RTP, statistically speaking, in a single session, they can expect to receive $95 back. The RTP of a game is directly determined by the arrangement of symbols on virtual reels. From a mathematical perspective, there is no justification for keeping the distribution of symbols on the reels concealed from the players. It is well-established that gambling games are inherently mathematically biased, and legal regulators closely oversee all aspects of gambling games.</text:p><text:p text:style-name="P44"/><text:p text:style-name="P44"><text:soft-page-break/>Consequently, a player&apos;s knowledge of the virtual reels&apos; specific content does not give them any advantage. This situation is analogous to roulette, where players are fully aware of the order and colors of numbers. However, slot machines add an element of mystery by not disclosing the virtual reel configurations in the game rules.</text:p><text:p text:style-name="P44"/><text:p text:style-name="P44">If someone wishes to estimate a game&apos;s RTP without access to the original game source code, the only viable approach is to reconstruct the sequences of the reels from observed data chunks (Vaidyanathan [46]). This task can be time-consuming due to its highly combinatorial nature (Lewis [47]).</text:p><text:p text:style-name="P44"/><text:p text:style-name="P46">Sequence reconstruction is a frequently encountered problem in genetics. The essence of this issue lies in the challenge of reconstructing a complete sequence of ordered information when only fragments of it are known. In the realm of genetics, reconstruction pertains to sequences comprised of the four nucleotide bases: cytosine (C), guanine (G), adenine (A), and thymine (T). Sequence reconstruction also finds applications in other domains, such as cryptography and encoding. </text:p><text:p text:style-name="P46"/><text:p text:style-name="P46">In this study, an approximate reconstruction of sequences is pursued. The innovative approach involves searching for an optimal solution within the space of chunks rather than within the space of complete sequences.</text:p><text:p text:style-name="P44"><text:soft-page-break/></text:p><text:p text:style-name="P44">In most sequencing problems, the ultimate objective is the precise reconstruction of the analyzed sequence (Parsons [48]). However, achieving exact reconstruction is optional when dealing with virtual slot machine reels. It suffices for the reels to be reconstructed so that the player&apos;s subjective experience matches the original and reconstructed reels. This research proposes an approximate reconstruction of slot machine reels using genetic algorithms. The quality of the solutions provided by the genetic algorithm is assessed by calculating the Euclidean distance between the chunks of the candidate solution and those from the original sequences. The ultimate aim of this sequencing is to reconstruct virtual reels with properties identical to the original ones, even though an exact match between the reconstructed and original sequences may not be achieved.</text:p><text:p text:style-name="P33"/><text:p text:style-name="P45">The sequencing of virtual slot machine reels can be achieved through the exact reconstruction of the reels, but this level of precision is unnecessary. It suffices to achieve identical gameplay behavior with reconstructed reels in front of the players. When approximate reconstruction is applicable, the process commences with the collection of virtual reel chunk samples. In most cases, the length of the reel is not known in advance. In such instances, statistical analysis should be conducted to estimate the necessary number of samples for the most accurate reconstruction <text:soft-page-break/>possible. A histogram of the chunks can unveil the frequency of appearance for each observed chunk.</text:p><text:p text:style-name="P45"/><text:p text:style-name="P45">Chromosomes are encoded as candidate sequences derived from the set of possible game symbols within a particular reel. Chunk samples are extracted from the candidate sequence, mirroring the number taken from the original sequence. All candidate solution quality assessments are based on these sampled chunks.</text:p><text:p text:style-name="P45"/><text:p text:style-name="P45">Estimating fitness value involves calculating the average Euclidean distance for a sorted set of chunks in both the candidate and the original sequences as pairs. Sorting ensures that candidate chunks correspond to their respective original chunks when calculating the Euclidean distance between pairs. Original chunks are sorted only once when initially collected as samples. Candidate chunks are sorted each time the candidate sequence changes, such as crossover and/or mutation. To obtain the average value offered as chromosome fitness, all distances between chunk pairs are summed and divided by the number of chunks. The average Euclidean distance is negated, as lower fitness corresponds to a more significant deviation of the candidate solution from the original sequence. Embracing this fitness value estimation implies that the fitness value of the original sequence&apos;s distance from itself is zero, the highest possible fitness value.</text:p><text:p text:style-name="P45"><text:soft-page-break/></text:p><text:p text:style-name="P45"><draw:frame draw:style-name="fr2" draw:name="Frame24" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="4.0783in" style:rel-height="scale-min" draw:z-index="46"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image24" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="3.6902in" style:rel-height="scale" draw:z-index="47"><draw:image xlink:href="Pictures/1000000000000404000002F5472EFE60EDB51585.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure23" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">24</text:sequence>: Original sequences of five virtual reels</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P45"/><text:p text:style-name="P45">Fitness value estimation relies on the chunk list, whereas crossover and mutation operate on candidate sequences. Any change in the candidate sequences prompts an immediate recalculation of chunk samples. Modified uniform crossover is employed, introducing a normal distribution (mean of 50% and standard deviation of 20%) for determining the participation rate of the two parents. This means that one of the parents has a more significant influence on offspring formation. Since the length of the original sequence is typically unknown, the offspring <text:soft-page-break/>sequence&apos;s length must be estimated during the crossover process. The number of unique symbols in the virtual reels determines the lower bound for candidate sequence length, while the total length of all chunks combined defines the upper bound. Estimating candidate sequence length enables the genetic algorithm to optimize this parameter.</text:p><text:p text:style-name="P45"/><text:p text:style-name="P45">In most cases, parents have varying lengths, resulting in offspring that differ in length from the parents. Values are adjusted iteratively, starting from the beginning when the offspring exceeds the parents&apos; length. This approach naturally produces longer offspring, as virtual reels are used in looping during real-time gameplay. Mutation involves randomly replacing a single value in the candidate sequence with the random value drawn from the chunks of the original sequence.</text:p><text:p text:style-name="P45"/><text:p text:style-name="P45"><text:soft-page-break/><draw:frame draw:style-name="fr2" draw:name="Frame25" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="4.0783in" style:rel-height="scale-min" draw:z-index="48"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image25" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="3.6902in" style:rel-height="scale" draw:z-index="49"><draw:image xlink:href="Pictures/1000000000000404000002F557A75994DDEDD677.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure24" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">25</text:sequence>: Reconstructed sequences of five virtual reels</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P45"/><text:p text:style-name="P45">For selection, three different chromosomes are randomly chosen. The two with better fitness among the three are selected as parents. The third chromosome is chosen for removal from the population, allowing the newly generated offspring only if it demonstrates superior fitness. This selection operator indirectly enforces the elitism rule.</text:p><text:p text:style-name="P33"/><text:p text:style-name="P33">Genetic algorithms combinatorially arrange the individual chunks of the game&apos;s reels. This approach is stochastic, and arriving at a possible solution is a purely probabilistic process. Additionally, it is possible to <text:soft-page-break/>propose a significantly more deterministic approach based on tree structures to generate all potential arrangements according to the observed chunks for reel reconstruction. </text:p><text:p text:style-name="P33"/><text:p text:style-name="P33">The task of reel reconstruction is quite similar to the task of constructing new reels. The primary difference lies in the observations of the game screen and the requirement for the observed patterns to be accurately represented in the reconstructed reels.</text:p><text:p text:style-name="P33"/><text:p text:style-name="P47">The primary objective of this proposition is to infer the discrete probability distribution of symbols across virtual reels. Each virtual reel comprises symbols, typically represented as numbers in the programmatic context. The arrangement of these symbols is a critical aspect of game design, often undertaken by statisticians, although not exclusively (Keremedchiev [49]). Mathematicians responsible for designing slot machine games must carefully select a symbol distribution that yields a specific RTP. It is worth noting that symbol distribution impacts RTP and influences game volatility and the overall gameplay experience, which is essential for player engagement.</text:p><text:p text:style-name="P47"/><text:p text:style-name="P47">The RTP of a game is calculated by dividing the total winnings by the total losses and then multiplying the result by one hundred to express it as a percentage. RTP is a standard slot machine characteristic and is subject to <text:soft-page-break/>government laws and regulations. In mathematical terms, RTP represents an expected value. In this research, RTP holds particular significance because it provides the sole means of assessing the accuracy of reconstructed reels. Even though the specifics of the reels are kept confidential as trade secrets, information about the paytable, betting lines, and the RTP of each game is widely available. The primary objective in reconstructing the reels is to ensure that the outcomes closely align with the known RTP.</text:p><text:p text:style-name="P33"/><text:p text:style-name="P48">In the literature, Monte-Carlo Search is better known as Monte-Carlo Tree Search (MCTS). It focuses on analyzing the most promising moves, using random search space sampling to expand the search tree. We will use the term Monte-Carlo Search because, in the case of this research, the structure of optimization is a graph structure, not a tree structure.</text:p><text:p text:style-name="P48"/><text:p text:style-name="P48">MCS is a heuristic search algorithm in the decision-construction process. It has everyday applications in gameplay. MCS is used in cases where building a graph is time-consuming, rendering all exact number algorithms inefficient. Games like Chess and Go are the most renowned examples of MCS usage. MCS is applicable not only in deterministic games but also in games like Poker.</text:p><text:p text:style-name="P48"/><text:p text:style-name="P48"><text:soft-page-break/>The foundation of MCS lies in analyzing the most promising further constructions. Search options are expanded via random sampling of the search space. The application of MCS in the process is based on numerous attempts. The reel is reconstructed to the maximum allowed length in each run by randomly selecting observed patterns. Each reel reconstruction is assessed based on well-defined reel start and end points and rare, game-dependent patterns.</text:p><text:p text:style-name="P48"/><text:p text:style-name="P48">At the proposed algorithm&apos;s core is screen pattern classification and the utilization of these patterns as operations in finite-state machines.</text:p><text:p text:style-name="P48"/><text:p text:style-name="P48">As input for the algorithm, screen observations are employed. Each reel is observed a specified number of times (approximately 600 times through experimentation). Observed patterns can be created manually, but image processing algorithms can be employed for automated symbol identification. The slot machine screens&apos; images are relatively static, making them ideal candidates for image processing procedures.</text:p><text:p text:style-name="P48"/><text:p text:style-name="P48">The data input processing follows these steps:</text:p><text:p text:style-name="P48">1. Input data as a sequence of characters (observed patterns).</text:p><text:p text:style-name="P48">2. Identify unique patterns and represent them as nodes in the transitions graph.</text:p><text:p text:style-name="P48"><text:s text:c="4"/>2.1 For each node, check for matches with others.</text:p><text:p text:style-name="P48"><text:soft-page-break/><text:s text:c="4"/>2.2 If a match is found, store it as an edge in the transitions graph.</text:p><text:p text:style-name="P48"><text:s text:c="4"/>2.3 If no match is found, repeat step 2.1.</text:p><text:p text:style-name="P48">3. Calculate symbol frequencies in observed patterns.</text:p><text:p text:style-name="P48">4. Initialize the reel start with a known starting pattern.</text:p><text:p text:style-name="P48"><text:s text:c="4"/>4.1 Select a random transition based on the current reel ending.</text:p><text:p text:style-name="P48"><text:s text:c="4"/>4.2 Repeat until the reel reaches its maximum length or the end pattern is encountered.</text:p><text:p text:style-name="P48">5. Evaluate and store the generated solutions.</text:p><text:p text:style-name="P48">6. Return to step 4 until a predefined time is reached.</text:p><text:p text:style-name="P48">7. Finish.</text:p><text:p text:style-name="P48"/><text:p text:style-name="P48">In step 2.1, one pattern matches the end of another found at the beginning of the second pattern. Symbol frequencies in observed patterns (step 3) are employed as statistical estimations for the quality of the generated solution. If the frequencies of symbols in the generated reel closely resemble the frequencies of observed patterns, the generated reel should be closer to the original one. Due to less frequent symbols in the reels, each virtual reel can be described with a well-known pattern for the start and end. These patterns typically revolve around special symbols. Well-known patterns can be checked between the start and end, serving as control sequences. In step 5, three types of evaluation are performed. First, symbol frequencies are calculated, followed by the Euclidean distance between the two frequency vectors. Subsequently, the count of <text:soft-page-break/>missing observed patterns in the generated reel is determined. Finally, the generated pattern is dissected into pieces of the size of the observed patterns, and each piece is checked against the transitions nodes list. If a piece is absent from the list, it indicates that the reel contains a piece not observed during the data collection stage.</text:p><text:p text:style-name="P33"/><text:p text:style-name="P49">In computerized slot machines, apart from the base game, additional features like free spins and various bonus games are often added. The RTP, expressed as a percentage, is determined by multiplying the ratio of the total amount won to the total amount bet by one hundred (Kamanas [50]). This RTP indicator represents the mathematical expectation of winnings. It indicates how many out of 100 bets can be expected to be won, on average, during one spin of the reels. For example, in a game with a 98% RTP, betting 100 coins yields an average return of 98 coins as winnings.</text:p><text:p text:style-name="P49"/><text:p text:style-name="P49">Slot machines are inherently skewed in favor of the house, offering players lower odds than other casino games. Different regulatory authorities worldwide allow for a range of RTP values, typically from 75% to 98%. It is economically impractical for gambling operators to set an RTP exceeding 100%, as this would result in the casino losing more money than it gains.</text:p><text:p text:style-name="P49"/><text:p text:style-name="P49"><text:soft-page-break/>In addition to RTP, each slot machine is characterized by its volatility. While legal regulators closely oversee RTP, volatility is not always mandated or monitored. Volatility in a game dictates how frequently wins occur and their size. Slot machines are often designed to align with the social preferences of the region in which they are distributed and played. For instance, American players favor high-volatility games, which offer infrequent but substantial wins. In Eastern Europe, on the other hand, players typically prefer low-volatility games, where wins occur more frequently but are smaller in size. The volatility of a game is often rated on a scale of one to five stars, indicating low to high volatility.</text:p><text:p text:style-name="P49"/><text:p text:style-name="P49">Various approaches can be employed to address different forms of uncertainty, such as the volatility inherent in gambling games. Some of these approaches rely on established optimization strategies proposed by Wald, Laplace, Hurwitz, and Savage. In certain situations, heuristic methods can also prove effective. A comprehensive review of uncertainty measures in evidence theory and an analysis of related controversies can be found in (Deng [51]). In cases where prior information is available regarding parameters, uncertainty can be quantified by considering this prior knowledge, as demonstrated in (Patra [52]).</text:p><text:p text:style-name="P33"/><text:p text:style-name="P50">The volatility of a slot machine becomes a matter of interest when the gambling product undergoes scrutiny from regulators or testing from <text:soft-page-break/>competing manufacturers. In the legal regulation of slot machines, manufacturers are required to provide comprehensive and detailed documentation for each product. This documentation should include precise values for the virtual reels. With access to the paytable, virtual reel data, winning combinations, and rules governing free spins and bonus games, calculating the RTP and volatility involves combinatorial calculations. However, only the pay-out table, pay lines/patterns, free spins rules, and bonus game rules are known when the manufacturer&apos;s documentation is unavailable. Determining RTP and volatility necessitates empirical research when the virtual reels remain undisclosed.</text:p><text:p text:style-name="P50"/><text:p text:style-name="P50">A rough estimate of the RTP value can be made by repeatedly initiating reel spins. Some research laboratories, such as GLI, approximate RTP by playing 14000 spins, considering the total winnings and total bets, and then calculating the ratio between the two. Volatility can be calculated as an estimate of deviation from the central tendency using the following formula:</text:p><text:p text:style-name="P50"/><text:p text:style-name="P50">V = C * sqrt( sum( Hi * (Pi-R)^2 ) / N )</text:p><text:p text:style-name="P50"/><text:p text:style-name="P50">Here, V represents the volatility index, C denotes the confidence interval, Hi signifies the frequency of the i-th winning combination, Pi stands for the value of the i-th win, R is the value of RTP, and N is the number of <text:soft-page-break/>spins. It is worth noting that volatility is calculated by playing only one pay line, typically the central one. Most modern slot machines are rarely played with a single pay line, and in some games, a single-line configuration is impossible.</text:p><text:p text:style-name="P50"/><text:p text:style-name="P50">In the most common game screen configuration, slot machines feature 5 reels and 3 rows, making 15 symbols (5x3) visible on the screen, allowing for various pay-out patterns. When the exact virtual reels are known, it is possible to collect sufficient statistics through Monte Carlo simulations to estimate volatility. However, when virtual reels remain unknown, information must be gathered through multiple observations of different spins and stops. Stopping the virtual reels reveals authentic snippets of the symbol order. Often, the animation simulating rotating reels does not provide genuine information about the symbols&apos; locations but images designed to create the illusion of rotation. Hence, the only reliable and authentic information is the game screen state after the spin has stopped. Additionally, it is essential to emphasize that games must be explored in real mode with actual bets, as demonstration modes frequently employ different virtual reels.</text:p><text:p text:style-name="P50"/><text:p text:style-name="P50">Monte Carlo, simulations of slot machines entail stopping the virtual reels at randomly selected positions while considering the symbols before and after the random stop. In the absence of complete information about the <text:soft-page-break/>virtual reels, observing the segments obtained from different reel stops is proposed. These resulting segments provide a statistical insight into the actual symbol positions. In the simulation, the playing screen is not constructed from actual reels. Instead, segments are chosen based on their encounter frequency.</text:p><text:p text:style-name="P125"/><text:h text:style-name="P111" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc7834_527140494"/>Summary<text:bookmark-end text:name="__RefHeading___Toc7834_527140494"/></text:h><text:p text:style-name="P125"/><text:p text:style-name="P126">Chapter 4 focuses on tackling the complexity of combinatorial puzzles, particularly delving into the world of Rubik&apos;s Cube. The chapter starts by highlighting the allure of games and their ties to combinatorial problems, emphasizing their relevance in various fields, from economics to entertainment.</text:p><text:p text:style-name="P126"/><text:p text:style-name="P126">The Rubik&apos;s Cube is an emblematic example of a combinatorial puzzle due to its staggering number of possible combinations, making it a challenging optimization task. The chapter explores the application of parallel evolutionary algorithms, specifically genetic algorithms, to solve the Rubik&apos;s Cube problem. These algorithms mimic biological evolution, representing solutions as vectors within a solution space. They undergo recombination through operations like crossover and mutation, aided by a selection process that ensures the persistence of the best solutions.</text:p><text:p text:style-name="P126"><text:soft-page-break/></text:p><text:p text:style-name="P126">The chapter delves into the technicalities of implementing genetic algorithms for solving the Rubik&apos;s Cube, including parallel computing strategies, population distribution, and individual migration. It explains the representation of the cube in computer memory using data structures and defines a minimal grammar for cube operations.</text:p><text:p text:style-name="P126"/><text:p text:style-name="P126">The approach proposes representing individuals in genetic algorithms as formal grammar sentences, allowing variable lengths and various operations to manipulate the cube. It details evaluation methods using Euclidean and Hausdorff distances to measure the fitness of solutions, aiming for a smaller fitness value as an indicator of a better solution.</text:p><text:p text:style-name="P126"/><text:p text:style-name="P126">The chapter delves into the realm of slot machines, focusing on their evolution from mechanical devices to modern digital interfaces, emphasizing the crucial role of virtual reels in determining gameplay outcomes. The chapter explores the discrete, combinatorial challenge of arranging symbols on these virtual reels to meet statistical parameters like return to player and volatility.</text:p><text:p text:style-name="P126"/><text:p text:style-name="P126">It highlights the use of meta-heuristic algorithms, particularly Genetic Algorithms, in optimizing symbol distributions on slot machine reels. The RTP value holds significance, as it is a crucial parameter subject to <text:soft-page-break/>government regulations and mathematical calculations, impacting the gaming experience and player returns.</text:p><text:p text:style-name="P126"/><text:p text:style-name="P126">The chapter delves into the mechanics of slot machines, explaining their operation with computerized reels determined by Random Number Generators. It emphasizes the significance of RTP percentages, varying across regions and subject to legal mandates, impacting player payouts.</text:p><text:p text:style-name="P126"/><text:p text:style-name="P126">Detailing the application of GAs in problem-solving, the chapter delineates the process, emphasizing the genetic representation of the solution space and fitness function for evaluating solutions. It elucidates the application of GAs in optimizing symbol distributions, ensuring RTP adherence through Monte Carlo simulations, and using an elitism rule for population survival.</text:p><text:p text:style-name="P126"/><text:p text:style-name="P126">The optimization model involves representing slot machine symbols as numbers distributed on reels, employing a finite and discrete solution domain. It outlines the population initialization process, selection methods based on fitness values, crossover operations, and termination criteria for the GA process.</text:p><text:p text:style-name="P126"/><text:p text:style-name="P126">Further, it discusses historical patents related to slot machines, the dominance of these machines in the gambling industry, and the <text:soft-page-break/>development of advanced optimization approaches in game design, touching upon concepts like Discrete Differential Evolution for achieving desired symbol distributions.</text:p><text:p text:style-name="P126"/><text:p text:style-name="P126">The chapter proposes a multi-criteria cost function for optimization, transforming diverse parameters into a single criterion through linear transformation. Symbol diversity, RTP, and prize equalization are vital considerations, with emphasis on symbol arrangement, RTP compliance, and balancing payouts.</text:p><text:p text:style-name="P126"/><text:p text:style-name="P126">The chapter beautifully elaborates on Differential Evolution and its adaptations for discrete optimization, focusing on the problem of inferring the discrete probability distribution of symbols across virtual reels in slot machines.</text:p><text:p text:style-name="P126"/><text:p text:style-name="P126">DE, a stochastic optimization algorithm, aims to minimize an objective function mapping a parameter vector &apos;x&apos; in an n-dimensional real-valued space. Unlike traditional evolutionary algorithms, DE does not use binary encoding and lacks a self-adaptive parameter probability density function. It emphasizes the mutation process based on the population&apos;s solution distribution, employing various strategies like DE/rand/1/bin.</text:p><text:p text:style-name="P126"/><text:p text:style-name="P126"><text:soft-page-break/>When applied to slot reel optimization, DE requires handling constraints such as ensuring slot reels exclusively contain valid symbols according to game rules. Discrete Differential Evolution adjusts DE for discrete value computation, representing individuals as 2D arrays of symbols with integer values and ensuring the symbols&apos; validity.</text:p><text:p text:style-name="P126"/><text:p text:style-name="P126">The algorithm operates through steps involving the selection of target and base vectors, mutation, crossover, fitness evaluation, and retention for the next generation. This process tackles multi-criteria optimization, converting it into a single-criteria problem using weighted coefficients for symbol diversity, target RTP, and prize equalization. Monte Carlo simulations estimate the target RTP and prize equalization, while symbol diversity calculations involve direct reel analysis.</text:p><text:p text:style-name="P126"/><text:p text:style-name="P126">Furthermore, the proposition dives into the challenge of reconstructing virtual slot machine reels. It outlines an approach involving genetic algorithms, assessing solutions based on the Euclidean distance between candidate and original sequence chunks. This approximation aims to replicate gameplay behavior rather than achieve an exact match between the reconstructed and original reels.</text:p><text:p text:style-name="P126"/><text:p text:style-name="P126">The chapter also touches on Monte Carlo Search, applying it to reconstruct virtual slot machine reels through pattern classification and <text:soft-page-break/>finite-state machines. It outlines a step-by-step process involving observed pattern identification, reel generation, and evaluation based on the quality of generated solutions.</text:p><text:p text:style-name="P126"/><text:p text:style-name="P126">Emphasizing the importance of return-to-player (RTP) percentages in slot machines, the chapter highlights how game design and symbol distributions impact RTP, influencing game volatility and player engagement. It discusses uncertainty in slot machine games, regulatory oversight, and the importance of accurate documentation for each product, particularly regarding virtual reel data and winning combinations.</text:p><text:p text:style-name="P126"/><text:p text:style-name="P126">Moreover, it delves into estimating RTP and volatility through empirical research, Monte Carlo simulations, and mathematical formulas. The volatility index calculation considers frequencies of winning combinations, their values, RTP, and the number of spins. It stresses the significance of genuine observation post-reel stop for authentic information about symbol positions.</text:p><text:p text:style-name="P126"/><text:p text:style-name="P126">Overall, the chapter lays out a comprehensive framework for applying parallel evolutionary algorithms, specifically genetic algorithms, to address the intricate combinatorial optimization presented by Rubik&apos;s Cube. The chapter provides a comprehensive exploration of employing <text:soft-page-break/>Genetic Algorithms in optimizing symbol distributions on slot machine reels, emphasizing the crucial role of RTP and proposing multi-criteria optimization strategies for enhancing gameplay experiences. The chapter explores optimization algorithms, their adaptation for slot machine reel distributions, the significance of RTP, volatility, and the complex yet crucial task of reconstructing virtual slot machine reels based on observed patterns and simulation.</text:p><text:p text:style-name="P125"/><text:h text:style-name="P107" text:outline-level="1"><text:bookmark-start text:name="__RefHeading___Toc2347_1821873452"/>5 Problems with Human Evaluation of Fitness Function<text:bookmark-end text:name="__RefHeading___Toc2347_1821873452"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P3">In some optimization tasks, achieving an acceptable solution cannot be determined through exact mathematical calculations but results from subjective human evaluation. An example of such tasks includes those where a picture&apos;s beauty or music&apos;s melody is evaluated. Human intuition is also something that modern computers cannot formalize yet. An example of such a situation is the intuition about whether the price of a specific exchange-traded commodity will rise or fall. People process information in both their conscious and subconscious minds. This highly complex processing leads to what is referred to as intuition. We specify a solution, and that solution turns out to be correct without us being able to explain why that particular solution is chosen. In such tasks involving evolutionary algorithms, there is no alternative but to create an evaluation function through a dialogue between humans and computing machines.</text:p><text:p text:style-name="P51"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc2207_1422372469"/><text:span text:style-name="T18">5.1 </text:span>Human Evaluation in Art<text:bookmark-end text:name="__RefHeading___Toc2207_1422372469"/></text:h><text:p text:style-name="P51"/><text:p text:style-name="P52"><text:soft-page-break/>Due to the high degree of formalism in music, many composers developed various compositional methods over the centuries. With computers capable of playing music, the concept of automatic music generation has garnered the attention of numerous researchers.</text:p><text:p text:style-name="P52"/><text:p text:style-name="P52">Many systems for generating musical scores existed long before the advent of computers. One such system was Mozart&apos;s Musikalisches Wurfelspiel, which utilized dice throws to select measures from a vast collection of small phrases randomly. When combined, these phrases created musical pieces that human players could perform. While these works were not composed using modern computers, Mozart employed rudimentary algorithmic techniques that are now commonly used with electronic binary digital computers since the Second World War.</text:p><text:p text:style-name="P52"/><text:p text:style-name="P52">The world&apos;s first computer-generated digital music originated in Australia, created by programmer Geoff Hill on the CSIRAC computer designed and built by Trevor Pearcey and Maston Beard. Subsequently, Iannis Xenakis was one of the earliest composers to write music with a computer. He developed programs in the FORTRAN language that generated numerical data, which he then transcribed into scores for traditional musical instruments. An example of his work is ST/48 from 1962. Although Xenakis could have composed this music manually, the computer&apos;s <text:soft-page-break/>computational power was essential for the complex calculations required to transform probabilistic mathematics into musical notation.</text:p><text:p text:style-name="P52"/><text:p text:style-name="P52">Computers have also been employed to mimic the music of renowned past composers like Mozart. One contemporary exponent of this technique is David Cope. He authored computer programs that analyze the works of other composers to produce new compositions in a similar style. His program, Experiments in Musical Intelligence, is famous for creating &quot;Mozart&apos;s 42nd Symphony&quot; and has significantly impacted composers such as Bach and Mozart. He also combines his creations with those of the computer in his own pieces.</text:p><text:p text:style-name="P52"/><text:p text:style-name="P52">Some critics argue that computers cannot produce music of the same quality as great composers. In contrast, others question whether this is the primary objective of producing music in this manner.</text:p><text:p text:style-name="P52"/><text:p text:style-name="P52">Joy Schoenberger&apos;s project (Schoenberger [53]) aims to evolve a compositional model with a genetic map based on the piece&apos;s characteristics to be composed. The rules of Western Tonal Theory dictate the fitness function, and composition occurs at the phrase level. The resulting song is a coherent work due to shared genotypes among its phrases with differing phenotypes.</text:p><text:p text:style-name="P52"/><text:p text:style-name="P52"><text:soft-page-break/>Bruce Jacob describes a composition process that blends the best of two extremes: traditional stochastic methods seen in M and Jam Factory and complex rule-based systems like EMI or Cypher. This approach achieves the simplicity of a stochastic process and the determinism of a rule-based system (Jacob [54]).</text:p><text:p text:style-name="P52"/><text:p text:style-name="P52">Scott Draves conducts exciting work in his project Electric Sheep (Draves [55]). Electric Sheep is a distributed screen-saver that harnesses idle computers into a render farm to animate and evolve artificial life forms called sheep. Users&apos; votes form the basis for the genetic algorithm&apos;s fitness function in the space of fractal animations. Users can also manually design sheep for inclusion in the gene pool. Electric Sheep serves as an amplifier of human collaborators&apos; creativity rather than a traditional genetic algorithm optimizing a fitness function.</text:p><text:p text:style-name="P52"/><text:p text:style-name="P52">The primary challenge for researchers is achieving global harmony. In Schoenberger&apos;s work, they achieved excellent local harmony (a few tones), but global results could have been more satisfying. In Draves&apos; project, they successfully achieved harmony at a global level, but it was applied to movie composition. A combined approach that incorporates the strengths of both methods is proposed for music improvisation through interaction between humans and computers.</text:p><text:p text:style-name="P52"/><text:p text:style-name="P53"><text:soft-page-break/>The primary challenge of achieving global harmony in composition can be addressed by employing Distributed Differential Evolution and utilizing user aesthetic evaluations. This is similar to the approach taken in the Electric Sheep project. The melody is not divided into separate components in the proposed model, as was done in (Unehara [56]). However, a more significant number of user evaluations are presented to tackle the issues described in (Unehara [56]).</text:p><text:p text:style-name="P53"/><text:p text:style-name="P53">The proposed model aims to advance existing Genetic Algorithm models, as presented by (Unehara [56]) and (Mitchell [57]), for music composition. This advancement involves the integration of distributed DE to enhance convergence and the inclusion of human aesthetic factors gathered from individuals worldwide, as demonstrated in the works of (Takagi [58]) and (Wiggins [59]).</text:p><text:p text:style-name="P53"/><text:p text:style-name="P53">Musical improvisation has played an integral role in music since its inception, remaining a vital element in Western Classical music for over a thousand years. It is also prominent in various traditional music genres, including flamenco, pygmy, and other African music, as well as Eastern classical music, such as Carnatic, Hindustani, and Arabic. Additionally, it has found its place in modern music genres like rap. Moreover, it has been a fundamental aspect of European classical music.</text:p><text:p text:style-name="P53"/><text:p text:style-name="P53"><text:soft-page-break/>Human-Computer Interaction (HCI), sometimes called Computer-Human Interaction, symbolized as Χ χ Chi from the Greek alphabet, is the study of interactions between people (users) and computers. This interdisciplinary field connects computer science with various other areas of study and research. User-computer interactions occur at the user interface, encompassing software and hardware, including general-purpose computer peripherals and large-scale mechanical systems such as aircraft and power plants.</text:p><text:p text:style-name="P53"/><text:p text:style-name="P53">The General Musical Instrument Digital Interface (MIDI) is a synthesizer specification that imposes several requirements beyond the abstract MIDI standard. MIDI provides a protocol ensuring that different instruments can interoperate at a fundamental level, such as pressing keys on a MIDI keyboard, causing an attached MIDI sound module to play musical notes. However, General MIDI (GM) goes further in two ways: it mandates that all GM-compatible instruments meet specific minimum feature sets, including the ability to play at least 24 notes simultaneously (polyphony). GM also assigns specific interpretations to many parameters and control messages left unspecified in MIDI, defining instrument sounds for each of the 128 program numbers.</text:p><text:p text:style-name="P53"/><text:p text:style-name="P53">The Java Remote Method Invocation API, or Java RMI, is a Java application programming interface for performing remote procedure calls. Two <text:soft-page-break/>common interface implementations exist the original one, JRMP, and a version compatible with CORBA. The term &quot;RMI&quot; may refer solely to the programming interface or encompass the API and JRMP. In contrast, &quot;RMI-IIOP,&quot; which stands for &quot;RMI over IIOP,&quot; denotes the RMI interface, with most functionality delegated to the supporting CORBA implementation. The original RMI API was generalized to accommodate different implementations. Furthermore, work was undertaken on CORBA to add a pass-by-value capability supporting the RMI interface. However, it is essential to note that RMI-IIOP and JRMP implementations are somewhat different in their interfaces.</text:p><text:p text:style-name="P53"/><text:p text:style-name="P54">In the model, music information is presented as a sequence of MIDI events. This representation should be used for encoding in the implementation of DE.</text:p><text:p text:style-name="P54"/><text:p text:style-name="P54"><text:soft-page-break/><draw:frame draw:style-name="fr2" draw:name="Frame26" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="5.2882in" style:rel-height="scale-min" draw:z-index="50"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image26" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="4.9in" style:rel-height="scale" draw:z-index="51"><draw:image xlink:href="Pictures/1000000000000319000003061E3B29D22DFED0EC.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure25" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">26</text:sequence>: Fundamental RMI Client-Server architecture</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P54"/><text:p text:style-name="P54">The primary DE population should be located on the RMI server machine. The server machine should distribute the primary population to all connected RMI clients, as illustrated in Figure 26.</text:p><text:p text:style-name="P54"/><text:p text:style-name="P54"><text:soft-page-break/>On each client machine, the primary population should be used to generate the new DE generation by applying the basic DE operations, crossover, and mutation. Users will evaluate the fitness function by voting for different melodies based on their subjective opinions. Human aesthetic evaluation in distributed DE is the innovation proposed here. The actual DE selection should be made on the server-side machine according to the votes from the client-side machines. After this operation, a new primary population should be selected on the server-side machine. The process of distributed generation continues, and multiple clients can participate in the melody creation process, as shown in Figure 27.</text:p><text:p text:style-name="P54"/><text:p text:style-name="P54">The model is implemented as a Java RMI distributed application. Two programs are created: a server-side application and a client-side application. Server-side applications implement the data management algorithm, MIDI representation of the DE data, synchronization between the data produced by different clients, and primary population selection. Client-side applications implement all DE operations, including playing the MIDI sequences and user voting.</text:p><text:p text:style-name="P54"/><text:p text:style-name="P54">The data flow and calculation process follows these steps:</text:p><text:p text:style-name="P54"><text:s text:c="2"/>1. Initialize the primary DE population on the server.</text:p><text:p text:style-name="P54"><text:s text:c="2"/>2. Wait for clients to connect to the server.</text:p><text:p text:style-name="P54"><text:s text:c="2"/>3. Distribute the primary population to all connected clients.</text:p><text:p text:style-name="P54"><text:soft-page-break/><text:s text:c="2"/>4. Each client calculates its dataset.</text:p><text:p text:style-name="P54"><text:s text:c="2"/>5. Each user on the client side evaluates newly created MIDI sequences.<text:line-break/> <text:s/>6. The server collects all client-side calculations.</text:p><text:p text:style-name="P54"><text:s text:c="2"/>7. The server forms the new primary DE population.</text:p><text:p text:style-name="P54"><text:s text:c="2"/>8. Repeat step 3 or step 2 if no connected clients exist.</text:p><text:p text:style-name="P54"/><text:p text:style-name="P54">This experimental model aims to achieve better harmony in computer-generated music. It incorporates actual human aesthetic criteria, with expectations to improve upon the results achieved by other researchers.</text:p><text:p text:style-name="P54"/><text:p text:style-name="P54"><draw:frame draw:style-name="fr2" draw:name="Frame27" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="3.6382in" style:rel-height="scale-min" draw:z-index="52"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image27" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="3.25in" style:rel-height="scale" draw:z-index="53"><draw:image xlink:href="Pictures/100000000000036200000231C3F786A1AC5A613C.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure26" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">27</text:sequence>: Client-Server interaction</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P54"/><text:p text:style-name="P54"><text:soft-page-break/>Toward the end of this section, it is essential to consider the fundamental problem concerning the copyrights of all melodies generated in this manner. Who should be considered the author? Currently, machines cannot be authors. The person who created the system performed programming, data collection, and data manipulation but did not create the music. Users provided their subjective opinions but did not create the music either. This issue becomes even more complex with multiple users. Who, then, owns the copyrights in this case?</text:p><text:p text:style-name="P51"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc2209_1422372469"/><text:span text:style-name="T18">5.2 </text:span>Intuitive Human Evaluation<text:bookmark-end text:name="__RefHeading___Toc2209_1422372469"/></text:h><text:p text:style-name="P51"/><text:p text:style-name="P55">In the field of distributed computing (Thain [60]), there is a specialized branch that incorporates human brain capabilities to solve complex tasks, known as Human-Aided Computing (Shenoy [61]). Generally, distributed computing is employed for complex tasks where desktop computers and even supercomputers cannot produce results within an acceptable timeframe or computational cost (Anderson [62]). A problem is deemed suitable for a distributed computing system if it falls within the realm of parallel algorithms. This implies that different calculation steps should exhibit a certain degree of independence during execution (Hoare [63]). The primary distinctions between parallel computing and distributed computing lie in the fact that distributed systems are heterogeneous, <text:soft-page-break/>exhibit high network latency, lack a central clock, and, in most cases, consist of calculating nodes that are not controlled by a single entity (Mattson [64]).</text:p><text:p text:style-name="P55"/><text:p text:style-name="P55">Distributed computing becomes invaluable when the problems to be solved exceed the capabilities of modern computers. A notable example is when subjective human opinion is required to assess beauty (Graf [65]). One of the most renowned human-distributed computing projects is the Electric Sheep screensaver. This project generates artificial animals as fractal animations (Barnsley [66]). These artificial entities evolve using genetic algorithms (Johnson [67]). A crucial aspect of this project is that users determine the fitness value for genetic algorithm individuals through voting (Cho [68]). Users cast a vote with a thumb-up if they like the animation or a thumb-down if they dislike it. The user votes are collected on the server side and used in the evolution of genetic algorithms. The subjective opinions of many individuals regarding beauty are critical in creating such computer-generated art.</text:p><text:p text:style-name="P55"/><text:p text:style-name="P55">The utilization of artificial neural networks in financial forecasting, as a form of distributed computing, was effectively demonstrated in the MoneyBee project (Bohn [69]). Training artificial neural networks involves numerous possibilities for parallel calculations (Seiffert [70]), making it highly suitable for distributed solutions. The project was structured as a <text:soft-page-break/>screensaver that harnessed donated computing power, similar to (Krieger [71]). The weights of the artificial neural networks were refined during participants&apos; idle computer usage periods. The MoneyBee project&apos;s success served as the VitoshaTrade project&apos;s foundation. In VitoshaTrade, artificial neural networks are trained as a background process within the MetaTrader 4 trading platform. The screensaver concept was replaced with the background process of a running application. With the expansion of mobile devices in the VitoshaTrade project, the Android Active Wallpaper interface was introduced. This transition moved calculations from the background process of a running application to the background service of the Android OS wallpaper. The most significant advantages of mobile distributed computing are that these devices operate continuously (24/7 mode) and are far more numerous than desktop computers.</text:p><text:p text:style-name="P51"/><text:p text:style-name="P56">Initially developed, the VitoshaTrade project does not incorporate human opinions into the financial forecasting process. Modern capabilities of Android OS mobile devices offer methods to collect human votes. In this proposal, we introduce an innovative human-computer distributed computing voting system. On a daily basis, a notification is displayed on the Android screen, prompting the user to vote either up or down for a specific currency pair value change. User information is collected on lightweight PHP/MySQL server applications.</text:p><text:p text:style-name="P56"><text:soft-page-break/>For users&apos; votes to be utilized in future forecasting, the collected information should be organized based on each user&apos;s voting frequency and success rate in guessing. This challenge can be efficiently addressed using self-organizing maps. In this proposal, the identification of four groups (clusters) of interest is done:</text:p><text:p text:style-name="P56"/><text:p text:style-name="P56"><text:s text:c="2"/>1. Users with high voting frequency and a high success rate in guessing.</text:p><text:p text:style-name="P56"><text:s text:c="2"/>2. Users with low voting frequency but a high success rate in guessing.</text:p><text:p text:style-name="P56"><text:s text:c="2"/>3. Users with low voting frequency and a low success rate in guessing.</text:p><text:p text:style-name="P56"><text:s/>4. Users with high voting frequency but a low success rate in guessing.</text:p><text:p text:style-name="P56"/><text:p text:style-name="P56">Determining the boundaries of these four groups is complex, making self-organizing maps an ideal tool for this clustering problem. The network is trained using information collected from users with varying activity levels. Some users engage with the system for extended periods, providing a broader time spread of information. Network input includes the number of successful guesses, the number of unsuccessful guesses, and the rate of voting days.</text:p><text:p text:style-name="P56"/><text:p text:style-name="P56">Financial forecasting can be highly intricate due to numerous interdependent factors. Even successful users make mistakes in their forecasts. A personal success rate is calculated for each user in the system. However, when making future forecasts, each user&apos;s rating and <text:soft-page-break/>correction coefficient are considered based on the group in which they were classified. Through this forecasting process, a collective of individuals offers subjective opinions on the future change in currency pair values. Some individuals vote based on their knowledge, others on personal calculations, and a third group votes solely on intuition, even if they cannot explain their reasoning. In psychology, it is well-established that a group of people can often arrive at superior solutions compared to even the top experts in a particular field.</text:p><text:p text:style-name="P56"/><text:p text:style-name="P56">Human-computer distributed computing holds significant promise for financial forecasting. Trading intuition is an aspect that remains beyond the reach of today&apos;s computers. The aggregated opinions of a group of individuals, combined with their knowledge and subconscious information processing, can yield much more reliable forecast results. With the support of modern mobile devices and wireless communication channels, human-computer-based distributed computing has become widely accessible and cost-effective.</text:p><text:p text:style-name="P56"/><text:p text:style-name="P56">The primary drawback of group decision-making lies in the challenge of proving the correctness of decisions made and the difficulty of replicating the same solution.</text:p><text:p text:style-name="P127"/><text:h text:style-name="P112" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc7836_527140494"/><text:soft-page-break/>Summary<text:bookmark-end text:name="__RefHeading___Toc7836_527140494"/></text:h><text:p text:style-name="P127"/><text:p text:style-name="P128">Chapter 5 delves into the complexities and challenges of human evaluation in optimizing functions, specifically focusing on two domains: art and intuitive human evaluation in distributed computing.</text:p><text:p text:style-name="P128"/><text:p text:style-name="P128">This section highlights the limitations of using human evaluation in optimization tasks, where solutions depend on subjective assessment, like in art or financial forecasting. It discusses how human intuition and subconscious processing contribute to decision-making, often producing solutions without clear explanations. The chapter emphasizes the necessity of a dialogue between humans and computers to create evaluation functions in evolutionary algorithms.</text:p><text:p text:style-name="P128"/><text:p text:style-name="P128">The segment explores the historical evolution of computer-generated music, from Mozart&apos;s algorithmic compositions to modern computer programs like David Cope&apos;s &quot;Experiments in Musical Intelligence.&quot; It reviews various approaches employed in music composition, including genetic algorithms and rule-based systems. The focus is on achieving harmony in computer-generated music through interactions between human aesthetic evaluations and computational models.</text:p><text:p text:style-name="P128"/><text:p text:style-name="P128"><text:soft-page-break/>The chapter outlines a proposed model for music improvisation, integrating Distributed Differential Evolution and human aesthetic factors from global users. It discusses the significance of musical improvisation in various genres and its integration into the proposed model.</text:p><text:p text:style-name="P128"/><text:p text:style-name="P128">It then details the technical implementation of the proposed model using Java RMI for distributed application development. It explains the process flow for generating music with human evaluation in a distributed computing environment.</text:p><text:p text:style-name="P128"/><text:p text:style-name="P128">Finally, it raises the complex issue of copyright ownership for melodies generated by machines, as neither the programmers nor the users creating the system can be considered the music&apos;s creators.</text:p><text:p text:style-name="P128"/><text:p text:style-name="P128">This section delves into the application of human-aided distributed computing, especially in areas like the Electric Sheep screensaver project and financial forecasting using distributed artificial neural networks. It emphasizes the role of human opinions in assessing beauty and how collective human intuition can improve forecasting in financial markets.</text:p><text:p text:style-name="P128"/><text:p text:style-name="P128">The chapter introduces an innovative voting system that collects user opinions on currency pair value changes, aiming to leverage human intuition alongside computational power for more accurate financial <text:soft-page-break/>predictions. It discusses the challenges of integrating subjective human opinions into financial forecasting models and highlights the promise of human-computer distributed computing in this field.</text:p><text:p text:style-name="P128"/><text:p text:style-name="P128">In closing, it acknowledges the potential of group decision-making and the difficulties in validating such decisions and consistently reproducing similar results.<text:line-break/></text:p><text:p text:style-name="P127"/><text:h text:style-name="P107" text:outline-level="1"><text:bookmark-start text:name="__RefHeading___Toc2349_1821873452"/>6 Evolutionary Algorithms in Vectorization Problems<text:bookmark-end text:name="__RefHeading___Toc2349_1821873452"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P3">In vectorization tasks, the objective is to convert analog data discretized into a vector representation. For images, this often involves translating bitmaps into graphic primitives. When dealing with sound, the process begins by capturing analog air vibrations, converting them into samples, and then attempting to transcribe them into a notational form. It is important to note that the vectorization process is often slow, and its success rate can be uncertain. Despite these challenges, it finds practical application in various everyday needs.</text:p><text:p text:style-name="P57"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc2287_789959763"/><text:span text:style-name="T19">6.1 </text:span>Sound Vectorization<text:bookmark-end text:name="__RefHeading___Toc2287_789959763"/></text:h><text:p text:style-name="P57"/><text:p text:style-name="P58">In audio-based multimedia, there are two standard formats: WAVE audio and MIDI audio. WAVE audio represents air vibrations assembled into complex sounds, while MIDI audio consists of a sequence of musical events dictating the order in which different instruments play notes. WAVE is a raw sound format, while MIDI is a form of vector sound format. MIDI audio was developed alongside electronic musical instruments, serving as a standard for real-time audio information exchange and <text:soft-page-break/>efficient file storage. Converting MIDI to WAVE is easily accomplished by modern computers, often performed within hardware sound cards via the MIDI synthesizer module. Conversely, transforming WAVE to MIDI is a relatively complex task, akin to transcribing music from auditory notes to written notations. Some audio, such as game background music, is more suited to the MIDI format.</text:p><text:p text:style-name="P58"/><text:p text:style-name="P58">In cases where WAVE information needs conversion into a group of simple MIDI events, this process involves reducing information. It transforms actual sounds into a set of primary musical events. The issue of vectorization, extensively discussed in the literature, draws parallels to notable implementations like creating an image of the Mona Lisa using no more than 50 polygons. This process involved optimizing colors and polygon points with Genetic Algorithms. Similar principles can be applied to sound vectorization, wherein WAVE information is transformed into a MIDI sequence.</text:p><text:p text:style-name="P58"/><text:p text:style-name="P58">The proposed model relies on Genetic Algorithms applied to a set of MIDI events. It is an unsupervised system that takes digital WAVE sounds as input and produces simplified, stylized vector data as output. WAVE sounds typically contain non-essential details that hinder audio quality and increase computational costs. Simplifying WAVE sounds involves eliminating irrelevant elements while retaining perceptually dominant <text:soft-page-break/>ones. This simplification process finds application in music composition and game development. In sound simplification and vectorization, digital WAVE samples serve as input, generating simplified, stylized vector data as output. The objective is to break down the original sound into simple MIDI events for sound reconstruction.</text:p><text:p text:style-name="P58"/><text:p text:style-name="P58">Each MIDI event in the GA-based system describes its type, status, and time. The GA population comprises ordered sets of events, each with a fitness value calculated as the Euclidean distance between the approximate and original sound. MIDI events assemble the approximated sound initially. The GA population starts with randomly generated event sets where type, status, and time are randomly assigned. Throughout the GA&apos;s evolutionary process, individuals are selected from the population, and processes like crossover, mutation, evaluation, and survival are applied to refine the sound approximation. This model employs binary crossover and applies mutation by time shifting and altering event status within individuals. The fitness value of individuals is determined by comparing the approximated sound with the original. During the survival process, the decision is made on which individuals to keep, whether newly created or preexisting, with the only stopping criterion being the total number of sound evaluations.</text:p><text:p text:style-name="P58"/><text:p text:style-name="P58"><text:soft-page-break/>The use of GA proves efficient in sound approximation within the constraints of sound simplification. Optimization convergence relies on the probabilistic nature of GA. Sound comparison, being time-consuming, slows down the optimization process. Future research may explore implementing GA as a distributed computing algorithm, especially within evolutionary algorithms. Treating the set of events as a multidimensional space allows for improved optimization and better results.</text:p><text:p text:style-name="P57"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc1936_376025042"/>6.2 Image Vectorization<text:bookmark-end text:name="__RefHeading___Toc1936_376025042"/></text:h><text:p text:style-name="P57"/><text:p text:style-name="P59">In geometry, there is a class of optimization problems known as packing problems, which involve arranging objects tightly within containers. In the most common scenario, the goal is to pack a single container densely. For each packing problem, there is a dual covering problem, in which we determine how many identical objects are needed to completely cover every region of a container, allowing for overlapping in typical cases. A covering problem is presented in which an image will be covered with regular ellipses of different colors, permitting overlap. The optimization objective function is closely tied to the composite picture and is less associated with achieving optimal packing. The criteria for effective optimization are measured by the Euclidean distance between the <text:soft-page-break/>original and approximated images, which are compared in the HSV color space.</text:p><text:p text:style-name="P59"/><text:p text:style-name="P59">Some drawing devices are unable to represent visual information as pixels. For such devices, visual information must be transformed into a collection of simple geometric shapes, which involves reducing information. This transformation process converts colored images into a set of straightforward geometric shapes, a problem well-documented in literature - one notable implementation involved approximating the Mona Lisa&apos;s image using no more than 50 polygons. Genetic algorithms optimized both the colors and the points of the polygons.</text:p><text:p text:style-name="P59"/><text:p text:style-name="P59">Raster images typically contain numerous details irrelevant to a visual query, increasing manipulation costs. Simplifying raster images is a process that removes the least essential elements while retaining perceptually dominant elements and shapes. Image simplification plays a crucial role in document analysis and recognition, serving as a preprocessing step for high-level object recognition, including optical character recognition (OCR) and graphic object recognition. In image simplification and vectorization, digital photographs serve as input to generate simplified, stylized vector data as output. The objective of image simplification through vectorization is to break down the original image <text:soft-page-break/>into simple geometric regions, with color reduction also applied in this process.</text:p><text:p text:style-name="P59"/><text:p text:style-name="P59">Ellipses were chosen as the approximation shapes for image reconstruction. Each shape is defined by its X and Y coordinates, rotation angle, and color. In the GA population, each individual consists of an ordered set of shapes and corresponding fitness values. Shapes in the ordered set are arranged so that the most frequently used color in the picture is applied first, while less-used colors are applied last. Fitness values are calculated based on the Euclidean distance between the approximate and original images.</text:p><text:p text:style-name="P59"/><text:p text:style-name="P59">The approximated image is assembled using shapes, in this case, ellipses, with different colors specified in a colormap provided as input to the program. Color reduction is performed in the HSV color space. Each color from the original image is matched to a color in the given colormap as the initial step. The GA population starts with randomly generated sets of shapes, with X and Y coordinates and rotation angles chosen randomly for each shape. The color of each shape, however, is determined by the color at the original image&apos;s X and Y pixel coordinates. This initialization results in an approximated image that closely approximates the optimal image. During the GA&apos;s evolutionary process, individuals are selected from the population, and crossover, mutation, evaluation, and survival are applied <text:soft-page-break/>to refine the image approximation. A binary crossover is employed, and mutation is applied as X and Y shifting and color changes for one shape within an individual.</text:p><text:p text:style-name="P59"/><text:p text:style-name="P59">During the evaluation phase, the approximated image is compared with the original image, and the comparison result determines the individual&apos;s fitness value. In the survival process, a decision is made regarding which individual to retain, either the newly created one or the pre-existing individual. The only stopping criterion used is the total number of image evaluations, with the elitism rule applied. The initial population is constructed by randomly selecting shapes based on the pixel&apos;s color from the original image. A colormap consisting of sixteen primary colors is employed. As the optimization procedure progresses, the gaps in the image become smaller, and the orientation of the basic shapes improves. A digital plotter with oil paint can generate vectorized images, which can be incorporated as part of the concept for the digital home.</text:p><text:p text:style-name="P59"/><text:p text:style-name="P59">Genetic algorithms can be highly efficient, and image approximation is accurate within the bounds of color simplification. The convergence of optimization is linked to the probabilistic nature of GAs.</text:p><text:p text:style-name="P57"/><text:p text:style-name="P60">Packing problems have been recognized in geometry for centuries, gaining even more significance with the widespread development of soft <text:soft-page-break/>computing. The objective is to arrange specific objects within a container efficiently. A well-known instance of this problem arises when a single container needs to be filled as densely as possible (Lodi [72]). Each packing problem has a corresponding dualism as a covering problem. In covering problems, the question is how many objects are needed within an arrangement to cover the maximum area in the container. This problem is defined in two scenarios: one where objects can overlap and another where they cannot. Notable problems in two-dimensional space include packing circles in a circle (Fodor [73]), packing circles in a square (Huang [74]), packing circles in an isosceles or right triangle (Xu [75]), packing circles in an equilateral triangle (Nurmela [76]), and packing squares in a square (Stromquist [77]).</text:p><text:p text:style-name="P60"/><text:p text:style-name="P60">The described problem is a multi-objective optimization problem, ideally suited for a solution using genetic algorithms applied to a covering problem. The area to cover is rectangular, and the covering shapes are identical-sized ellipses. They must be positioned at different coordinates and orientation angles. The ultimate goal is to generate a bitmap image for translation into G-Code instructions, allowing for applying acrylic paint using a 2D plotting machine.</text:p><text:p text:style-name="P57"/><text:p text:style-name="P61">Some visualization devices are not capable of handling a high number of colors. This is the case with 2D plotting machines. They have a limited <text:soft-page-break/>number of different colors. Also, different colors are handled consequently. It means that the device does only a single color in time, and proper job scheduling is needed. The other significant feature of the plotting devices is that they operate with graphical primitives (lines, arcs, and other simple shapes). Such handling of the visual information is pretty different than what is used in pixel-based devices. Suppose a bitmap image should be visualized on a 2D plotting machine. In that case, the bitmap image should be transformed into a set of instructions (usually GCodes), and the number of colors should be drastically reduced. This process has two standard components - color reduction and vectorization.</text:p><text:p text:style-name="P61"/><text:p text:style-name="P61">Bitmap images usually have much more information than is needed for visual interception. Because of this, a process for image simplification can be applied so that less informative elements can be reduced or completely removed. Meanwhile, the most informative elements are kept (Ferreira [78]). Bitmap simplification is widely applied in different areas. For example, it has a significant role in optical character recognition (Wenyin [79]). Conversion from vector graphics to bitmap is pretty stable, robust, and reliable, but this is not the case with the conversion in the opposite direction (Tombre [80]). Digital photographs are used as input to the image simplification algorithm. The output of the simplification algorithm is stylized vector data. The simplification algorithm aims to represent the original image with simple geometric regions.</text:p><text:p text:style-name="P61"><text:soft-page-break/></text:p><text:p text:style-name="P61">Genetic algorithms are chosen for the process of global optimization. The natural process of evolution inspires genetic algorithms. The proposed sub-optimal solutions by the genetic algorithms are organized in the population. Each individual in the population is represented as a vector, and it is called a chromosome. In many cases, the initial population is initialized with randomly generated chromosomes. However, for some problems like the problem presented in this research, the initial population is initialized according to statistical estimation of the original bitmap image. Searching for a better suboptimal solution in genetic algorithms is organized as epochs of evolution with the production of new generations. Each new generation appears after three basic operations - selection, crossover, and mutation. The selection operator is the core of optimization convergence. The empirical expectation is that the offspring would be even better when better-fitted chromosomes are selected for offspring production. The selection operator relies totally on the fitness value calculation. This research calculates fitness value according to three components - bitmap image closeness, size of blank areas, and the number of primitive shapes used for image approximation. These three different optimization goals make this optimization problem a multi-objective problem.</text:p><text:p text:style-name="P61"/><text:p text:style-name="P61"><text:soft-page-break/>An ellipse with predefined width and height is selected as the base 2D visual primitive for the approximation. The ellipse is appropriate because acrylic paint will be drawn with a small brush. It is accepted that each stroke with the brush will resemble an ellipse. Sharp lines or long rectangles are the other options, but they are less close to the shape of a single brush stroke. For simplicity in the current research, it is accepted that all ellipses are identically sized. This can be elaborated on in some further research. Each ellipse is described with four numerical values - coordinates of the center (x and y values), angle of rotation around the center, and index of color from the provided list of colors.</text:p><text:p text:style-name="P61"/><text:p text:style-name="P61">A list of ellipses with variable sizes presents the genetic algorithm chromosome. The list of ellipses is sorted before fitness value calculation because the most frequent colors should be drawn first, and the less frequent colors should be drawn last. Such a drawing organization is needed because ellipses can overlap, and the most frequent one can completely cover the least frequent one. The length of the chromosomes can vary, which is one of the criteria put under optimization. If the approximated image can be drawn with fewer primitive objects, it will reduce drawing time (operating efficiency) and paint consumption (cost efficiency).</text:p><text:p text:style-name="P61"/><text:p text:style-name="P61"><text:soft-page-break/>The exploration part of genetic algorithms is done with the crossover operator. With chromosomes as lists of ellipses, uniform crossover perfectly fits the need for exploration. Elements from parent chromosomes are organized so children can be produced with different lengths. The exploitation part of genetic algorithms is done with the mutation operator. In this research, a randomly selected ellipse from the child chromosome is randomly changed as coordinates of the center, rotation angle, and color index. The trade-off between exploration and exploitation in genetic algorithms affects the completeness of wide optimization space investigation and near sub-optimal solutions investigation (Hussain [81]).</text:p><text:p text:style-name="P61"/><text:p text:style-name="P61"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame28" text:anchor-type="as-char" svg:width="5in" draw:z-index="54"><draw:text-box fo:min-height="3.75in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image28" text:anchor-type="paragraph" svg:width="5in" style:rel-width="100%" svg:height="3.75in" style:rel-height="scale" draw:z-index="55"><draw:image xlink:href="Pictures/1000000100000258000001C2AA8991FB16A3397A.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure27" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">28</text:sequence>: Bitmap image used as an input</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P61"/><text:p text:style-name="P62">The first component in the multi-objective fitness function is the closeness between the original and approximated bitmap images. The smaller the difference between both images, the better the sub-optimal solution. For this objective, the minimum is desired as sub-optimality. For the calculation of closeness, the original bitmap is taken unmodified. The approximated image on the other side is generated by drawing all chromosome’s ellipses in a bitmap image. Both images have identical dimensions. The current research calculates the average Euclidean <text:soft-page-break/>distance between the pixels in the images. Such a measure of image similarity is not accurate enough. It does not take into account image composition. In some further research, it will be better for some kind of hierarchical granularity to be involved.</text:p><text:p text:style-name="P62"/><text:p text:style-name="P62">The second component in the multi-objective fitness function is the total amount of blank area. The perfect case for this objective is when there is no blank area. As with the previous objective, the minimum is desired for better sub-optimality. The amount of blank area is easily calculated by filling the initial drawing area of the approximated image with a particular transparency value. The amount of blank area is the count of the pixels that remain unchanged after the ellipses drawing.</text:p><text:p text:style-name="P61"/><text:p text:style-name="P61">The third component in the multi-objective fitness function is the count of drawn ellipses. It is evident that fewer ellipses will produce a picture faster with less paint usage. As with the previous two objectives, the minimum is desired as a better sub-optimality. This objective dramatically opposes the other two objectives. If the number of drawn ellipses is zero, it will not cost time, and it will not cost paint. The problem is that with zero ellipses, image distancing will be high, and the blank area will be the maximum possible.</text:p><text:p text:style-name="P61"/><text:p text:style-name="P61"><text:soft-page-break/>The photograph shown in Figure 28 is used as an input for the experiments. A set of 12 primary colors is used for the output image, as follows: FFFFFF (White), FFFF00 (Yellow), CC7722 (Ochre), E34234 (Vermillon), DC143C(Crimson), 49796B (Hooker’s Green), 40826D (Viridian), 120A8F (Ultramarine Blu), 0000FF (Coblat Blue), E97451 (Burnt Sienna), 635147 (Raw Umber), 000000 (Black). On a trial and error basis, 37 individuals were chosen for the population size. The arity of tournament selection is chosen to be 2. The crossover rate is chosen to be 95%. The mutation rate is chosen to be 1%. The elitism rule is applied to 5% of the population. A time limit of one hour is chosen for the stopping criteria.</text:p><text:p text:style-name="P61"/><text:p text:style-name="P61"><draw:frame draw:style-name="fr1" draw:name="Frame29" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="56"><draw:text-box fo:min-height="2.5075in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image29" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="2.5075in" style:rel-height="scale" draw:z-index="57"><draw:image xlink:href="Pictures/1000000100000708000003841F46557A800B8C1C.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure28" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">29</text:sequence>: Six variations of approximated images</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P61"/><text:p text:style-name="P61"><text:soft-page-break/>As shown in Figure 29, the most informative elements of the bitmap image are kept in the approximated images. There are some deviations in the results, but they are expected because of the probabilistic nature of the genetic algorithms. The number of ellipses used for the approximation is acceptable (if the number is too high, it will be time and paint-consuming), but the presence of prominent blank areas is not of the desired amount. Each interested reader can try different genetic algorithm parameters with different images of his/her own choice because of the implemented command-line interface.</text:p><text:p text:style-name="P61"/><text:p text:style-name="P63">The image approximation model can be expanded by incorporating Ant Colony Optimization (ACO). ACO is applied to a graph of shapes represented by position and orientation. It operates as an unsupervised system that inputs digital photographs and generates simplified, stylized vector data as output.</text:p><text:p text:style-name="P63"/><text:p text:style-name="P63">ACO is a search heuristic inspired by real-world ant behavior (Kazharov [82], Mohd Murtadha [83]). It is commonly used for optimizing paths in graphs (candidate solutions). Pathways can progressively approach the optimum by employing techniques for ant movement and pheromone updates. ACO is also categorized as a population-based algorithm because artificial ants explore the solution space. Each ant investigates different <text:soft-page-break/>paths, leaving a trail of pheromones. The standard representation of solutions involves a graph with nodes and edges.</text:p><text:p text:style-name="P63"/><text:p text:style-name="P63">Typically, optimization begins with a randomly generated graph, which is also the subject of implementation. The optimization process is iterative, with artificial ants navigating the graph. Based on information from graph nodes and edges, each ant leaves a trail by depositing pheromones. The optimization mainly focuses on paths within the graph. Paths with the highest accumulated pheromones become the preferred solutions for the next iteration of the algorithm. Termination of the algorithm usually occurs upon reaching a maximum number of repetitions or the desired accuracy level.</text:p><text:p text:style-name="P63"/><text:p text:style-name="P63">To execute ACOs, two key elements are required: 1. Graph representation of the solution space (solution domain); 2. An appropriate pheromone estimation rule to evaluate the solution domain. Once these conditions are met, ACO can initiate population initialization and iterative population improvement through the repetitive application of ants&apos; movements (Dorigo [84]).</text:p><text:p text:style-name="P57"/><text:p text:style-name="P64">In the ACO extension, ellipses have been selected as the approximation shapes for image reconstruction. Each shape is defined by its X and Y coordinates, rotation angle, and color. The ACO graph comprises an <text:soft-page-break/>ordered set of shapes (as nodes) and edges (links between nodes of different colors). The sequence of shapes is arranged so that the predominant color in the picture is applied first, with less utilized colors painted subsequently. The Euclidean distance between the approximate and original images determines the quality of the approximated image. Shapes, namely ellipses in this case, form the approximated image with distinct colors defined by a color map provided as input to the program. Color reduction occurs within the HSV color space, where each color in the original image corresponds to a color in the given color map.</text:p><text:p text:style-name="P64"/><text:p text:style-name="P64">Initially, ACO begins with randomly generated sets of shapes (nodes) and edges linking nodes of varying colors. Random X and Y coordinates, as well as rotation angles, are assigned to each shape. However, the shape&apos;s color is determined by reducing the color found at the original image&apos;s corresponding X and Y pixel coordinates. This method results in an approximated image that closely resembles the optimal approximation. Throughout the ACO improvement process, ellipses are chosen from the graph. Selected ellipses undergo rotation, and color adjustments refine the image approximation. Only ellipses with distinct colors are optimized. Although not currently implemented, X and Y shifting could also be applied. During the evaluation phase, the approximated image is compared against the original image to determine the optimization <text:soft-page-break/>quality. The stopping criteria employed rely on the total number of repetitions.</text:p><text:p text:style-name="P64"/><text:p text:style-name="P64">The basic algorithm is structured as follows:</text:p><text:p text:style-name="P64"/><text:p text:style-name="P64">1. Load the original image into memory.</text:p><text:p text:style-name="P64">2. Initialize a random ACO structure.</text:p><text:p text:style-name="P64">3. Optimization:</text:p><text:p text:style-name="P64"><text:s text:c="3"/>(a) Generate solutions.</text:p><text:p text:style-name="P64"><text:s text:c="3"/>(b) Implement daemon actions.</text:p><text:p text:style-name="P64"><text:s text:c="3"/>(c) Update pheromones.</text:p><text:p text:style-name="P64"><text:s text:c="3"/>(d) Generate image in memory.</text:p><text:p text:style-name="P64"><text:s text:c="3"/>(e) Compare generated and original images for quality estimation.</text:p><text:p text:style-name="P64"><text:s text:c="3"/>(f) Stop when a predefined number of generations is reached.</text:p><text:p text:style-name="P64"><text:s text:c="3"/>(g) Repeat from (a).</text:p><text:p text:style-name="P64">4. Report the results.</text:p><text:p text:style-name="P57"/><text:p text:style-name="P57"><text:soft-page-break/><draw:frame draw:style-name="fr2" draw:name="Frame30" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="6.8083in" style:rel-height="scale-min" draw:z-index="58"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image30" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="6.4201in" style:rel-height="scale" draw:z-index="59"><draw:image xlink:href="Pictures/10000001000000EC0000013AAB0C4E1D9554684A.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure29" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">30</text:sequence>: Bitmap image used as an input</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P57"/><text:p text:style-name="P65"><text:soft-page-break/>Randomly taken shapes assemble the initial graph according to the pixel&apos;s color from the original image. An image of a flower is used, shown in Figure 30, but any other picture can be sent as input for the optimization program. Sixteen primary colors are used as color map as follows: Aqua (00FFFF), Black (000000), Blue (0000FF), Fuchsia (FF00FF), Gray (808080), Green (008000), Lime (00FF00), Maroon (800000), Navy (000080), Olive (808000), Purple (800080), Red (FF0000), Silver (C0C0C0), Teal (008080), White (FFFFFF), Yellow (FFFF00).</text:p><text:p text:style-name="P65"/><text:p text:style-name="P65"><text:soft-page-break/><draw:frame draw:style-name="fr2" draw:name="Frame31" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="4.8382in" style:rel-height="scale-min" draw:z-index="60"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image31" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="4.45in" style:rel-height="scale" draw:z-index="61"><draw:image xlink:href="Pictures/10000001000002C400000274A0028BF2B7E7399A.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure30" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">31</text:sequence>: Six variations of optimized images</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P65"/><text:p text:style-name="P65">Figure 31 shows that using ACO may be very efficient, and image approximation is pretty accurate in the limits of color simplification. Optimization convergence is related to the probabilistic nature of ACO. Image comparison is time-consuming and slows down the optimization process.</text:p><text:p text:style-name="P57"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc6782_3046959508"/><text:soft-page-break/>6.3 String Rewriting for 3D Fractal Generation<text:bookmark-end text:name="__RefHeading___Toc6782_3046959508"/></text:h><text:p text:style-name="P57"/><text:p text:style-name="P57">String rewriting represents a modification of the concept found in context-free grammar. This modification eliminates the distinction between terminal and non-terminal symbols. Each symbol within string rewriting is considered non-terminal and capable of generating longer sequences. Consequently, this approach forms infinite structures akin to fractals. In a video presentation by Jack Hodkinson, he proposes using 2D images instead of text symbol pixels. Each pixel, or geometric square, is subdivided into nine smaller squares. The arrangement of these nine squares is determined by the pixel&apos;s color, establishing a rule for the subdivision of the encompassing area. Hodkinson presents a methodology for configuring the fractal to replicate a specific 2D image, such as the glyph representing the Greek letter π. This particular challenge is widely recognized in fractal theory as the Fractal Inverse Problem (FIP), constituting an optimization problem. Employing a genetic algorithm emerges as a favorable option to construct a set of rules for string rewriting with 2D pixels. Hodkinson conducted experiments involving substitution matrices of various sizes (not limited to 3×3 but also encompassing 2×2 and 4×4). Additionally, he conducted experiments with different color sets, commencing with black and white and culminating in a final successful experiment utilizing thirty shades of blue.</text:p><text:p text:style-name="P57"/><text:p text:style-name="P57"><text:soft-page-break/>This proposition addresses FIP, a well-known concept in fractal theory (Guerin [85], Nettleton [86]). Within the theory, a branch is dedicated to formal languages associated with context-free grammar. In such grammars, there exists a set of production rules. Applying these rules results in generating all possible strings within a specified formal language (Ochoa [87]). In Context-Free Grammars (CFGs), a production rule represents a simple replacement operation. These rules are applied without considering the context. The left-hand side of a CFG rule always comprises a non-terminal symbol, indicating that non-terminal symbols are not part of the resulting string. On the other hand, the right-hand side of a CFG rule includes terminal symbols, non-terminal symbols, or a combination of both. The fundamental concept in CFGs lies in substituting non-terminal symbols based on the given rules until no non-terminal symbols remain in the generated string.</text:p><text:p text:style-name="P57"/><text:p text:style-name="P57">A string rewriting system (SRS) is a substitution system where the rules do not incorporate non-terminal symbols. Each terminal symbol can appear both on the rule&apos;s left-hand side (LHS) and the right-hand side (RHS). With this definition, SRSs prove helpful in generating infinite structures. They are primarily employed in creating certain fractals (such as the Box Fractal, Cantor Dust, Cantor Square Fractal, and Sierpinski Carpet). While most fractals are generated using binary rules (black and white colors), Hodkinson&apos;s research demonstrates that the color limit can be expanded.</text:p><text:p text:style-name="P57"><text:soft-page-break/></text:p><text:p text:style-name="P57"><draw:frame draw:style-name="fr1" draw:name="Frame32" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="62"><draw:text-box fo:min-height="4.4583in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image32" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="4.4583in" style:rel-height="scale" draw:z-index="63"><draw:image xlink:href="Pictures/1000000100000384000003200B9A1458AFC527AB.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure31" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">32</text:sequence>: Fractal generated by Jack Hodkinson</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P57"/><text:p text:style-name="P66">The issue in 1D is well elucidated by (Shonkwiler [88]). However, Hodkinson&apos;s study employs genetic algorithms to reconstruct the 2D shape of the Greek letter π using substitution rules involving 30 shades of blue pixels, as depicted in Figure <text:span text:style-name="T20">32</text:span>. Although (Zelinka [89]) explores differential evolution, this approach is not adopted in Hodkinson&apos;s <text:soft-page-break/>research due to the optimization occurring within a discrete space. The pivotal challenge in this study revolves around selecting the rules required to achieve the ultimate objective. GAs present a promising approach for such combinatorial optimization tasks. Each rule comprises an LHS pixel with a specific color and nine pixels on the RHS arranged in a 3×3 substitution matrix. The image initiates with a single pixel and expands in size by a factor of 3 with each iteration. The present proposition extends the concept from 2D to 3D space, marking the author&apos;s primary contribution. Voxels are utilized instead of pixels, and a 3×3×3 cube replaces the square 3×3 matrix.</text:p><text:p text:style-name="P57"/><text:p text:style-name="P57">A finite 3D space is represented by the model&apos;s three-dimensional array of voxels. Each voxel is encoded with a single long integer value containing RGB (red, green, blue) information for the voxel&apos;s color. This model selects 32 different 3×3×3 matrices, each corresponding to one of the 32 colors. The steps for fractal generation are outlined in the algorithm:</text:p><text:p text:style-name="P57"/><text:p text:style-name="P57">1: procedure Generate(Level, Voxel)</text:p><text:p text:style-name="P57">2: <text:s text:c="2"/>if Level &lt;= 0 then</text:p><text:p text:style-name="P57">3: <text:s text:c="4"/>return</text:p><text:p text:style-name="P57">4: <text:s text:c="2"/>v &lt;- Split(Voxel)</text:p><text:p text:style-name="P57">5: <text:s text:c="2"/>for each v do</text:p><text:p text:style-name="P57">6: <text:s text:c="4"/>Paint(v)</text:p><text:p text:style-name="P57"><text:soft-page-break/>7: <text:s text:c="4"/>Generate(Leve-1, v)</text:p><text:p text:style-name="P57"/><text:p text:style-name="P57">The voxel is split into 27 smaller voxels, each with 1/3 of the original side size in Step 4. Each sub-voxel is painted according to the color of the transition matrix pattern in Step 6. The generator procedure is recursively called for each sub-voxel in Step 7. Fractal generation stops when the recursion level reaches zero in Step 3.</text:p><text:p text:style-name="P57"/><text:p text:style-name="P57">Individuals in the genetic algorithm are represented by 32 transition matrices, each sized 3×3×3. Long integer numbers represent RGB colors for all 27 cells of the transition matrix.</text:p><text:p text:style-name="P57"/><text:p text:style-name="P57">The order of the matrices determines the sequence of transitions based on the colors used. Parents in the population are selected using tournament selection with an arity of two. Uniform crossover, the most symmetric crossover, randomly selects 50% of elements from each parent. The random color change is used for the mutation operator. Evaluation is performed by applying each of the 32 transition matrices at a specific recursion depth. The fitness function utilizes the Euclidean distance between the generated fractal and the target shape.</text:p><text:p text:style-name="P57"/><text:p text:style-name="P57"><text:soft-page-break/>The initial fractal is a single cube painted with the color indexed first in the color order. Genetic algorithms are global heuristic optimizations, meaning the initial voxel&apos;s color does not affect the outcome.</text:p><text:p text:style-name="P57"/><text:p text:style-name="P57">Expanding from 2D to 3D dimensions significantly amplifies the complexity of the optimization process. Due to this heightened complexity, a less intricate form is utilized than the Greek letter π; a voxelized sphere is presented in Figure <text:span text:style-name="T21">33</text:span>. As one of the simplest 3D objects, the sphere was chosen as the optimization target. The initial color within the color set (1 out of 32) is considered transparent for visualization purposes. The objective of the calculation is to create a single-sided cube to identify a 3×3×3 set of transitions facilitating the fractal generation of the intended 3D object (in this case, a sphere).</text:p><text:p text:style-name="P57"/><text:p text:style-name="P57"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame33" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="64"><draw:text-box fo:min-height="4.0118in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image33" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="4.0118in" style:rel-height="scale" draw:z-index="65"><draw:image xlink:href="Pictures/10000001000006C1000005676E6463D60409FCF0.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure32" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">33</text:sequence>: The voxelized sphere as fractal optimization target object</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P57"/><text:p text:style-name="P57">The optimization process converges much more slowly in the 3D space compared to what Hodkinson accomplished in the 2D space. As depicted in Figure 3<text:span text:style-name="T22">4</text:span>, progressing from a single cube to the desired shape occurs sluggishly. Although the intended shape is a sphere, the intermediate stages resemble cubes more closely than the desired target object.</text:p><text:p text:style-name="P57"/><text:p text:style-name="P57"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame34" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="66"><draw:text-box fo:min-height="1.3366in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image34" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="1.3366in" style:rel-height="scale" draw:z-index="67"><draw:image xlink:href="Pictures/1000000100000B09000002F116F6E041C6A0F0E0.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure33" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">34</text:sequence>: Intermediate snapshots during the optimization process</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P57"/><text:p text:style-name="P57">Extending the concept of string rewriting into 3D space involves the utilization of voxels instead of pixels, employing a 3×3×3 substitution matrix. While the study is limited to virtual models, advancements in industrial tomography enable the scanning of 3D objects, subsequently allowing the printing of fractals using a 3D color printer. The extension of fractal generation from 2D to 3D space highlights that the process in 3D demands significantly more time than the 2D space. Speed enhancement is plausible through implementing parallel genetic algorithms (Shonkwiler [90]). Representing 3D objects via voxels consumes substantial computational resources, and the resulting objects possess limited practical utility. One potential application lies in compressing 3D images, akin to the concept of 2D image compression (Vences [91], Al-Bundi [92]). The results promise further exploration of the fractal essence within our three-dimensional world.</text:p><text:p text:style-name="P129"/><text:h text:style-name="P113" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc7838_527140494"/><text:soft-page-break/>Summary<text:bookmark-end text:name="__RefHeading___Toc7838_527140494"/></text:h><text:p text:style-name="P129"/><text:p text:style-name="P130">Chapter 6 delves into three distinct realms of application for evolutionary algorithms: sound vectorization, image vectorization, and string rewriting for 3D fractal generation.</text:p><text:p text:style-name="P130"/><text:p text:style-name="P130">The section on sound vectorization begins by defining the process of converting analog data, particularly sounds, into a vector representation. It explores the differences between WAVE and MIDI audio formats and their conversions, outlining how Genetic Algorithms are applied to simplify WAVE sounds into MIDI sequences. The GA-based system works with MIDI events, optimizing their types, statuses, and timing to approximate the original sound effectively. It details the GA&apos;s iterative process involving selection, crossover, mutation, evaluation, and survival to refine the sound approximation.</text:p><text:p text:style-name="P130"/><text:p text:style-name="P130">Moving to image vectorization, the chapter explores using GAs to simplify raster images into stylized vector data by approximating them with ellipses. It emphasizes the multi-objective nature of the optimization problem, aiming to minimize differences between original and approximated images while reducing blank areas and the count of drawn ellipses. The process involves initialization, crossover, mutation, and <text:soft-page-break/>fitness evaluation of ellipses, resulting in an approximation that reflects the essential elements of the original image.</text:p><text:p text:style-name="P130"/><text:p text:style-name="P130">Lastly, it introduces string rewriting for 3D fractal generation. It explains how evolutionary algorithms, particularly GAs, are employed to develop transition matrices that dictate the subdivision of voxels in a three-dimensional space. This approach, expanding from Hodkinson&apos;s 2D image replication to 3D shapes, navigates a complex optimization challenge in generating desired 3D objects from simple initial structures like cubes or spheres.</text:p><text:p text:style-name="P130"/><text:p text:style-name="P130">Each section highlights the unique challenges, methodologies, and applications of evolutionary algorithms in these diverse domains, showcasing their potential to solve complex problems across sound, image, and three-dimensional space.</text:p><text:p text:style-name="P129"/><text:h text:style-name="P107" text:outline-level="1"><text:bookmark-start text:name="__RefHeading___Toc2351_1821873452"/>7 Evolutionary Algorithms in Combination with Curve Fitting<text:bookmark-end text:name="__RefHeading___Toc2351_1821873452"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P84">Curve fitting is among the most popular approaches to predicting future outcomes based on prior measurements. There are various methods by which curve fitting can be employed in forecasting. It may involve directly identifying a curve or utilizing a curve as a supportive tool.</text:p><text:p text:style-name="P84"/><text:p text:style-name="P84">Time series forecasting holds significant real-world applications across multiple domains. This forecasting method finds widespread use in statistics, signal processing, pattern recognition, econometrics, mathematical finance, weather forecasting, earthquake prediction, electroencephalography, control engineering, astronomy, communications engineering, and any field of applied mathematics involving temporal measurements.</text:p><text:p text:style-name="P84"/><text:p text:style-name="P84">Over the past few decades, time series forecasting has garnered considerable attention from researchers in the machine learning domain. Numerous forecasting models have been developed using various prediction approaches. Artificial neural networks stand as a prominent example of such forecasting techniques. The primary objective involves understanding the data dependencies between past and future values of <text:soft-page-break/>a time series when employing an artificial neural network. If the weights of the artificial neural networks are regarded as coefficients of a complex polynomial, forecasting can be approached as a curve-fitting problem.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc2688_3397589795"/>7.1 Fitting the Curve Directly<text:bookmark-end text:name="__RefHeading___Toc2688_3397589795"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P85">Financial time series forecasting has garnered significant research interest for decades (Nava [93]). Accurate financial forecasts are crucial for essential decision-making (Catania [94]). Time series consist of ordered measurements of specific variables conducted in a temporal manner (Chen [95]). Values are typically measured at regular intervals, although this is not mandatory. Time series analysis applies to processes exhibiting clear repetition patterns (Mueen [96]).</text:p><text:p text:style-name="P85"/><text:p text:style-name="P85">Measurements arranged in chronological order are represented as points in a two-dimensional space. Often, these points are connected by straight lines for visualization, constituting a simplified form of linear interpolation between adjacent measurements. Various curves can be proposed to generalize the data&apos;s nature (known as the curve-fitting problem) through this organization of points in a two-dimensional space.</text:p><text:p text:style-name="P85"/><text:p text:style-name="P85"><text:soft-page-break/>For instance, linear regression can be used for trend estimation. In this scenario, the parameters of the line equation are calculated so that the line closely fits the given points. A more complex generalization involves the Lagrange polynomial. Here, the parameters of a polynomial with the lowest degree intersecting the corresponding Y value at each X value (coinciding at each point) are estimated.</text:p><text:p text:style-name="P85"/><text:p text:style-name="P85">Theoretically, an infinite number of curves can satisfy the condition of passing through a finite number of points in two-dimensional space. Similarly, if calculations performed in an artificial neural network are assessed as mathematical expressions, they provide a rough description of an equation in two-dimensional space. The main advantage of artificial neural networks is their ability to self-adjust coefficients (Aljarah [97]) in this equation.</text:p><text:p text:style-name="P85"/><text:p text:style-name="P85">In some cases, optimization within artificial neural networks is achieved using a genetic algorithm (Aljarah [98], Zhang [99]). Here, the intriguing aspect is how genetic algorithms can be employed for curve fitting as a self-adaptive system akin to artificial neural networks.</text:p><text:p text:style-name="P85"/><text:p text:style-name="P85">It is acceptable to use optimization with genetic algorithms where each population member represents a mathematical formula, drawing a curve in two-dimensional space. With a population of curves formed in this <text:soft-page-break/>manner, it becomes easier to evaluate which one best approximates the measured points and yields the most accurate predictive results.</text:p><text:p text:style-name="P85"/><text:p text:style-name="P85">Arithmetic expressions can be stored in computer memory in various ways. One commonly used representation is an expression tree. While this representation is convenient in many cases, arithmetic expressions are encoded as simple text strings in the current proposition. In this scenario, the crossover operator involves a straightforward single cut and swap. Mutation is organized as a random change of a mathematical operator, function, or operand.</text:p><text:p text:style-name="P85"/><text:p text:style-name="P85">When optimization is done in a mobile distributed computing environment, there is a global genetic algorithm population and many local populations. Each new individual in the local genetic algorithm population is tested by calculating the proposed formula and the mean square error with the real-life values. Lower values of the total mean square indicate better individuals with higher fitness levels. The best-discovered local individuals are reported to the centralized server and incorporated into a global genetic algorithm population. When a new mobile device connects, a subset of the global genetic population is sent to establish and evolve a new local population.</text:p><text:p text:style-name="P85"/><text:p text:style-name="P85"><text:soft-page-break/>Genetic algorithms show promise in forecasting financial time series using formula-generated predictors. Their nature enables a high degree of parallel computations, creating efficient distributed computing systems, even on modern mobile devices.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P88">The alternative approach treats time series as signals (Korenberg [100]) in the temporal domain. The measured values are utilized to construct approximation curves. The expectation is that when a specific curve fits the known data, it will exhibit forecasting capabilities for future unknown values. Determining coefficients for the chosen mathematical formulas constitutes a complex optimization problem. The size of the variable space increases during the model refinement process, rendering the optimization problem even more challenging.</text:p><text:p text:style-name="P88"/><text:p text:style-name="P88">The Fast Fourier Transform is the most commonly used method for signal decomposition encountered in the literature (Stankovic [101]). It is efficient in terms of time consumption and efficiency due to its classification within the group of exact numerical methods. Despite these advantages, it does not determine the minimum number of sine functions to be used in the interpolation process and subsequent extrapolation. The generalization capability of a sinusoidal approximation tool is closely tied to performing calculations with small numbers and identifying optimal values for the coefficients used.</text:p><text:p text:style-name="P88"><text:soft-page-break/></text:p><text:p text:style-name="P88">Time series represent points measured in a temporal sequence. These discrete measures allow for the fitting of curves based on them. For instance, Lagrange Interpolating Polynomials are among the most popular tools in this field. It is widely acknowledged that an infinite number of curves can fit a finite number of discrete points. In signal processing, using sine functions for curve fitting is prevalent. In many financial time series, numerous fluctuations occur. Financial time series are excellent candidates for approximation using sine functions. Nearly all time series exhibit a linear component known as a trend. This trend can be easily approximated with the equation of a line (Equation 1). The summation of sine functions can then approximate the fluctuations.</text:p><text:p text:style-name="P87"/><text:p text:style-name="P87"><text:span text:style-name="T25">y(t)=B.t+C+A1.sin(ω1.t+φ1)+A2.sin(ω2.t+φ2)+A3.sin(ω3.t+φ3)+…</text:span><text:tab/><text:tab/><text:span text:style-name="T24">(1)</text:span></text:p><text:p text:style-name="P87"/><text:p text:style-name="P87">Here, 𝐵 represents the slope of the linear component, 𝐶 signifies the intercept of the linear component. Additionally, <text:span text:style-name="T27">An</text:span> denotes the amplitude of the 𝑛 sine function, <text:span text:style-name="T27">ωn</text:span> stands for the angular velocity of the 𝑛 sine function, and <text:span text:style-name="T27">φn</text:span> refers to the phase of the 𝑛 sine function. The incremental aspect of the approximation involves initially optimizing the linear component alone. All sine amplitudes, angular velocities, and phases are set to zero. Once the linear component is estimated, the coefficients of the first sine function are optimized. The optimization <text:soft-page-break/>process halts when there is no further convergence. Subsequently, the second sine function is optimized, followed by others hierarchically. Sine functions are continually added until a significant difference in the achieved minimum mean square root error exists. The mean square root error is calculated between time series measures and the approximated values.</text:p><text:p text:style-name="P87"/><text:p text:style-name="P87">For determining the coefficients in the equation, the hybrid optimization algorithm, which combines Differential Evolution and Particle Swarm Optimization, can be highly effective. It is ideally implemented as a plug-in in LibreOffice.</text:p><text:p text:style-name="P86"/><text:p text:style-name="P86"><draw:frame draw:style-name="fr1" draw:name="Frame35" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="68"><draw:text-box fo:min-height="2.8717in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image35" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="2.8717in" style:rel-height="scale" draw:z-index="69"><draw:image xlink:href="Pictures/100000010000073600000421596AAF242BB433B0.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure34" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">35</text:sequence>: Coefficients for linear approximation</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P86"><text:soft-page-break/></text:p><text:p text:style-name="P86">The incremental optimization process begins by seeking optimal values used as coefficients in the linear equation (Figure 35). Once the trend has been estimated, the coefficients for the first sine function are targeted for optimization (Figure 36). The second sine function is added after achieving the most accurate possible approximation with a single sine function.</text:p><text:p text:style-name="P86"/><text:p text:style-name="P86"><draw:frame draw:style-name="fr1" draw:name="Frame36" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="70"><draw:text-box fo:min-height="2.8717in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image36" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="2.8717in" style:rel-height="scale" draw:z-index="71"><draw:image xlink:href="Pictures/1000000100000736000004214FAE0EB099A61E73.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure35" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">36</text:sequence>: Coefficients for the first sine function</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P86"/><text:p text:style-name="P86"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame37" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="72"><draw:text-box fo:min-height="2.8717in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image37" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="2.8717in" style:rel-height="scale" draw:z-index="73"><draw:image xlink:href="Pictures/1000000100000736000004214645DF8BAB5EFE36.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure36" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">37</text:sequence>: Coefficients for the second sine function</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P86"/><text:p text:style-name="P86">Visually, it becomes apparent that a linear component with two sine functions (Figure 37) is adequate for forecasting with the provided input data. Adding a third sine function (Figure 38) demonstrates that two sine functions are sufficient, as observing a significantly better minimum in the optimization target cell becomes challenging. The incremental approximation process reaches this threshold because excessive sine functions can lead to undesired model over-fitting.</text:p><text:p text:style-name="P86"/><text:p text:style-name="P86"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame38" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="74"><draw:text-box fo:min-height="2.8717in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image38" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="2.8717in" style:rel-height="scale" draw:z-index="75"><draw:image xlink:href="Pictures/1000000100000736000004217966A7B0520D76C8.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure37" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">38</text:sequence>: Coefficients for the third sine function</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P86"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc2690_3397589795"/>7.2 Fitness Function <text:span text:style-name="T23">as</text:span> Curve<text:bookmark-end text:name="__RefHeading___Toc2690_3397589795"/></text:h><text:p text:style-name="P67"/><text:p text:style-name="P67">The foundation of genetic algorithms lies in a collection of potential optimal solutions referred to as the population. Typically, the initial population is randomly generated <text:span text:style-name="T28">(Maaranen [</text:span><text:span text:style-name="T29">102</text:span><text:span text:style-name="T28">])</text:span>. Optimization progresses through epochs consisting of new generations. These new generations come into being through crossover and mutation processes. Mating between individuals occurs after the selection of the better-adapted individuals. The fitness value of each individual is computed by applying the candidate solution to the optimized target function. The <text:soft-page-break/>effectiveness of genetic algorithms is closely linked to the swift calculation of the fitness value. However, when the target function is highly time-consuming, the efficiency of genetic algorithms diminishes significantly.</text:p><text:p text:style-name="P67"/><text:p text:style-name="P68">Each individual in the genetic algorithm population is an n-dimensional vector in the n-dimensional space. Supplying the n-dimensional vector from the variable space to the target function produces a target function value. The fitness value is used to measure the candidate solution&apos;s goodness (Katoch [103]).</text:p><text:p text:style-name="P68"/><text:p text:style-name="P68">An evolutionary process organizes the creation of offspring, which leads to new generations within the population. This process begins with a mating procedure called selection. During the selection phase, the better-fitted individuals produce offspring (Drezner [104]). The aim is that better-fitted individuals have a higher likelihood of generating even better solutions.</text:p><text:p text:style-name="P68"/><text:p text:style-name="P68">A crossover operation is applied to the selected parents. During the crossover procedure, the parents exchange some genes (Singh [105]). To explore various parts of the variable space, the mutation is applied to the newly produced individuals (Drezner [104]).</text:p><text:p text:style-name="P68"/><text:p text:style-name="P68"><text:soft-page-break/>The mutation randomly selects a gene and modifies it. The offspring are then evaluated using the target function, and based on the achieved fitness values, a decision is made whether to include them in the next generation or discard them.</text:p><text:p text:style-name="P68"/><text:p text:style-name="P68">An elitism rule is employed in certain genetic algorithm implementations (Rao [106]). This rule stipulates that some best-fitted individuals survive in each new generation.</text:p><text:p text:style-name="P67"/><text:p text:style-name="P69">The Lagrange Polynomial is a well-established mathematical tool for interpolation. It represents a specific polynomial of the lowest possible degree capable of interpolating a given set of known points (Yeh [107]). These points are paired in 2D Euclidean space as (xi, yi). For k points, the polynomial&apos;s degree is less than or equal to k. An essential characteristic of the Lagrange Polynomial is its uniqueness.</text:p><text:p text:style-name="P69"/><text:p text:style-name="P69">When constructing the Lagrange Polynomial, a trade-off must be considered between achieving a better fit and obtaining a smoother fitting function. As the number of known points increases, the polynomial&apos;s degree also increases - a higher polynomial degree results in more significant oscillations and less smooth interpolation. Lagrange Polynomials with higher degrees are less suitable for predictor functions.</text:p><text:p text:style-name="P67"/><text:p text:style-name="P71"><text:soft-page-break/>As previously mentioned, the efficiency of searching for optimal solutions is highly dependent on numerous calculations of the target function. Each new candidate solution, attained after recombination (crossover and mutation), must be evaluated to acquire a fitness value, determining survival and mating chances.</text:p><text:p text:style-name="P71"/><text:p text:style-name="P71">The optimization process is relatively fast when the target function is not time-consuming. Many target functions can be quickly calculated, but there are instances where they are exceedingly time-consuming. Examples of such time-consuming target functions include Monte Carlo simulated results and bitmap image comparison. Calculating target functions in combinatorial problems can also be time-intensive. Additionally, there is an added delay in fitness value calculation when the target function depends on human-computer interaction.</text:p><text:p text:style-name="P71"/><text:p text:style-name="P71">While the original fitness function calculation cannot be avoided, efforts can be made to reduce the number of its calculations. The present research proposes a Lagrange Polynomial approximation of the fitness function. Coefficients for a distinct Lagrange Polynomial are calculated for each dimension within the multidimensional search space. With an n-dimensional search space, there will be n Lagrange Polynomials. The average of these Lagrange functions will be employed as a fitness value.</text:p><text:p text:style-name="P71"/><text:p text:style-name="P71"><text:soft-page-break/>The approximation process begins with several calculations of the original fitness function, establishing the initial pool of the best solutions found. The pool size determines the number of points used to compute Lagrange Polynomials, with its exact size defining the maximum degree of the polynomial. A higher degree of the polynomial introduces more oscillations in the interpolating curve, as is commonly known.</text:p><text:p text:style-name="P71"/><text:p text:style-name="P71">Hence, the size of the pool of best-found solutions becomes a self-adapting parameter. The time taken to find a better solution for pool inclusion is the self-adapting criterion, where less time indicates a better approximation.</text:p><text:p text:style-name="P70"/><text:p text:style-name="P70">Solutions evaluated as superior by the approximate fitness function are not guaranteed superior according to the original fitness function. Therefore, any solution with an approximation-based fitness value superior to all other known solutions is also evaluated with the original fitness function. Upon discovering a new best solution based on the original fitness function, it joins the pool of points for computing the Lagrange Polynomial, prompting a recomputation of all polynomials. Simultaneously, the worst-rated solution is removed as a new solution enters the pool.</text:p><text:p text:style-name="P70"/><text:p text:style-name="P70"><text:soft-page-break/>The proposed approximation significantly expedites the evaluation of individuals in a genetic algorithm population for problems wherein the fitness function requires hours or days for a single calculation. This capability enables the application of meta-heuristics for global optimization in areas where their use was previously hindered due to the time-consuming evaluation of intermediate candidate solutions.</text:p><text:p text:style-name="P67"/><text:p text:style-name="P72">Exploring alternatives like B-Splines instead of Lagrange Polynomials <text:s/>expand<text:span text:style-name="T30">s</text:span> the scope of the current proposal.</text:p><text:p text:style-name="P72"/><text:p text:style-name="P72">B-Splines offer advantages in terms of flexibility and smoothness. They are known for their ability to represent complex curves and surfaces effectively. Implementing B-Splines in this context could enhance the accuracy of fitness function approximation, especially in scenarios where Lagrange Polynomials might introduce too many oscillations or struggle to capture intricate relationships within the fitness landscape.</text:p><text:p text:style-name="P72"/><text:p text:style-name="P72">Utilizing B-splines would define the control points and knot vectors to construct the spline functions. This approach could lead to a more nuanced representation of the fitness function across the multidimensional search space.</text:p><text:p text:style-name="P72"/><text:p text:style-name="P72"><text:soft-page-break/>Introducing B-Splines as an alternative could be a valuable extension to the research, offering insights into the comparative performance and suitability of different interpolation techniques for approximating fitness functions in optimization processes.</text:p><text:p text:style-name="P131"/><text:h text:style-name="P114" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc7840_527140494"/>Summary<text:bookmark-end text:name="__RefHeading___Toc7840_527140494"/></text:h><text:p text:style-name="P131"/><text:p text:style-name="P132">Chapter 7 focuses on leveraging evolutionary algorithms with curve-fitting techniques for predictive modeling, particularly in time series forecasting. The chapter delves into applying curve fitting in forecasting, emphasizing its utility across various domains like finance, statistics, signal processing, and more.</text:p><text:p text:style-name="P132"/><text:p text:style-name="P132">It explores two main approaches: direct curve fitting and treating time series as signals. In direct curve fitting, methods like linear regression and Lagrange polynomials approximate data points. This section discusses how artificial neural networks can be viewed as curve-fitting tools and how genetic algorithms can optimize them.</text:p><text:p text:style-name="P132"/><text:p text:style-name="P132">The chapter elaborates on fitting curves using sine functions, especially in financial time series, where a linear trend combined with sine functions can approximate fluctuations. It details an incremental optimization <text:soft-page-break/>process for determining coefficients in such equations, employing hybrid optimization algorithms like Differential Evolution and Particle Swarm Optimization.</text:p><text:p text:style-name="P132"/><text:p text:style-name="P132">Another section introduces the concept of using the fitness function as a curve. Genetic algorithms, which rely on populations of potential solutions, are explained along with their evolution through crossover, mutation, and elitism. The Lagrange Polynomial is presented to balance accuracy and smoothness in interpolation. At the same time, a novel approach of approximating the fitness function through Lagrange Polynomials is proposed to expedite evaluations in computationally intensive scenarios.</text:p><text:p text:style-name="P132"/><text:p text:style-name="P132">Toward the end, the chapter suggests exploring B-Splines as an alternative to Lagrange Polynomials. It highlights their advantages in representing complex relationships and enhancing accuracy in fitness function approximation within multidimensional search spaces. The introduction of B-Splines is a potential extension, offering insights into the performance and suitability of different interpolation techniques in optimization processes.</text:p><text:p text:style-name="P131"/><text:h text:style-name="P107" text:outline-level="1"><text:bookmark-start text:name="__RefHeading___Toc2353_1821873452"/>8 Evolutionary Algorithms in Multi-<text:span text:style-name="T39">О</text:span>bjective Problems <text:span text:style-name="T36">and Multi-Modal Functions</text:span><text:bookmark-end text:name="__RefHeading___Toc2353_1821873452"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P92">Multi-objective optimization is a part of general optimization, holding significant practical importance as many real-world problems are amenable to modeling with multiple conflicting objectives (Arora [108], Abraham [109]). The prevalent approach to solving such problems primarily revolves around transforming multiple objectives into a single objective, including issues related to group decision-making.</text:p><text:p text:style-name="P92"/><text:p text:style-name="P92">Many practical optimization problems are inherently defined as nonlinear problems featuring multiple conflicting objectives (Zitzler [110]). Due to the lack of proper solution techniques, these problems undergo mathematical transformation into single-objective problems, allowing the utilization of well-established approaches for solving single-objective problems. The complexity arises from multi-objective problems yielding a set of trade-off optimal solutions (Pareto-optimal set (Zitzler [111])). In contrast, single-objective problems generally lead to a singular optimum solution. Multi-objective optimization aims to identify as many Pareto-optimal solutions as possible. The abundance of Pareto-optimal solutions <text:soft-page-break/>is crucial. Each pairwise combination represents a trade-off among the objectives, offering the decision-maker a more comprehensive array of options for compromising between objectives (Deb [112]).</text:p><text:p text:style-name="P92"/><text:p text:style-name="P92">In multi-objective optimization, two or more functions aim for the optimal value using the same input vector. If the input vector is denoted by <text:span text:style-name="T27">𝑥</text:span>, its components can be represented as <text:span text:style-name="T27">{𝑥</text:span><text:span text:style-name="T31">1</text:span><text:span text:style-name="T27">,…,𝑥</text:span><text:span text:style-name="T31">𝑛</text:span><text:span text:style-name="T27">}</text:span>. The presence of multiple functions for simultaneous optimization <text:span text:style-name="T27">{𝑓</text:span><text:span text:style-name="T31">1</text:span><text:span text:style-name="T27">,…,𝑓</text:span><text:span text:style-name="T31">𝑚</text:span><text:span text:style-name="T27">}</text:span> (Emmerich [113]) gives rise to two optimization spaces – the search space of variables and the search space of objectives (Deb [114]). The variables may have lower and upper bounds as potential values alongside finite constraints. Additionally, linear and nonlinear equations/inequalities may be specified for compliance. When only one objective or constraint exhibits a nonlinear nature, the problem becomes nonlinear.</text:p><text:p text:style-name="P92"/><text:p text:style-name="P92">The most utilized scalarization formula is the additive model (Miettinen [115]). In this model, each objective carries an associated weight (Marler [116]). The weighted objectives are generalized as follows:</text:p><text:p text:style-name="P91"/><text:p text:style-name="P91"><text:span text:style-name="T26">𝑓 = 𝑤</text:span><text:span text:style-name="T32">1</text:span><text:span text:style-name="T26">𝑓</text:span><text:span text:style-name="T32">1</text:span><text:span text:style-name="T26">(𝑥) + ... + 𝑤</text:span><text:span text:style-name="T32">𝑚</text:span><text:span text:style-name="T26">𝑓</text:span><text:span text:style-name="T32">𝑚</text:span><text:span text:style-name="T26">(𝑥)</text:span><text:tab/><text:tab/><text:tab/><text:tab/><text:tab/><text:tab/><text:tab/>(2)</text:p><text:p text:style-name="P91"/><text:p text:style-name="P91"><text:soft-page-break/>All objective functions require normalization to obtain a dimensionless function value (Equation 2). Various schemes, as demonstrated in (Marler [117]), can be employed to achieve these normalizations.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc6444_464923766"/><text:span text:style-name="T40">8</text:span>.1 <text:span text:style-name="T33">LibreOffice Calc NLP Solver</text:span><text:bookmark-end text:name="__RefHeading___Toc6444_464923766"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P90">LibreOffice is free, open-source software that provides applications tailored for small office needs. The software primarily comprises the following modules: Writer (for text processing), Calc (for spreadsheets), Impress (for presentations), Draw (for vector graphics), Math (for mathematical formulas), and Base (for databases). Within the spreadsheet module, users can activate linear and nonlinear solvers (Tools -&gt; Solver) to address optimization problems. Nonlinear optimization problems are particularly intriguing due to their complexity compared to linear problems. The NLP Solver submodule in LibreOffice Calc is a hybrid combination of Differential Evolution and Particle Swarm Optimization.</text:p><text:p text:style-name="P90"/><text:p text:style-name="P90">The NLP Solver is written in Java as of the current stable release. However, the development team intends to rewrite this submodule in C++, though this is a medium- to long-term task. The NLP Solver in LibreOffice Calc is designed to find solutions for single-objective tasks exclusively. Consequently, users are provided only one solution, even when the task is <text:soft-page-break/>multimodal and possesses multiple global optima. The heuristic nature of both algorithms in the solver means that the suggested solutions do not guarantee global optimality. Generally, these solutions are proximate to the global optimum or one of the global optima in cases where multiple optima exist.</text:p><text:p text:style-name="P73"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc2625_1241039964"/><text:span text:style-name="T34">8.2 </text:span>Solving Multi-Objective Problems by Single Objective Solver<text:bookmark-end text:name="__RefHeading___Toc2625_1241039964"/></text:h><text:p text:style-name="P73"/><text:p text:style-name="P74">The LibreOffice NLP solver is applicable for multi-objective tasks, and while it is feasible to integrate the necessary software functionality into the Java code of the solver, undertaking such work is exceedingly intricate and time-consuming, even for seasoned software professionals. The presence of scripting languages like Python in LibreOffice offers a far more straightforward approach to tackling multi-objective tasks with a single-objective solver <text:span text:style-name="T35">( https://github.com/TodorBalabanov/Multi-Objective-Optimization-with-LibreOffice-Solver )</text:span>.</text:p><text:p text:style-name="P74"/><text:p text:style-name="P74">The initial step in this calculation involves assigning a weighting factor to each objective, as illustrated in Equation 2. Subsequently, the second step pertains to the solver only providing a singular solution. The solver must be initiated multiple times to compute solutions proximate to the Pareto <text:soft-page-break/>front. For each solver initiation, different weighting coefficients for the objectives are chosen. These coefficient variations signify the capacity of the decision-maker to ascertain varying degrees of importance for individual objectives.</text:p><text:p text:style-name="P74"/><text:p text:style-name="P74">Once an initial set of solutions is obtained, the decision-maker can manually adjust the objective coefficients and trigger the solver to generate new solutions near the Pareto front.</text:p><text:p text:style-name="P73"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc4553_2316191826"/>8.3 Random Numbers in Optimization Metaheuristics<text:bookmark-end text:name="__RefHeading___Toc4553_2316191826"/></text:h><text:p text:style-name="P73"/><text:p text:style-name="P75">When optimizing metaheuristics, such as differential evolution and particle swarm optimization, a true random number generator ensures the statistical quality of the generated random numbers. However, access to a true random number generator is rare in practical scenarios. Consequently, various pseudo-random number generators are employed, each exhibiting distinct statistical qualities in the generated numbers. In this context, the impact of employing different pseudo-random number generators is explored within a hybrid implementation of differential evolution and particle swarm optimization.</text:p><text:p text:style-name="P75"/><text:p text:style-name="P75"><text:soft-page-break/>The Linear Congruent Generator (Class Random in Java SE) stands out as the most widely used among the pseudo-random number generators commonly utilized in practice. The experimental phase of this analysis involves Java source code integrated into the LibreOffice Calc NLP Solver. The standard Java distribution provides both the LCG and SecureRandom generators (Class SecureRandom in Java SE), prompting a comparative assessment. Notably, a significant discrepancy between these generators lies in their performance. This discrepancy raises a fundamental question regarding whether statistically superior numbers can compensate for speed differentials, potentially leading to faster convergence in optimization processes.</text:p><text:p text:style-name="P73"/><text:p text:style-name="P77">LibreOffice&apos;s current stable release version as of the time of writing is 7.0.6.2. All experiments described herein were conducted using a developmental version of the software (7.3.0.0.alpha0+). This developmental version was utilized due to the released version&apos;s absence of random number generator functionality. A distinct flag has been incorporated into Calc&apos;s NLP Solver settings dialog to toggle between the LCG and SecureRandom generators (Figure 39).</text:p><text:p text:style-name="P77"/><text:p text:style-name="P77"><text:soft-page-break/><draw:frame draw:style-name="fr2" draw:name="Frame39" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="4.0882in" style:rel-height="scale-min" draw:z-index="76"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image39" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="3.7in" style:rel-height="scale" draw:z-index="77"><draw:image xlink:href="Pictures/10000000000001C20000014C0DFA10870066209C.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure38" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">39</text:sequence>: The user interface for switching random number generators</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P77"/><text:p text:style-name="P77">For the benchmark model, a feed-forward artificial neural network training approach is employed ( https://github.com/VelbazhdSoftwareLLC/LibreOffice-Calc-Three-Layers-Perceptron-Builder ). This neural network is trained explicitly for forecasting financial time series, as documented by Tomov [118]. It consists of three layers—input, hidden, and output—with a total weight count of 555.</text:p><text:p text:style-name="P76"/><text:p text:style-name="P76"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame40" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="78"><draw:text-box fo:min-height="2.8339in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image40" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="2.8339in" style:rel-height="scale" draw:z-index="79"><draw:image xlink:href="Pictures/10000000000002D20000019853CC6D0A6327CBFC.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure39" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">40</text:sequence>: Optimization convergence with Linear Congruential Generator</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P76"/><text:p text:style-name="P76"><draw:frame draw:style-name="fr1" draw:name="Frame41" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="80"><draw:text-box fo:min-height="2.8043in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image41" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="2.8043in" style:rel-height="scale" draw:z-index="81"><draw:image xlink:href="Pictures/10000000000002D800000197901E14A8B25A4F82.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure40" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">41</text:sequence>: Optimization convergence with SecureRandom generator</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P76"/><text:p text:style-name="P76"><text:soft-page-break/>The findings (Figures 40 and 41) unequivocally demonstrate the justification for employing a generator that produces statistically superior numbers within LibreOffice Calc&apos;s NLP Solver module. This utilization does not negatively impact performance; in some scenarios, it enhances optimization convergence.</text:p><text:p text:style-name="P76"/><text:p text:style-name="P76">Further research guidelines may encompass conducting more comprehensive tests on various generators. Observing the SecureRandom generator&apos;s behavior across diverse operating systems and hardware setups is crucial. Additionally, studying optimization models with varying levels of complexity and nonlinearity remains pivotal.</text:p><text:p text:style-name="P73"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc14058_2316191826"/>8.4 Multidimensionality and Performance<text:bookmark-end text:name="__RefHeading___Toc14058_2316191826"/></text:h><text:p text:style-name="P73"/><text:p text:style-name="P78">Numerous types of optimization tasks exist in practical applications. Several efficient and accurate numerical methods have been developed specifically for linear optimization problems characterized by a linear objective function and constraints (Fang [119]). Challenges become pronounced when optimization problems involve any form of nonlinearity (Scales [120]). Nonlinear optimization problems can encompass infinite nonlinear functions, and even the simplest nonlinearity, such as a second-degree function, presents considerable challenges.</text:p><text:p text:style-name="P78"><text:soft-page-break/></text:p><text:p text:style-name="P78">Four popular test functions—Rastrigin, Sphere, Rosenbrock, and Styblinski–Tang (Peng [121])—have been chosen for the current analysis. These functions are notable for their ability to be computed across significant dimensionalities (axes within multidimensional spaces). The pursuit of optimal and suboptimal values is conducted using the LibreOffice Calc Nonlinear Solver (Dasovic [122]).</text:p><text:p text:style-name="P73"/><text:p text:style-name="P73"><draw:frame draw:style-name="fr1" draw:name="Frame42" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="82"><draw:text-box fo:min-height="3.8799in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image42" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="3.8799in" style:rel-height="scale" draw:z-index="83"><draw:image xlink:href="Pictures/10000001000009CD0000079539A62E276149DBF3.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure41" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">42</text:sequence>: Achieved sub-optimal values</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P73"/><text:p text:style-name="P73"><text:soft-page-break/>Independent experiments were conducted for each of the four functions. These functions were utilized across various dimensions: 1, 5, 10, 50, 100, 500, and 1000. However, when extending beyond 1000 dimensions, LibreOffice Calc significantly slows down, making obtaining results within an acceptable computing time impractical on the modern desktop computer configuration.</text:p><text:p text:style-name="P73"/><text:p text:style-name="P73">For each of the four functions, within a space of up to 100 dimensions, the solver consistently identifies solutions close to the global optimum (Figure 42). However, attaining the global optimum becomes challenging as the dimensions sharply increase.</text:p><text:p text:style-name="P93"/><text:p text:style-name="P93"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame43" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="84"><draw:text-box fo:min-height="3.8992in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image43" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="3.8992in" style:rel-height="scale" draw:z-index="85"><draw:image xlink:href="Pictures/10000001000009D9000007A8E5DC299E8BC46B3E.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure42" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">43</text:sequence>: Number of generations</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P79"/><text:p text:style-name="P73">The algorithm stagnates for three functions within spaces up to 100 dimensions and does not utilize the promised 2000 optimization cycles (Figure 43). The spherical function, however, exhibits stagnation at around 500 dimensions.</text:p><text:p text:style-name="P94"/><text:p text:style-name="P94"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame44" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="86"><draw:text-box fo:min-height="3.9016in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image44" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="3.9016in" style:rel-height="scale" draw:z-index="87"><draw:image xlink:href="Pictures/10000001000009D9000007A9CF0F7CB46FFA9BF9.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure43" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">44</text:sequence>: Computation time</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P79"/><text:p text:style-name="P79">Across all four functions, the computational time, capped at a maximum of 2000 optimization cycles, escalates rapidly in direct proportion to the expansion of dimensions within the multidimensional space, as depicted in (Figure 44).</text:p><text:p text:style-name="P73"/><text:p text:style-name="P80">The measurements obtained are heavily influenced by how functions are presented in LibreOffice Calc. Each dimension&apos;s value is calculated using a formula placed on individual rows within the worksheet. The process <text:soft-page-break/>involves computing many formulas, potentially up to 1000 in the worst-case scenario, and summing them to derive a final value in a target cell, causing a considerable delay.</text:p><text:p text:style-name="P80"/><text:p text:style-name="P80">The increased computation time in higher dimensions primarily stems from the spreadsheet&apos;s mechanism for processing formulas rather than the complexity of computing the test functions.</text:p><text:p text:style-name="P80"/><text:p text:style-name="P80">The results indicate that the optimization module&apos;s ability to ascertain the global optimum diminishes with the expansion of variable space dimensionality. Another significant observation from the results is that the optimization module&apos;s efficiency heavily relies on the initial starting point chosen for commencing the optimization process.</text:p><text:p text:style-name="P133"/><text:h text:style-name="P115" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc7842_527140494"/>Summary<text:bookmark-end text:name="__RefHeading___Toc7842_527140494"/></text:h><text:p text:style-name="P133"/><text:p text:style-name="P134">Chapter 8 delves into the complexities of multi-objective optimization and the challenges faced when addressing problems with conflicting objectives. It highlights the significance of multi-objective optimization in real-world scenarios and the hurdles in transforming multiple objectives into one for conventional optimization approaches. The chapter introduces the concept of Pareto-optimal solutions, crucial in multi-<text:soft-page-break/>objective optimization, and emphasizes their importance in offering decision-makers various trade-off options among objectives.</text:p><text:p text:style-name="P134"/><text:p text:style-name="P134">Within this realm, it explores the limitations of existing tools, particularly LibreOffice Calc&apos;s NLP Solver, designed for single-objective tasks. Despite its limitations, the chapter discusses a novel approach utilizing scripting languages like Python to adapt the single-objective solver for multi-objective tasks, presenting steps for employing this technique.</text:p><text:p text:style-name="P134"/><text:p text:style-name="P134">The chapter also investigates the role of random number generators in optimization metaheuristics, comparing the impact of different generators in hybrid implementations. It underlines the significance of employing generators that produce statistically superior numbers within the NLP Solver module, showcasing their potential to enhance optimization convergence without compromising performance.</text:p><text:p text:style-name="P134"/><text:p text:style-name="P134">Furthermore, the chapter examines the performance of optimization tasks across varying dimensionalities using popular test functions. It demonstrates the solver&apos;s ability to find solutions close to the global optimum in lower dimensions but highlights challenges in higher dimensions, attributing computational delays more to spreadsheet processing mechanisms than function complexity. It notes the influence of the initial starting point on optimization efficiency.</text:p><text:p text:style-name="P134"><text:soft-page-break/></text:p><text:p text:style-name="P134">In essence, the chapter explores the intricacies of multi-objective optimization, offers insights into adapting single-objective solvers, evaluates random number generators&apos; impact, and discusses the solver&apos;s performance across different dimensionalities, shedding light on the complexities and limitations in tackling such problems.</text:p><text:p text:style-name="P133"/><text:h text:style-name="P107" text:outline-level="1"><text:bookmark-start text:name="__RefHeading___Toc2355_1821873452"/>9 Modification of Components in Evolutionary Algorithms<text:bookmark-end text:name="__RefHeading___Toc2355_1821873452"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P95">In evolutionary computing, primarily within metaheuristics for global optimization, the ability to tweak different elements holds immense potential. When we delve into tasks requiring specific solutions, altering components within evolutionary metaheuristics becomes incredibly beneficial.</text:p><text:p text:style-name="P95"/><text:p text:style-name="P95">These modifications are typically carried out in crucial stages of the evolutionary process, such as selection, crossover, mutation, and elitism operations. Adjusting these elements allows for tailoring the optimization process to suit the problem better.</text:p><text:p text:style-name="P95"/><text:p text:style-name="P95">The beauty of these modifications lies in their potential to streamline computational efficiency. By fine-tuning the selection criteria, adjusting crossover techniques, refining mutation rates, or optimizing elitism strategies, we can aim for solutions that are more fitting and demand fewer computational resources. This reduction in computational time is pivotal in tackling complex problems efficiently, allowing for quicker iterations and improved convergence toward optimal solutions.</text:p><text:p text:style-name="P95"/><text:p text:style-name="P95"><text:soft-page-break/>The significance of these modifications lies not just in their potential to enhance the quality of solutions but also in their ability to make the optimization process more time-effective and resource-efficient, thereby advancing the applicability of evolutionary computing in solving real-world problems.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc3714_2324440366"/>9.1 DNA Inspired Encoding<text:bookmark-end text:name="__RefHeading___Toc3714_2324440366"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P96">The fundamental concept behind genetic algorithms is to emulate the process of natural selection and reproduction to generate a population of potential solutions to a problem. The computation begins with a random population of candidate solutions and then applies operators such as selection, crossover, and mutation (Lambora [123]) to create new generations of candidate solutions.</text:p><text:p text:style-name="P96"/><text:p text:style-name="P96">RNA (ribonucleic acid) is a molecule that plays a crucial role in genetic information transfer in living organisms. Just as RNA serves as a type of genetic code carrying information from DNA to ribosomes, genetic algorithms utilize a population of candidate solutions as a form of genetic code conveying information about potential solutions to a problem. Like RNA, genetic algorithms utilize selective pressure and random mutation to generate diverse populations of candidate solutions, which are then <text:soft-page-break/>evaluated based on their fitness or ability to solve the problem at hand. The evolution of these populations over time mirrors how genetic information evolves in living organisms, with the fittest solutions surviving and propagating while less fit solutions die out. Heuristics in genetic algorithms are a specific simulation of the processes representing evolution in the real world. In this proposition, single genetic chromosomes are paired to mimic DNA structure, where each candidate solution has its complementary part. Both solutions form complementary pairs.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P97">As is widely known, the population of widely used genetic algorithms consists of chromosomes (or individuals). Each chromosome has its existence within the population, mimicking the RNA structure in the biological world.</text:p><text:p text:style-name="P97"/><text:p text:style-name="P97">While binary encoding is the most common method in genetic algorithms, it may not always be the best choice for every optimization or simulation problem. Real-number encoding is often more suitable for problems involving real numbers like benchmark functions such as Rastrigin, Sphere, Rosenbrock, and Styblinski-Tang (Jamil [124]). Real-number encoding allows a more direct representation of the problem domain; the genes in the chromosome represent real values instead of binary values. <text:soft-page-break/>This representation can lead to faster convergence and better solutions for specific problems, especially those involving continuous variables.</text:p><text:p text:style-name="P97"/><text:p text:style-name="P97">Various methods for real-number encoding exist, such as floating-point or Gray encoding. In floating-point encoding, the genes in the chromosome are represented as floating-point numbers. In contrast, in Gray encoding, the genes are represented as a series of binary digits converted to real numbers using a specific formula.</text:p><text:p text:style-name="P97"/><text:p text:style-name="P97">Extending the analogy of genetic algorithms to paired DNA strands is possible. In this approach, each chromosome in the population would have a complementary pair, akin to the base pairs in DNA strands. This approach is sometimes referred to as double-stranded genetic algorithms (DSGAs) (Zang [125]). In DSGAs, each chromosome is paired with another to form a complementary pair, which is then treated as a single entity during selection, crossover, and mutation operations.</text:p><text:p text:style-name="P97"/><text:p text:style-name="P97">Using paired chromosomes in DSGAs can offer several advantages over traditional genetic algorithms. For instance, paired chromosomes can allow for more efficient search space exploration, as the complementary pairs provide additional information that aids in the search process. Additionally, paired chromosomes can more effectively handle constraints in optimization problems.</text:p><text:p text:style-name="P97"><text:soft-page-break/></text:p><text:p text:style-name="P97">However, it is essential to note that DSGAs may be more computationally expensive than traditional genetic algorithms since they require maintaining complementary pairs and executing operations on the pairs rather than individual chromosomes.</text:p><text:p text:style-name="P97"/><text:p text:style-name="P97">The proposition involves organizing chromosomes as real-number twins with opposite signs in the genetic algorithm&apos;s population, sometimes called real-number twin optimization. In this approach, each chromosome in the population is represented by two twins (Yang [126]), where the values in one twin have opposite signs. This approach leverages the universality of real-number encoding, which allows encoding most problems as real numbers.</text:p><text:p text:style-name="P97"/><text:p text:style-name="P97">One advantage of using twin chromosomes is that they share a single fitness value, which is better calculated for each twin. This helps avoid premature convergence to suboptimal solutions, as the twin chromosomes provide complementary information about the search space.</text:p><text:p text:style-name="P97"/><text:p text:style-name="P97">Furthermore, using twin chromosomes can also help maintain diversity in the population. Even when one of the twins has significantly worse fitness, they survive together between generations.</text:p><text:p text:style-name="P97"><text:soft-page-break/></text:p><text:p text:style-name="P97">However, it is worth noting that using twin chromosomes may come with additional computational costs, as the opposite sign operation must be applied to each gene in the chromosome. Additionally, using twin chromosomes may not be the best choice for problems that do not lend themselves well to real-number encoding.</text:p><text:p text:style-name="P97"/><text:p text:style-name="P97">The encoding for the benchmark function involves real numbers, but the implementation of the mutation is done as binary data. The crossover is retained in the real numbers domain. Real-number twins are presented in a single double-valued array, with the first half representing the first twin and the second half representing the second twin. When real-number twins are generated for the first time, they are formed as opposite sign complements. The complementary pair is evaluated by assessing each twin separately and choosing the two with better fitness.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P108" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc3716_2324440366"/>9.2 Hierarchical Brute Force Selection<text:bookmark-end text:name="__RefHeading___Toc3716_2324440366"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P98">Drawing inspiration from neutral evolution, genetic algorithms operate through three fundamental processes: selection, crossover, and mutation. While crossover and mutation introduce new potential solutions, selection plays a crucial role in identifying superior parents.</text:p><text:p text:style-name="P98"><text:soft-page-break/></text:p><text:p text:style-name="P98">Over the past fifty years, numerous selection methods have emerged in literature: Proportional, Tournament, Rank-Based, Boltzmann, Soft Brood, Disruptive, Nonlinear Ranking, and Competitive Selection. A novel addition to this array is a selection operator founded on recursive generation creation. In this approach, individuals within the population mate at each recursion level, with only the most exceptional individuals progressing to higher levels.</text:p><text:p text:style-name="P98"/><text:p text:style-name="P98">The genetic algorithm organizes its population in a hierarchical structure. At the lowest tier, a sub-population forms from randomly generated individuals. Within each sub-population, individuals mate extensively, with the most exceptional offspring advancing to the next level. This cascade continues, constructing upper sub-populations from the cream of the lower levels.</text:p><text:p text:style-name="P98"/><text:p text:style-name="P98">Elaborating on the brute-force mating concept involves mating until a more fitting offspring cannot be produced. While deterministic, this method risks overlooking potentially optimal solutions. However, mating until no improvement is possible guarantees the elevation of the best local solution up the hierarchy.</text:p><text:p text:style-name="P98"/><text:p text:style-name="P98"><text:soft-page-break/>Expanding the size of subpopulations presents a potential enhancement. Beginning with smaller subpopulations at lower levels and gradually increasing their size higher in the hierarchy could significantly impact the genetic algorithm&apos;s performance.</text:p><text:p text:style-name="P98"/><text:p text:style-name="P98">Starting with smaller groups of individuals at the lowest tier allows a more focused exploration of the solution space. This concentrated approach encourages a diverse range of mating and genetic variation. As the algorithm progresses to higher levels, enlarging the subpopulations accommodates the selection of a broader pool of potential solutions.</text:p><text:p text:style-name="P98"/><text:p text:style-name="P98">The advantage lies in the initial stages, where a minor subpopulation facilitates exhaustive exploration of genetic combinations within a confined scope. This fine-tuning at lower levels ensures that only the most promising individuals advance to higher tiers. Subsequently, enlarging the subpopulations at higher levels broadens the genetic diversity and enhances the chances of discovering superior solutions.</text:p><text:p text:style-name="P98"/><text:p text:style-name="P98">By optimizing the subpopulation sizes in this hierarchical manner, the algorithm can balance exploration and exploitation efficiently. It allows for thoroughly exploring diverse genetic combinations and selecting the best-performing individuals for further progression. This strategic adjustment <text:soft-page-break/>in subpopulation size across the hierarchy could refine the genetic algorithm&apos;s performance in finding optimal solutions.</text:p><text:p text:style-name="P98"/><text:p text:style-name="P98">The proposed selection operator shows promise in high-dimensional solution spaces. However, due to its intensive CPU usage, it is most suitable in hybrid implementations, such as initializing the genetic algorithm&apos;s population. Future exploration might focus on replacing the brute-force aspect with a more efficient alternative, enhancing its applicability and efficiency in broader contexts.</text:p><text:p text:style-name="P135"/><text:h text:style-name="P116" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc7844_527140494"/>Summary<text:bookmark-end text:name="__RefHeading___Toc7844_527140494"/></text:h><text:p text:style-name="P135"/><text:p text:style-name="P136">Chapter 9 is on evolutionary algorithms, focusing on the modifications applied to different components within evolutionary metaheuristics. These adjustments, made in crucial stages like selection, crossover, mutation, and elitism operations, aim to tailor the optimization process for better problem-solving.</text:p><text:p text:style-name="P136"/><text:p text:style-name="P136">The chapter begins by exploring DNA-inspired encoding in genetic algorithms, likening chromosomes to RNA structures, and discussing real-number encoding&apos;s advantages for specific problem types. It introduces the concept of double-stranded genetic algorithms, which pair <text:soft-page-break/>chromosomes for more efficient search space exploration, albeit with higher computational costs. Another encoding approach, real-number twin optimization, uses twin chromosomes with opposite signs to enhance diversity and prevent premature convergence.</text:p><text:p text:style-name="P136"/><text:p text:style-name="P136">The chapter introduces hierarchical brute force selection, a novel approach where a hierarchical structure is formed within the population. This method involves recursive generation creation, with individuals mating at each level and only the best offspring progressing upwards. The algorithm balances exploration and exploitation by adjusting subpopulation sizes across hierarchy levels, enabling exhaustive exploration at lower levels and broader diversity at higher tiers.</text:p><text:p text:style-name="P136"/><text:p text:style-name="P136">While these modifications promise to improve solution quality and efficiency, there are trade-offs, such as increased computational costs for some approaches and the need for hybrid implementations. Future exploration could focus on refining these methods for broader applicability and efficiency.</text:p><text:p text:style-name="P135"/><text:h text:style-name="P107" text:outline-level="1"><text:bookmark-start text:name="__RefHeading___Toc7846_527140494"/>Final Remarks<text:bookmark-end text:name="__RefHeading___Toc7846_527140494"/></text:h><text:p text:style-name="P123"/><text:p text:style-name="P124">The book is an in-depth exploration of evolutionary algorithms and their applications across various domains, mainly focusing on distributed computing, optimization challenges, and the complexities of predictive modeling. It covers various topics, from employing Artificial Neural Networks and Differential Evolution for time series prediction in financial markets to using genetic algorithms for solving combinatorial puzzles like the Rubik&apos;s Cube and optimizing symbol distributions in slot machines.</text:p><text:p text:style-name="P124"/><text:p text:style-name="P124">The chapters are detailed and comprehensive, covering:</text:p><text:p text:style-name="P124"/><text:p text:style-name="P124">1. Evolutionary Algorithms in Financial Forecasting: Discusses the challenges of predicting time series, emphasizing the role of ANNs and DE in constructing robust predictive models.</text:p><text:p text:style-name="P124"/><text:p text:style-name="P124">2. Distributed Evolutionary Computing: Explores migration strategies in evolutionary algorithms, particularly the Accidental Node Involvement model, for optimizing distributed populations.</text:p><text:p text:style-name="P124"/><text:p text:style-name="P124">3. Enhancements to Artificial Neural Networks: Proposes modifications to ANNs to improve their efficiency in time series forecasting.</text:p><text:p text:style-name="P124"><text:soft-page-break/></text:p><text:p text:style-name="P124">4. Combinatorial Puzzles and Optimization: Focuses on using genetic algorithms for solving complex combinatorial puzzles like the Rubik&apos;s Cube and optimizing symbol distributions in slot machines.</text:p><text:p text:style-name="P124"/><text:p text:style-name="P124">5. Human Evaluation in Optimization and Distributed Computing: Explores the integration of human evaluation in evolutionary algorithms, especially in music improvisation and financial forecasting.</text:p><text:p text:style-name="P124"/><text:p text:style-name="P124">6. Evolutionary Algorithms in Sound, Image, and 3D Fractal Generation: Discusses the application of evolutionary algorithms in sound and image vectorization and 3D fractal generation.</text:p><text:p text:style-name="P124"/><text:p text:style-name="P124">7. Curve-Fitting Techniques and Time Series Forecasting: Explores the use of curve fitting in predictive modeling, especially in financial time series, employing genetic algorithms and Lagrange Polynomials.</text:p><text:p text:style-name="P124"/><text:p text:style-name="P124">8. Multi-Objective Optimization Challenges: Focuses on addressing problems with conflicting objectives, adapting single-objective solvers for multi-objective tasks, and evaluating random number generators&apos; impact on optimization.</text:p><text:p text:style-name="P124"/><text:p text:style-name="P124"><text:soft-page-break/>9. Modifications in Evolutionary Metaheuristics: Discusses various modifications applied to components within evolutionary algorithms to tailor the optimization process for improved problem-solving.</text:p><text:p text:style-name="P124"/><text:p text:style-name="P124">The book covers a wide spectrum of applications and modifications in evolutionary algorithms, addressing challenges across different domains. It delves into technical details while emphasizing practical applications, making it a comprehensive resource for readers interested in evolutionary computation, optimization, and predictive modeling.</text:p><text:p text:style-name="P123"/><text:bibliography text:style-name="Sect1" text:name="Bibliography1"><text:bibliography-source><text:index-title-template text:style-name="Bibliography_20_Heading">Bibliography</text:index-title-template><text:bibliography-entry-template text:bibliography-type="article" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="book" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="booklet" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="conference" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="custom1" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="custom2" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="custom3" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="custom4" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="custom5" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="email" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="inbook" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="incollection" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="inproceedings" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="journal" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="manual" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="mastersthesis" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="misc" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="url"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="phdthesis" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="proceedings" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="techreport" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="unpublished" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="www" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template></text:bibliography-source><text:index-body><text:index-title text:style-name="Sect1" text:name="Bibliography1_Head"><text:p text:style-name="P17">Bibliography</text:p></text:index-title></text:index-body></text:bibliography><text:p text:style-name="P3"><text:span text:style-name="T6">[1] </text:span>Dunis, C.L. Williams, M., Modelling and trading the eur/usd exchange rate: Do neural network models perform better? Derivatives Use, Trading and Regulation, 8(3), pp. 211-239 (2002)</text:p><text:p text:style-name="P6">[<text:span text:style-name="T7">2</text:span>] Giles, C.L., Lawrence, S. Tsoi, A.C., Noisy time series prediction using a recurrent neural network and grammatical inference. Machine Learning, 44(1/2), pp. 161-183 (2001)</text:p><text:p text:style-name="P7">[3] Moody, J.E., Economic forecasting: Challenges and neural network solutions. Proceedings of the International Symposium on Artificial Neural Networks, Hsinchu, Taiwan (1995)</text:p><text:p text:style-name="P7">[4] Haykin, S., Neural Networks, A Comprehensive Foundation. Prentice-Hall, Inc., 2nd edition (1999)</text:p><text:p text:style-name="P7">[5] Werbos, P., Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78(10), pp. 1550-1560 (1990)</text:p><text:p text:style-name="P7">[6] Yao, X., Evolving artificial neural networks. Proc of the IEEE, 87(9), pp. 1423-1447 (1999)</text:p><text:p text:style-name="P7">[7] Holland, J., Adaptation In Natural and Artificial Systems. The University of Michigan Press (1975)</text:p><text:p text:style-name="P11">[8] Storn, R., Price, K. Differential evolution - a simple and efficient heuristic for global optimization over continuous spaces. Journal of Global Optimization 11, 341-359 (1997)</text:p><text:p text:style-name="P7"><text:soft-page-break/>[9] Goldberg, D.E.: Genetic and evolutionary algorithms come of age. Commun. ACM 37(3), 113-119 (1994)</text:p><text:p text:style-name="P7">[10] Pappa, G., Ochoa, G., Hyde, M., Freitas, A., Woodward, J., Swan, J.: Contrasting meta-learning and hyper-heuristic research: the role of evolutionary algorithms. Genet. Program. Evolvable Mach. 15(1), 3–35 (2014)</text:p><text:p text:style-name="P7">[11] Adamidis, P.: Review of parallel genetic algorithms bibliography. Technical Report version 1, Aristotle University of Thessaloniki, Thessaloniki, Greece (1994)</text:p><text:p text:style-name="P7">[12] Gordon, V.S., Whitley, D.: Serial and parallel genetic algorithms as function optimizers. In: Forrest S. (ed.) Proceedings of the Fifth International Conference on Genetic Algorithms, pp. 177–183. Morgan Kaufmann, San Mateo (1993)</text:p><text:p text:style-name="P7">[13] Lin, S.-C., Punch, W., Goodman, E.: Coarse-grain parallel genetic algorithms - categorization and new approach. In: Sixth IEEE Symposium on Parallel and Distributed Processing, IEEE Computer Society Press, Los Alamitos, CA (1994)</text:p><text:p text:style-name="P7">[14] Spiessens, P., Manderick, B.: A massively parallel genetic algorithm. In: Belew, R.K., Booker, L.B. (eds.) Proceedings of the 4th International Conference on Genetic Algorithms, pp. 279–286. Morgan Kaufmann, San Francisco (1991)</text:p><text:p text:style-name="P7">[15] Kruger, F., Wagner, D., Collet, P.: Massively parallel generational GA on GPGPU applied to power load profiles determination. In: Legrand, P., <text:soft-page-break/>Corsini, M.-M., Hao, J.-K., Monmarche, N., Lutton, E., Schoenauer, M. (eds.) EA 2013. LNCS, vol. 8752, pp. 227–239. Springer, Heidelberg (2014)</text:p><text:p text:style-name="P7">[16] Tanese, R.: Distributed genetic algorithms. In: Schaffer, J.D. (ed.) Proceedings of the 3rd International Conference on Genetic Algorithms, pp. 434–439. Morgan Kaufmann, San Francisco (1989)</text:p><text:p text:style-name="P7">[17] Uchida, T., Matsuzawa, T., Inoguchi, Y.: The influence of elitism strategy on migration intervals of a distributed genetic algorithm. In: Proceedings in Adaptation, Learning and Optimization, vol. 2, pp. 363–374 (2015)</text:p><text:p text:style-name="P7">[18] Gorges-Schleuter, M.: ASPARAGOS An asynchronous parallel genetic optimisation strategy. In: Schaffer, J.D. (ed.) Proceedings of the 3rd ICGA, pp. 422–427. Morgan Kaufmann, San Francisco (1989)</text:p><text:p text:style-name="P7">[19] Munetomo, M., Takai, Y., Sato, Y.: An efficient migration scheme for subpopulation-based asynchronously parallel GAs. Technical Report HIER-IS- 9301, Hokkaido University (1993)</text:p><text:p text:style-name="P7">[20] Voigt, H.M., Santibanez-Koref, I., Born, J.: Hierarchically structured distributed genetic algorithms. In: Manner, R., Manderick, B. (eds.) Proceedings of the International Conference Parallel Problem Solving from Nature, vol. 2, pp. 155–164. North-Holland, Amsterdam (1992)</text:p><text:p text:style-name="P7">[21] Belding, T.C.: The distributed genetic algorithm revisited. In: Eshelman, L.J. (ed.) Proceedings of the 6th International Conference on GAs, pp. 122–129. Morgan Kaufmann, San Francisco (1995)</text:p><text:p text:style-name="P7"><text:soft-page-break/>[22] Mejia-Olvera, M., Cantu-Paz, E.: DGENESIS-software for the execution of distributed genetic algorithms. In: Proceedings of the XX Conferencia Latinoamericana de Informatica, pp. 935–946. Monterrey, Mexico (1994)</text:p><text:p text:style-name="P7">[23] Baker, J.E.: Reducing bias and inefficiency in the selection algorithm. In: Grefenstette, J.J. (ed.) Proceedings of the Second International Conference on Genetic Algorithms, pp. 14–21. Lawrence Erlbaum Associates Publishers, Hillsdale (1987)</text:p><text:p text:style-name="P7">[24] Lim, T.Y.: Structured population genetic algorithms: a literature survey. Artif. Intell. Rev. 41(3), 385–399 (2014)</text:p><text:p text:style-name="P7"><text:span text:style-name="T13">[</text:span>25<text:span text:style-name="T13">]</text:span> Keremedchiev, D., Barova, M., Tomov, P., Mobile application as distributed computing system for artificial neural networks training used in perfect information games, Proceedings of 16th International scientific conference UNITECH16, Gabrovo, vol. 2, pp.389-393 (2016)</text:p><text:p text:style-name="P7"><text:span text:style-name="T13">[</text:span>26<text:span text:style-name="T13">]</text:span> Tomov, P., Monov, V., Artificial Neural Networks and Differential Evolution Used for Time Series Forecasting in Distributed Environment, Proceedings of International conference AUTOMATICS AND INFORMATICS, pp. 129-132, Sofia (2016)</text:p><text:p text:style-name="P7"><text:span text:style-name="T13">[</text:span>27<text:span text:style-name="T13">]</text:span> Zankinski, I., Stoilov, T., Effect of the Neuron Permutation Problem on Training Artificial Neural Networks with Genetic Algorithms in Distributed Computing, Proceedings of XXIV International Symposium Management of Energy, Industrial and Environmental Systems, Bankya, pp. 53-55 (2016)</text:p><text:p text:style-name="P7">[28] Karlik, B., Vehbi, A., Performance Analysis of Various Activation Functions in Generalized MLP Architectures of Neural Networks. <text:soft-page-break/>International Journal of Artificial Intelligence And Expert Systems (IJAE), vol. 1, no. 4, pp. 111-122 (2011)</text:p><text:p text:style-name="P7">[29] Atanasova T., Barova, M.: Exploratory analysis of time series for hypothesize feature values. In: Proceedings of International Scientific Conference UniTech17, Gabrovo, Bulgaria, vol. 2, pp. 399–403 (2017)</text:p><text:p text:style-name="P7">[30] Tashev, T., Hristov, H.: Modeling of synthesis of information processes with generalized nets. In: Drinov, M. (ed.) Cybernetics and Information Technologies, vol. 2, pp. 92–104. Academic Publishing House, Sofia (2003)</text:p><text:p text:style-name="P7">[31] Alexandrov, A.: Ad-hoc Kalman filter based fusion algorithm for real-time wireless sensor data integration. Flexible Query Answering Systems 2015. AISC, vol. 400, pp. 151–159. Springer, Cham (2016)</text:p><text:p text:style-name="P7">[32] Korf, R.: Finding optimal solutions to Rubik’s cube using pattern databases. In: AAAI-1998 Proceedings, pp. 700–705. AAAI Press, Menlo Park (1998)</text:p><text:p text:style-name="P7">[33] Randall, K.: Cilk - Efficient Multithreaded Computing. Doctor of Philosophy Thesis in Computer Science and Engineering, Massachusetts Institute of Technology, USA (1998)</text:p><text:p text:style-name="P7">[34] Poli, R., Kozak, J.: Genetic programming. In: Burke, E.K., Kendall, G. (eds.) Search Methodologies, pp. 143–185. Springer, Boston (2014)</text:p><text:p text:style-name="P7">[35] Brysbaert, M.: Algorithms for randomness in the behavioral sciences: A tutorial Behavior Research Methods, Instruments, and Computers vol.22 issue 1, pp.45-60, (1991)</text:p><text:p text:style-name="P7"><text:soft-page-break/>[36] Parke, J., Griffiths, M.: The psychology of the fruit machine: The role of structural characteristics (revisited), Paper presented at the annual conference of The British Psychological Society, Surrey, England (2001)</text:p><text:p text:style-name="P7">[37] Osesa, N.: Bitz and Pizzas Optimal stopping strategy for a slot machine bonus game, OR Insight, 22, pp.3144 (2009)</text:p><text:p text:style-name="P7">[38] Eiben, A. E: Genetic algorithms with multi-parent recombination. PPSN III: Proceedings of the International Conference on Evolutionary Computation. The Third Conference on Parallel Problem Solving from Nature: 7887, (1994)</text:p><text:p text:style-name="P7">[39] Ting, Chuan-Kang: On the Mean Convergence Time of Multi-parent Genetic Algorithms Without Selection. Advances in Artificial Life: 403412, (2005)</text:p><text:p text:style-name="P7">[40] Inge, S.: Electronic gaming device utilizing a random number generator for selecting the reel stop positions. US 4448419 A, Published 1984-05-15 (1984)</text:p><text:p text:style-name="P7">[41] Cooper, M.: How slot machines give gamblers the business. The Atlantic Monthly Group. Retrieved 2008-04-21 (2005)</text:p><text:p text:style-name="P7">[42] Price K.: An introduction to differential evolution. In David Corne, Marco Dorigo, and Fred Glover, editors, New Ideas in Optimization, Mc Graw-Hill, UK, 79-108 (1999)</text:p><text:p text:style-name="P7">[43] Goldberg D.: Genetic Algorithms in Search, Optimization and Machine Learning. Addison-Wesly Publishing Co., Reading, Massachusetts (1989)</text:p><text:p text:style-name="P7"><text:soft-page-break/>[44] Hans-Paul Schwefel, editor: Evolution and Optimization Seeking. John Wiley and Sons, New York (1995)</text:p><text:p text:style-name="P7">[45] Mezura-Montes, E., Velazquez-Reyes, J., Coello, C.: Modified Differential Evolution for Constrained Optimization. IEEE Congress on Evolutionary Computation, Vancouver, 25-32 (2006)</text:p><text:p text:style-name="P7">[46] Vaidyanathan, P.P., Phoong, S.M.: Reconstruction of Sequences from Nonuniform Samples, Proceedings of International Symposium on Circuits and Systems, vol. 1, 601-604 (1995)</text:p><text:p text:style-name="P7">[47] Lewis, P.O.: A Genetic Algorithm for Maximum-Likelihood Phylogeny Inference Using Nucleotide Sequence Data, Molecular Biology and Evolution, vol. 15, no. 3, 277-283 (1998)</text:p><text:p text:style-name="P7">[48] Parsons, R., Johnson, M.: A Case Study in Experimental Design Applied to Genetic Algorithms with Applications to DNA Sequence Assembly, American Journal of Mathematical and Management Sciences, vol. 17, no. 3-4, 369-396 (1997)</text:p><text:p text:style-name="P7">[49] Keremedchiev, D., Tomov, P., Barova, M.: Slot Machine Base Game Evolutionary RTP Optimization, Numerical Analysis and Its Applications, Lecture Notes in Computer Science, vol 10187. Springer, Cham (2017)</text:p><text:p text:style-name="P7">[50] Kamanas, P., Sifaleras, A., Samaras, N.: Slot machine RTP optimization using variable neighborhood search, Mathematical Problems in Engineering, 8784065 (2021)</text:p><text:p text:style-name="P7">[51] Deng, Y.: Uncertainty measure in evidence theory, Science China Information Sciences 63, 210201 (2020)</text:p><text:p text:style-name="P7"><text:soft-page-break/>[52] Patra, L. K., Kayal, S., Kumar, S.: Measuring uncertainty under prior information, IEEE Transactions on Information Theory 66(4), 2570–2580 (2020)</text:p><text:p text:style-name="P7">[53] Schoenberger, J.: Genetic Algorithms for Musical Composition with Coherency Through Genotype, Master&apos;s thesis, College of William and Mary (2002)</text:p><text:p text:style-name="P7">[54] Jacob, B.: Composing With Genetic Algorithms, Proceedings of the 1994 International Computer Music Conference, 452-455 (1995)</text:p><text:p text:style-name="P7">[55] Draves, S.: The Electric Sheep Screen-Saver: A Case Study in Aesthetic Evolution, Applications of Evolutionary Computing, Lecture Notes in Computer Science, vol 3449, Springer, Berlin, Heidelberg (2005)</text:p><text:p text:style-name="P7">[56] Unehara, M., Onisawa, T.: Music composition by interaction between human and computer, New Generation Computing, vol. 23, 181–191 (2005)</text:p><text:p text:style-name="P7">[57] Mitchell, M.: An introduction to genetic algorithms, Cambridge, MA: MIT Press (1999)</text:p><text:p text:style-name="P7">[58] Takagi, H.: Interactive Evolutionary Computation: Fusion of the Capabilities of EC Optimization and Human Evaluation, Proceedings of the IEEE, vol.89, no.9, 1275-1296 (2001)</text:p><text:p text:style-name="P7">[59] Wiggins, G. A., Papadopoulos, G., Phon-Amnuaisuk, S., Tuson, A.: Evolutionary Methods for Musical Composition, International Journal of Computing Anticipatory Systems (1999)</text:p><text:p text:style-name="P7"><text:soft-page-break/>[60] Thain, D., Tannenbaum, T., Livny, M.: Distributed Computing in Practice: The Condor Experience, Grid Performance, vol. 17, no. 2-4, 323-356 (2005)</text:p><text:p text:style-name="P7">[61] Shenoy, P., Tan, D. S.: Human-aided Computing: Utilizing Implicit Human Processing to Classify Images, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 845-854 (2008)</text:p><text:p text:style-name="P7">[62] Anderson, D.: Volunteer Computing the Ultimate Cloud, The ACM Magazine for Students, vol. 16, 3 (2010)</text:p><text:p text:style-name="P7">[63] Hoare, C. A. R.: Towards a Theory of Parallel Programming, The Origin of Concurrent Programming, Springer, New York, NY, 231-244 (1972)</text:p><text:p text:style-name="P7">[64] Mattson, T. G.: Programming Environments for Parallel and Distributed Computing: A Comparison of P4, Pvm, Linda, and Tcgmsg, The International Journal of Supercomputer Applications and High Performance Computing, vol. 9, no. 2, 138-161 (1995)</text:p><text:p text:style-name="P7">[65] Graf, J.: Interactive Evolutionary Algorithms in Design, Proceeding of the International Conference on Artificial Neural Nets and Genetic Algorithms, Ales, France, 227-230 (1995)</text:p><text:p text:style-name="P7">[66] Barnsley, M. F., Hutchinson, J.: New Methods in Fractal Imaging, Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation, 296-301 (2006)</text:p><text:p text:style-name="P7">[67] Johnson, C. G., Cardalda, J. J. R.: Genetic Algorithms in Visual Art and Music, Leonardo, vol. 35, no. 2, 175-184 (2002)</text:p><text:p text:style-name="P7"><text:soft-page-break/>[68] Cho, S.: Towards Creative Evolutionary Systems with Interactive Genetic Algorithm, Applied Intelligence, vol. 16, 129-138 (2002)</text:p><text:p text:style-name="P7">[69] Bohn, A., Guting, T., Mansmann, T., Selle, S.: MoneyBee Aktienkursprognose mit kunstlicher intelligenz bei hoher rechenleistung, Wirtschaftsinf, vol. 45, 325-333 (2003)</text:p><text:p text:style-name="P7">[70] Seiffert, U.: Artificial Neural Networks on Massively Parallel Computer Hardware, Neurocomputing, vol. 57, 135–150 (2004)</text:p><text:p text:style-name="P7">[71] Krieger, E., Vriend, G.: Models@Home Distributed Computing in Bioinformatics Using a Screensaver Based Approach, Bioinformatics, vol. 18, no. 2, 315-318 (2002)</text:p><text:p text:style-name="P7">[72] Lodi, A., Martello, S., Monaci, M.: Two-dimensional packing problems: A survey, European Journal of Operational Research (Elsevier), vol. 141, no. 2, 241–252 (2002)</text:p><text:p text:style-name="P7">[73] Fodor, F.: The Densest Packing of 13 Congruent Circles in a Circle, Beitrage zur Algebra und Geometrie, Contributions to Algebra and Geometry, vol. 44, no. 2, 431–440 (2003)</text:p><text:p text:style-name="P7">[74] Huang, W., Ye, T.: Greedy vacancy search algorithm for packing equal circles in a square, Operations Research Letters, vol. 38, 378–382 (2010)</text:p><text:p text:style-name="P7">[75] Xu, Y.: On the minimum distance determined by n (&lt;= 7) points in an isoscele right triangle, Acta Mathematicae Applicatae Sinica, vol. 12, no. 2, 169–175 (1996)</text:p><text:p text:style-name="P7"><text:soft-page-break/>[76] Nurmela, K., Conjecturally optimal coverings of an equilateral triangle with up to 36 equal circles, Experimental Mathematics, vol. 9, no. 2, 241–250 (2000)</text:p><text:p text:style-name="P7">[77] Stromquist, W., Packing 10 or 11 unit squares in a square, Electronic Journal of Combinatorics, vol. 10, no. 8, 1–11 (2003)</text:p><text:p text:style-name="P7">[78] Ferreira, A., Fonseca, M.J., Jorge, J.A., Ramalho, M.: Mixing Images and Sketches for Retrieving Vector Drawings, Proceedings of The 7th Eurographics Workshop on Multimedia, China, (2004)</text:p><text:p text:style-name="P7">[79] Wenyin, L., Dori, D.: From Raster to Vectors: Extracting Visual Information from Line Drawings, Pattern Analysis and Applications, (1999)</text:p><text:p text:style-name="P7">[80] Tombre, K., Ah-Soon, C., Dosch, P., Masini, G., Tabbone, S.: Stable and robust vectorization: How to make the right choices, Proceedings of the 3rd IAPR Intlernational Workshop on Graphics Recognition, Jaipur, India, 3–16 (1999)</text:p><text:p text:style-name="P7">[81] Hussain, A., Muhammad, Y.S.: Trade-off between exploration and exploitation with genetic algorithm using a novel selection operator, Complex &amp; Intelligent Systems, vol. 6, 1–14 (2020)</text:p><text:p text:style-name="P7">[82] Kazharov, A., Kureichik, V.: Ant colony optimization algorithms for solving transportation problems, Journal of Computer and Systems Sciences International, vol. 49, no. 1, 30-43 (2010)</text:p><text:p text:style-name="P7">[83] Mohd Murtadha, M.: Articulated Robots Motion Planning Using Foraging Ant Strategy, Journal of Information Technology - Special Issues in Artificial Intelligence, vol. 20, no. 4, 163-181 (2008)</text:p><text:p text:style-name="P7"><text:soft-page-break/>[84] Dorigo, M., Maniezzo, V., Colorni, A.: Ant System: Optimization by a Colony of Cooperating Agents, IEEE Transactions on Systems, Man, and Cybernetics-Part B, vol. 26, no. 1, 29-41 (1996)</text:p><text:p text:style-name="P7">[85] Guerin, E., Tosan, E.: Fractal inverse problem: approximation formulation anddifferential methods, Fractals in Engineering, 271-285 (2005)</text:p><text:p text:style-name="P7">[86] Nettleton, D.J., Garigliano, R.: Evolutionary algorithms and a fractal inverse problem, Biosystems, vol. 33 no. 3, 221–231 (1994)</text:p><text:p text:style-name="P7">[87] Ochoa, G.: On genetic algorithms and lindenmayer systems, PPSN LNCS, vol. 1498, 335-344 (1998) </text:p><text:p text:style-name="P7">[88] Shonkwiler, R., Mendivil, F., Deliu, A.: Genetic algorithms for the 1-D fractalinverse problem, Proceedings of the Fourth International Conference on GeneticAlgorithms, Morgan Kaufmann, 495-501 (1991)</text:p><text:p text:style-name="P7">[89] Zelinka, I.: Inverse fractal problem, Differential Evolution, NCS, Springer, Heidelberg, 479-498 (2005)</text:p><text:p text:style-name="P7">[90] Shonkwiler, R.: Parallel genetic algorithms, Proceedings of the Fifth International Conference on Genetic Algorithms, Morgan Kaufmann, 199-205 (1993)</text:p><text:p text:style-name="P7">[91] Vences, L., Rudomin, I., Carretera, K., Guadalupe, L.: Genetic algorithms for fractal image and image sequence compression, Proceedings of Comptacion Visual, 35-44 (1997)</text:p><text:p text:style-name="P7">[92] Al-Bundi, S.S., Al-Saidi, N.M., Al-Jawari, N.J.: Crowding optimization method to improve fractal image compressions based iterated function <text:soft-page-break/>systems, International Journal of Advanced Computer Science and Applications, vol. 7, no. 7, 392-401 (2016)</text:p><text:p text:style-name="P7">[93] Nava, N., Di Matteo, T., Aste, T.: Financial time series forecasting using empirical mode decomposition and support vector regression, Risks, vol. 6, no.1, Article no. 7 (2018)</text:p><text:p text:style-name="P7">[94] Catania, L., Grassi, S., Ravazzolo, F.: Predicting the volatility of cryptocurrency time-series, Mathematical and Statistical Methods for Actuarial Sciences and Finance, Springer, Cham, 203–207 (2018)</text:p><text:p text:style-name="P7">[95] Chen, J., Boccelli, D.L.: Real-time forecasting and visualization toolkit for multi-seasonal time series, Environmental Modelling &amp; Software, vol. 105, 244–256 (2018)</text:p><text:p text:style-name="P7">[96] Mueen, A., Keogh, E., Zhu, Q., Cash, S.,Westover, B.: Exact discovery of time series motifs, Proceedings of the SIAM International Conference on Data Mining, 473–484 (2009)</text:p><text:p text:style-name="P7">[97] Aljarah, I., Faris, H., Mirjalili, S.: Optimizing connection weights in neural networks using the whale optimization algorithm, Soft Computing, vol. 22, no. 1, 1–15 (2016)</text:p><text:p text:style-name="P7">[98] Aljarah, I., Faris, H., Mirjalili, S.: Optimizing connection weights in neural networks using the whale optimization algorithm, Soft Computing, vol. 22, no. 1, 1–15 (2016)</text:p><text:p text:style-name="P7">[99] Zhang, R., Tao, J.: A nonlinear fuzzy neural network modeling approach using an improved genetic algorithm, IEEE Transactions on Industrial Electronics, vol. 65, no. 7, 5882–5892 (2018)</text:p><text:p text:style-name="P7"><text:soft-page-break/>[100] Korenberg, M., Paarmann, L.: Orthogonal approaches to time-series analysis and system identification, IEEE Signal Processing Magazine, vol. 8, no. 3, 29–43 (1991)</text:p><text:p text:style-name="P7">[101] Stankovic, I., Dakovic, M., Ioana, C.: Decomposition and analysis of signals sparse in the dual polynomial Fourier transform, Microprocessors and Microsystems, vol. 63, 209–215 (2018)</text:p><text:p text:style-name="P89">[102] Maaranen, H., Miettinen, K. Penttinen, A.: On initial populations of a genetic algorithm for continuous optimization problems, Journal of Global Optimization, vol. 37, 405 (2007)</text:p><text:p text:style-name="P7">[103] Katoch, S., Chauhan, S.S. &amp; Kumar, V.: A review on genetic algorithm: past, present, and future, Multimedia Tools and Applications, vol. 80, 8091–8126 (2021)</text:p><text:p text:style-name="P7">[104] Drezner, Z., Drezner, T.D.: Biologically Inspired Parent Selection in Genetic Algorithms, Annals of Operations Research, vol. 287, 161–183 (2020)</text:p><text:p text:style-name="P7">[105] Singh, P.K.: A Modified Real-Coded Extended Line Crossover for Genetic Algorithm. Social Transformation – Digital Way, Communications in Computer and Information Science, vol 836. Springer, Singapore (2018)</text:p><text:p text:style-name="P7">[106] Rao, R.V., Saroj, A.: An elitism-based self-adaptive multi-population Jaya algorithm and its applications, Soft Computing, vol. 23, 4383–4406 (2019)</text:p><text:p text:style-name="P7"><text:soft-page-break/>[107] Yeh, K.C., Kwan, K.C.: A comparison of numerical integrating algorithms by trapezoidal, Lagrange, and spline approximation, Journal of Pharmacokinetics and Biopharmaceutics, vol. 6, 79-98 (1978)</text:p><text:p text:style-name="P7">[108] Abraham, A., Jain, L.: Evolutionary Multiobjective Optimization, Evolutionary Multiobjective Optimization, Advanced Information and Knowledge Processing, Springer, 1–6 (2005)</text:p><text:p text:style-name="P7">[109] Arora, J. S.: Chapter 18 – Multi-objective Optimum Design Concepts and Methods, Introduction to Optimum Design (Fourth Edition), Academic Press, 771–794 (2017)</text:p><text:p text:style-name="P7">[110] Zitzler, E., Laumanns, M., Bleuler, S.: A tutorial on evolutionary multiobjective optimization, Metaheuristics for Multiobjective Optimisation, Lecture Notes in Economics and Mathematical Systems, vol. 535, pp. 3–37 (2004)</text:p><text:p text:style-name="P7">[111] Zitzler, E., Thiele, L.: Multiobjective optimization using evolutionary algorithms – A comparative case study, Parallel Problem Solving from Nature, Lecture Notes in Computer Science, vol. 1498, 292–301 (1998)</text:p><text:p text:style-name="P7">[112] Deb, K.: Multi-objective optimization, Search Methodologies, Springer, Boston, MA, 403–449 (2014)</text:p><text:p text:style-name="P7">[113] Emmerich, M., Deutz, A: A tutorial on multiobjective optimization: fundamentals and evolutionary methods, Natural Computing, vol. 17, 585–609 (2018)</text:p><text:p text:style-name="P7">[114] Deb, K., Thiele, L., Laumanns, M., Zitzler, E.: Scalable test problems for evolutionary multiobjective optimization, Evolutionary Multiobjective <text:soft-page-break/>Optimization, Advanced Information and Knowledge Processing, Springer, London, 105–145 (2005)</text:p><text:p text:style-name="P7">[115] Miettinen, K., Makela, M.: On scalarizing functions in multiobjective optimization, OR Spectrum, vol. 24, no. 2, 193–213 (2002)</text:p><text:p text:style-name="P7">[116] Marler, R.T., Arora, J.S.: The weighted sum method for multi-objective optimization: new insights, Structural and Multidisciplinary Optimization, vol. 41, 853–862 (2010)</text:p><text:p text:style-name="P7">[117] Marler, R.T., Arora, J.S.: Survey of multi-objective optimization methods for engineering. Structural and Multidisciplinary Optimization, vol. 26, 369–395 (2004)</text:p><text:p text:style-name="P7">[118] Tomov, P.: Multilayer perceptron fast prototyping with differential evolution and particle swarm optimization in LibreOffice Calc, Problems of Engineering Cybernetics and Robotics, vol. 75, 5-14 (2021)</text:p><text:p text:style-name="P7">[119] Fang, S.-C., Puthenpura, S.: Linear optimization and extensions: theory and algorithms, <text:s/>Prentice-Hall, Inc., USA (1993)</text:p><text:p text:style-name="P7">[120] Scales, L. E.: Introduction to non-linear optimization, Springer-Verlag, Berlin, Heidelberg (1985)</text:p><text:p text:style-name="P7">[121] Peng, W.: Mixed Particle Swarm Optimization Algorithm with Multistage Disturbances, Frontiers in Genetic and Evolutionary Computation, vol. 1, 1-4 (2019)</text:p><text:p text:style-name="P7">[122] Dasovic, B., Klansek, U.: Comparison of spreadsheet-based optimization tools applied to construction site layout problem, IOP <text:soft-page-break/>Conference Series: Materials Science and Engineering, vol. 1209, no. 1 (2021)</text:p><text:p text:style-name="P7">[123] Lambora, A., Gupta, K., Chopra, K.: Genetic Algorithm- A Literature Review, International Conference on Machine Learning, Big Data, Cloud and Parallel Computing, Faridabad, India, 380-384 (2019)</text:p><text:p text:style-name="P7">[124] Jamil. M., Yang, X,; Zepernick, H.J.: 8 - Test Functions for Global Optimization: A Comprehensive Survey, Swarm Intelligence and Bio-Inspired Computation, Elsevier, 193-222 (2013)</text:p><text:p text:style-name="P7">[125] Zang, W., Zhang, W., Wang, Z., Jiang, D., Liu, X., Sun, M.: A Novel Double-Strand DNA Genetic Algorithm for Multi-Objective Optimization, IEEE Access, vol. 7, 18821-18839 (2019)</text:p><text:p text:style-name="P7">[126] Yang, S.: PDGA: the primal-dual genetic algorithm, IOS Press, 1-10 (2003)</text:p><text:p text:style-name="P7"/><text:table-of-content text:style-name="Sect1" text:name="Table of Contents1"><text:table-of-content-source text:outline-level="10"><text:index-title-template text:style-name="Contents_20_Heading">Table of Contents</text:index-title-template><text:table-of-content-entry-template text:outline-level="1" text:style-name="Contents_20_1"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="2" text:style-name="Contents_20_2"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="3" text:style-name="Contents_20_3"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="4" text:style-name="Contents_20_4"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="5" text:style-name="Contents_20_5"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="6" text:style-name="Contents_20_6"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="7" text:style-name="Contents_20_7"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="8" text:style-name="Contents_20_8"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="9" text:style-name="Contents_20_9"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="10" text:style-name="Contents_20_10"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template></text:table-of-content-source><text:index-body><text:index-title text:style-name="Sect1" text:name="Table of Contents1_Head"><text:p text:style-name="P104">Table of Contents</text:p></text:index-title><text:p text:style-name="P101"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc552_3689700921" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">1 Time Series Prediction by Artificial Neural Networks and Differential Evolution in Distributed Environment<text:tab/>1</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc554_3689700921" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">1.1 Introduction<text:tab/>1</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc556_3689700921" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">1.2 Problem Definition<text:tab/>4</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc472_3627904809" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">1.3 Machine Learning Tools<text:tab/>4</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc474_3627904809" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">1.4 Forecasting Organization<text:tab/>7</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc7828_527140494" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">Summary<text:tab/>15</text:a></text:p><text:p text:style-name="P101"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc351_966847004" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">2 Distributed Evolutionary Computing Migration Strategy by Accidental Node Involvement<text:tab/>18</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc979_966847004" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">2.1 Introduction<text:tab/>19</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc772_951591551" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">2.2 Accidental Node Involvement<text:tab/>21</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc632_1783862520" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">2.3 Distribution Parameterization<text:tab/>24</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc7830_527140494" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">Summary<text:tab/>28</text:a></text:p><text:p text:style-name="P101"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc634_1783862520" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">3 Modifications of Artificial Neural Networks<text:tab/>31</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc718_1268615346" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">3.1 Alternative Activation Function Derivative<text:tab/>31</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc1666_1268615346" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">3.2 Long Short Term Memory in Multilayer Perceptron Pair<text:tab/>39</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc1317_2021269790" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">3.3 Self-Growing Multilayer Perceptron for Time Series Forecasting<text:tab/>42</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc2431_612792312" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">3.4 Permutations in Graph Structure<text:tab/>49</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc7832_527140494" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">Summary<text:tab/>61</text:a></text:p><text:p text:style-name="P101"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc2345_1821873452" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">4 Evolutionary Algorithms in Games Combinatorial Problems<text:tab/>64</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc3265_968957226" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link"><text:soft-page-break/>4.1 Solving Combinatorial Puzzles with Parallel Evolutionary Algorithms<text:tab/>64</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc3267_968957226" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">4.2 Virtual Reels in Slot Machines<text:tab/>72</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc7834_527140494" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">Summary<text:tab/>98</text:a></text:p><text:p text:style-name="P101"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc2347_1821873452" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">5 Problems with Human Evaluation of Fitness Function<text:tab/>105</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc2207_1422372469" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">5.1 Human Evaluation in Art<text:tab/>105</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc2209_1422372469" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">5.2 Intuitive Human Evaluation<text:tab/>115</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc7836_527140494" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">Summary<text:tab/>120</text:a></text:p><text:p text:style-name="P101"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc2349_1821873452" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">6 Evolutionary Algorithms in Vectorization Problems<text:tab/>123</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc2287_789959763" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">6.1 Sound Vectorization<text:tab/>123</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc1936_376025042" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">6.2 Image Vectorization<text:tab/>126</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc6782_3046959508" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">6.3 String Rewriting for 3D Fractal Generation<text:tab/>145</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc7838_527140494" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">Summary<text:tab/>153</text:a></text:p><text:p text:style-name="P101"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc2351_1821873452" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">7 Evolutionary Algorithms in Combination with Curve Fitting<text:tab/>155</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc2688_3397589795" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">7.1 Fitting the Curve Directly<text:tab/>156</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc2690_3397589795" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">7.2 Fitness Function as Curve<text:tab/>164</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc7840_527140494" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">Summary<text:tab/>170</text:a></text:p><text:p text:style-name="P101"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc2353_1821873452" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">8 Evolutionary Algorithms in Multi-Оbjective Problems and Multi-Modal Functions<text:tab/>172</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc6444_464923766" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">8.1 LibreOffice Calc NLP Solver<text:tab/>174</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc2625_1241039964" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">8.2 Solving Multi-Objective Problems by Single Objective Solver<text:tab/>175</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc4553_2316191826" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">8.3 Random Numbers in Optimization Metaheuristics<text:tab/>176</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc14058_2316191826" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">8.4 Multidimensionality and Performance<text:tab/>180</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc7842_527140494" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link"><text:soft-page-break/>Summary<text:tab/>185</text:a></text:p><text:p text:style-name="P101"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc2355_1821873452" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">9 Modification of Components in Evolutionary Algorithms<text:tab/>188</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc3714_2324440366" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">9.1 DNA Inspired Encoding<text:tab/>189</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc3716_2324440366" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">9.2 Hierarchical Brute Force Selection<text:tab/>193</text:a></text:p><text:p text:style-name="P102"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc7844_527140494" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">Summary<text:tab/>196</text:a></text:p><text:p text:style-name="P101"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc7846_527140494" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">Final Remarks<text:tab/>198</text:a></text:p></text:index-body></text:table-of-content><text:p text:style-name="P3"/></office:text></office:body></office:document-content>