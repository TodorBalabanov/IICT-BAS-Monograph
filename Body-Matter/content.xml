<?xml version="1.0" encoding="UTF-8"?>
<office:document-content xmlns:office="urn:oasis:names:tc:opendocument:xmlns:office:1.0" xmlns:ooo="http://openoffice.org/2004/office" xmlns:fo="urn:oasis:names:tc:opendocument:xmlns:xsl-fo-compatible:1.0" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:meta="urn:oasis:names:tc:opendocument:xmlns:meta:1.0" xmlns:style="urn:oasis:names:tc:opendocument:xmlns:style:1.0" xmlns:text="urn:oasis:names:tc:opendocument:xmlns:text:1.0" xmlns:rpt="http://openoffice.org/2005/report" xmlns:draw="urn:oasis:names:tc:opendocument:xmlns:drawing:1.0" xmlns:dr3d="urn:oasis:names:tc:opendocument:xmlns:dr3d:1.0" xmlns:svg="urn:oasis:names:tc:opendocument:xmlns:svg-compatible:1.0" xmlns:chart="urn:oasis:names:tc:opendocument:xmlns:chart:1.0" xmlns:table="urn:oasis:names:tc:opendocument:xmlns:table:1.0" xmlns:number="urn:oasis:names:tc:opendocument:xmlns:datastyle:1.0" xmlns:ooow="http://openoffice.org/2004/writer" xmlns:oooc="http://openoffice.org/2004/calc" xmlns:of="urn:oasis:names:tc:opendocument:xmlns:of:1.2" xmlns:xforms="http://www.w3.org/2002/xforms" xmlns:tableooo="http://openoffice.org/2009/table" xmlns:calcext="urn:org:documentfoundation:names:experimental:calc:xmlns:calcext:1.0" xmlns:drawooo="http://openoffice.org/2010/draw" xmlns:xhtml="http://www.w3.org/1999/xhtml" xmlns:loext="urn:org:documentfoundation:names:experimental:office:xmlns:loext:1.0" xmlns:field="urn:openoffice:names:experimental:ooo-ms-interop:xmlns:field:1.0" xmlns:math="http://www.w3.org/1998/Math/MathML" xmlns:form="urn:oasis:names:tc:opendocument:xmlns:form:1.0" xmlns:script="urn:oasis:names:tc:opendocument:xmlns:script:1.0" xmlns:formx="urn:openoffice:names:experimental:ooxml-odf-interop:xmlns:form:1.0" xmlns:dom="http://www.w3.org/2001/xml-events" xmlns:xsd="http://www.w3.org/2001/XMLSchema" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:grddl="http://www.w3.org/2003/g/data-view#" xmlns:css3t="http://www.w3.org/TR/css3-text/" xmlns:officeooo="http://openoffice.org/2009/office" office:version="1.3"><office:scripts/><office:font-face-decls><style:font-face style:name="Arial Unicode MS" svg:font-family="&apos;Arial Unicode MS&apos;" style:font-family-generic="swiss"/><style:font-face style:name="Arial Unicode MS1" svg:font-family="&apos;Arial Unicode MS&apos;" style:font-family-generic="system" style:font-pitch="variable"/><style:font-face style:name="Calibri" svg:font-family="Calibri" style:font-adornments="Regular" style:font-family-generic="swiss" style:font-pitch="variable"/><style:font-face style:name="Liberation Sans" svg:font-family="&apos;Liberation Sans&apos;" style:font-family-generic="swiss" style:font-pitch="variable"/><style:font-face style:name="Liberation Serif" svg:font-family="&apos;Liberation Serif&apos;" style:font-family-generic="roman" style:font-pitch="variable"/><style:font-face style:name="PingFang SC" svg:font-family="&apos;PingFang SC&apos;" style:font-family-generic="system" style:font-pitch="variable"/><style:font-face style:name="Songti SC" svg:font-family="&apos;Songti SC&apos;" style:font-family-generic="system" style:font-pitch="variable"/></office:font-face-decls><office:automatic-styles><style:style style:name="P1" style:family="paragraph" style:parent-style-name="Header"><loext:graphic-properties draw:fill-gradient-name="gradient" draw:fill-hatch-name="hatch"/><style:paragraph-properties fo:line-height="100%" fo:text-align="end" style:justify-single-word="false"/></style:style><style:style style:name="P2" style:family="paragraph" style:parent-style-name="Footer"><style:paragraph-properties fo:text-align="end" style:justify-single-word="false"/><style:text-properties fo:language="en" fo:country="US"/></style:style><style:style style:name="P3" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US"/></style:style><style:style style:name="P4" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00358a2a"/></style:style><style:style style:name="P5" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="003682c7"/></style:style><style:style style:name="P6" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="00415f3c" officeooo:paragraph-rsid="00415f3c"/></style:style><style:style style:name="P7" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="00460fe0" officeooo:paragraph-rsid="00415f3c"/></style:style><style:style style:name="P8" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="004c56b5"/></style:style><style:style style:name="P9" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00560699"/></style:style><style:style style:name="P10" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="0056f98d"/></style:style><style:style style:name="P11" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="005815c8" officeooo:paragraph-rsid="005815c8"/></style:style><style:style style:name="P12" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00682738"/></style:style><style:style style:name="P13" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="006c7d80"/></style:style><style:style style:name="P14" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00748eff"/></style:style><style:style style:name="P15" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="007c82d7"/></style:style><style:style style:name="P16" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="0080c64c"/></style:style><style:style style:name="P17" style:family="paragraph" style:parent-style-name="Bibliography_20_Heading"><style:paragraph-properties fo:break-before="page"/><style:text-properties fo:language="en" fo:country="US"/></style:style><style:style style:name="P18" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="0099263e"/></style:style><style:style style:name="P19" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00c4e0a9"/></style:style><style:style style:name="P20" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00cae438"/></style:style><style:style style:name="P21" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00d49860"/></style:style><style:style style:name="P22" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00d78e37"/></style:style><style:style style:name="P23" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00ea19be"/></style:style><style:style style:name="P24" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00edde6d"/></style:style><style:style style:name="P25" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00f9b976"/></style:style><style:style style:name="P26" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="00fe3b76"/></style:style><style:style style:name="P27" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="01167a7d"/></style:style><style:style style:name="P28" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="0116d99f"/></style:style><style:style style:name="P29" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="01175038"/></style:style><style:style style:name="P30" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="012aa61e"/></style:style><style:style style:name="P31" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="012ce3ba"/></style:style><style:style style:name="P32" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="0136f81f"/></style:style><style:style style:name="P33" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b"/></style:style><style:style style:name="P34" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="0138eb2b"/></style:style><style:style style:name="P35" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="015abe49"/></style:style><style:style style:name="P36" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="015b7635"/></style:style><style:style style:name="P37" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="0160d280"/></style:style><style:style style:name="P38" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="01653046"/></style:style><style:style style:name="P39" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="016814b9"/></style:style><style:style style:name="P40" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="016c08e8"/></style:style><style:style style:name="P41" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="016eae89"/></style:style><style:style style:name="P42" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="01770657"/></style:style><style:style style:name="P43" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="017859be"/></style:style><style:style style:name="P44" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="017ecdd3"/></style:style><style:style style:name="P45" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="01815991"/></style:style><style:style style:name="P46" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="0182e956"/></style:style><style:style style:name="P47" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="01877feb"/></style:style><style:style style:name="P48" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="018d5398"/></style:style><style:style style:name="P49" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="01910a95"/></style:style><style:style style:name="P50" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="0198aecd"/></style:style><style:style style:name="P51" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="019bc2f4"/></style:style><style:style style:name="P52" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013aa37e" officeooo:paragraph-rsid="013aa37e"/></style:style><style:style style:name="P53" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013acf53" officeooo:paragraph-rsid="013acf53"/></style:style><style:style style:name="P54" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013be91f" officeooo:paragraph-rsid="013be91f"/></style:style><style:style style:name="P55" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013dae88" officeooo:paragraph-rsid="013dae88"/></style:style><style:style style:name="P56" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="01493454"/></style:style><style:style style:name="P57" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="014e86f1"/></style:style><style:style style:name="P58" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:paragraph-rsid="01509d9a"/></style:style><style:style style:name="P59" style:family="paragraph" style:parent-style-name="Contents_20_1"><style:paragraph-properties><style:tab-stops><style:tab-stop style:position="5.2126in" style:type="right" style:leader-style="dotted" style:leader-text="."/></style:tab-stops></style:paragraph-properties></style:style><style:style style:name="P60" style:family="paragraph" style:parent-style-name="Contents_20_2"><style:paragraph-properties><style:tab-stops><style:tab-stop style:position="5.0161in" style:type="right" style:leader-style="dotted" style:leader-text="."/></style:tab-stops></style:paragraph-properties></style:style><style:style style:name="P61" style:family="paragraph" style:parent-style-name="Contents_20_Heading"><style:paragraph-properties fo:break-before="page"/></style:style><style:style style:name="P62" style:family="paragraph" style:parent-style-name="Contents_20_1"><style:paragraph-properties><style:tab-stops><style:tab-stop style:position="5.2126in" style:type="right" style:leader-style="dotted" style:leader-text="."/></style:tab-stops></style:paragraph-properties></style:style><style:style style:name="P63" style:family="paragraph" style:parent-style-name="Contents_20_2"><style:paragraph-properties><style:tab-stops><style:tab-stop style:position="5.0161in" style:type="right" style:leader-style="dotted" style:leader-text="."/></style:tab-stops></style:paragraph-properties></style:style><style:style style:name="P64" style:family="paragraph" style:parent-style-name="Contents_20_Heading"><style:paragraph-properties fo:break-before="page"/></style:style><style:style style:name="P65" style:family="paragraph" style:parent-style-name="Contents_20_Heading"><style:paragraph-properties fo:break-before="page"/><style:text-properties fo:language="en" fo:country="US"/></style:style><style:style style:name="P66" style:family="paragraph" style:parent-style-name="Header"><loext:graphic-properties draw:fill-gradient-name="gradient" draw:fill-hatch-name="hatch"/><style:paragraph-properties fo:line-height="100%"/><style:text-properties officeooo:rsid="0022a018" officeooo:paragraph-rsid="00242fc2"/></style:style><style:style style:name="P67" style:family="paragraph" style:parent-style-name="Heading_20_1"><style:text-properties fo:language="en" fo:country="US"/></style:style><style:style style:name="P68" style:family="paragraph" style:parent-style-name="Heading_20_1"><style:paragraph-properties fo:break-before="page"/><style:text-properties fo:language="en" fo:country="US"/></style:style><style:style style:name="P69" style:family="paragraph" style:parent-style-name="Heading_20_2"><style:text-properties fo:language="en" fo:country="US"/></style:style><style:style style:name="P70" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="0138eb2b" officeooo:paragraph-rsid="0138eb2b"/></style:style><style:style style:name="P71" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013aa37e" officeooo:paragraph-rsid="013aa37e"/></style:style><style:style style:name="P72" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="013aa37e" officeooo:paragraph-rsid="01a5eedd"/></style:style><style:style style:name="P73" style:family="paragraph" style:parent-style-name="Standard"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="00460fe0" officeooo:paragraph-rsid="00415f3c"/></style:style><style:style style:name="T1" style:family="text"><style:text-properties officeooo:rsid="002867a0"/></style:style><style:style style:name="T2" style:family="text"><style:text-properties officeooo:rsid="003941f7"/></style:style><style:style style:name="T3" style:family="text"><style:text-properties fo:language="en" fo:country="US"/></style:style><style:style style:name="T4" style:family="text"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="006e0a29"/></style:style><style:style style:name="T5" style:family="text"><style:text-properties fo:language="en" fo:country="US" officeooo:rsid="00e6305c"/></style:style><style:style style:name="T6" style:family="text"><style:text-properties officeooo:rsid="003e1eeb"/></style:style><style:style style:name="T7" style:family="text"><style:text-properties officeooo:rsid="0042e914"/></style:style><style:style style:name="T8" style:family="text"><style:text-properties officeooo:rsid="004c56b5"/></style:style><style:style style:name="T9" style:family="text"><style:text-properties officeooo:rsid="004e218b"/></style:style><style:style style:name="T10" style:family="text"><style:text-properties officeooo:rsid="005fa15f"/></style:style><style:style style:name="T11" style:family="text"><style:text-properties officeooo:rsid="00ac6e4e"/></style:style><style:style style:name="T12" style:family="text"><style:text-properties officeooo:rsid="00be8bdc"/></style:style><style:style style:name="T13" style:family="text"><style:text-properties officeooo:rsid="00c4873f"/></style:style><style:style style:name="T14" style:family="text"><style:text-properties officeooo:rsid="00dc3064"/></style:style><style:style style:name="T15" style:family="text"><style:text-properties officeooo:rsid="00de182b"/></style:style><style:style style:name="T16" style:family="text"><style:text-properties officeooo:rsid="00dea8bf"/></style:style><style:style style:name="T17" style:family="text"><style:text-properties officeooo:rsid="00f2a227"/></style:style><style:style style:name="T18" style:family="text"><style:text-properties officeooo:rsid="01a2d07a"/></style:style><style:style style:name="fr1" style:family="graphic" style:parent-style-name="Frame"><style:graphic-properties fo:margin-left="0in" fo:margin-right="0in" fo:margin-top="0in" fo:margin-bottom="0in" style:wrap="dynamic" style:number-wrapped-paragraphs="no-limit" style:vertical-pos="top" style:vertical-rel="baseline" style:horizontal-pos="center" style:horizontal-rel="paragraph" fo:padding="0in" fo:border="none" loext:rel-width-rel="paragraph"/></style:style><style:style style:name="fr2" style:family="graphic" style:parent-style-name="Frame"><style:graphic-properties fo:margin-left="0in" fo:margin-right="0in" fo:margin-top="0in" fo:margin-bottom="0in" style:wrap="dynamic" style:number-wrapped-paragraphs="no-limit" style:vertical-pos="top" style:vertical-rel="baseline" style:horizontal-pos="center" style:horizontal-rel="paragraph" fo:padding="0in" fo:border="none"/></style:style><style:style style:name="fr3" style:family="graphic" style:parent-style-name="Frame"><style:graphic-properties fo:margin-left="0in" fo:margin-right="0in" fo:margin-top="0in" fo:margin-bottom="0in" style:wrap="dynamic" style:number-wrapped-paragraphs="no-limit" style:vertical-pos="middle" style:vertical-rel="baseline" style:horizontal-pos="from-left" style:horizontal-rel="paragraph" fo:padding="0in" fo:border="none" loext:rel-width-rel="paragraph"/></style:style><style:style style:name="fr4" style:family="graphic" style:parent-style-name="Graphics"><style:graphic-properties fo:margin-left="0in" fo:margin-right="0in" fo:margin-top="0in" fo:margin-bottom="0in" style:run-through="foreground" style:wrap="none" style:vertical-pos="top" style:vertical-rel="paragraph-content" style:horizontal-pos="center" style:horizontal-rel="paragraph-content" fo:padding="0in" fo:border="none" style:shadow="none" draw:shadow-opacity="100%" style:mirror="none" fo:clip="rect(0in, 0in, 0in, 0in)" draw:luminance="0%" draw:contrast="0%" draw:red="0%" draw:green="0%" draw:blue="0%" draw:gamma="100%" draw:color-inversion="false" draw:image-opacity="100%" draw:color-mode="standard" loext:rel-width-rel="paragraph"/></style:style><style:style style:name="fr5" style:family="graphic" style:parent-style-name="Graphics"><style:graphic-properties fo:margin-left="0in" fo:margin-right="0in" fo:margin-top="0in" fo:margin-bottom="0in" style:run-through="foreground" style:wrap="none" style:vertical-pos="top" style:vertical-rel="baseline" style:horizontal-pos="center" style:horizontal-rel="paragraph-content" fo:padding="0in" fo:border="none" style:shadow="none" draw:shadow-opacity="100%" style:mirror="none" fo:clip="rect(0in, 0in, 0in, 0in)" draw:luminance="0%" draw:contrast="0%" draw:red="0%" draw:green="0%" draw:blue="0%" draw:gamma="100%" draw:color-inversion="false" draw:image-opacity="100%" draw:color-mode="standard" loext:rel-width-rel="paragraph"/></style:style><style:style style:name="Sect1" style:family="section"><style:section-properties style:editable="false"><style:columns fo:column-count="1" fo:column-gap="0in"/></style:section-properties></style:style></office:automatic-styles><office:body><office:text text:use-soft-page-breaks="true"><text:sequence-decls><text:sequence-decl text:display-outline-level="0" text:name="Illustration"/><text:sequence-decl text:display-outline-level="0" text:name="Table"/><text:sequence-decl text:display-outline-level="0" text:name="Text"/><text:sequence-decl text:display-outline-level="0" text:name="Drawing"/><text:sequence-decl text:display-outline-level="0" text:name="Figure"/></text:sequence-decls><text:h text:style-name="P67" text:outline-level="1"><text:bookmark-start text:name="__RefHeading___Toc552_3689700921"/><text:span text:style-name="T8">1 </text:span>Time Series Prediction by Artificial Neural Networks and Differential Evolution in Distributed Environment<text:bookmark-end text:name="__RefHeading___Toc552_3689700921"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P3">Models for time series prediction using Artificial Neural Networks trained with Differential Evolution in a distributed computational environment have become popular in recent decades. Time series prediction is a complex task that demands the development of more effective and faster algorithms. Artificial Neural Networks are used as a base and trained with historical data. One of the main problems is how to select an accurate Artificial Neural Network training algorithm. There are two general approaches - exact numeric and heuristic optimization methods. When a suitable heuristic is applied, training can be done in a distributed computational environment. In this case, there is a much faster and more realistic output, which helps to achieve better predictions.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P69" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc554_3689700921"/><text:span text:style-name="T9">1.1 </text:span>Introduction<text:bookmark-end text:name="__RefHeading___Toc554_3689700921"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P5">Time series prediction is the process of forecasting future values in a series of data based on their known values in the past. Decision-makers <text:soft-page-break/>hold a significant responsibility in shaping an effective investment strategy. Investing involves taking acceptable risks with the expectation of a certain profit. The critical aspect of investment lies in balancing the risks taken with the anticipated profits. The primary trading activity revolves around exchanging currencies in the currency market, commonly known as the foreign exchange market or FOREX. Currencies are highly volatile and subject to frequent price changes. When engaging in FOREX trading, decision-makers must make three crucial decisions:</text:p><text:p text:style-name="P5">1. Predict whether the price will rise or fall;</text:p><text:p text:style-name="P5">2. Determine the appropriate volume to buy or sell;</text:p><text:p text:style-name="P5">3. Decide how long to maintain the open position.</text:p><text:p text:style-name="P5"/><text:p text:style-name="P5">Although these decisions may seem straightforward, accurately estimating the price-changing direction is challenging due to many influencing factors. The order volume is directly tied to the level of risk assumed. While high-volume orders can lead to substantial profits when the price-changing direction is well-estimated, they can also result in significant losses. The duration of holding an open position is crucial for maximizing profits or minimizing losses. Financial forecasting is paramount for traders in the currency market, mainly due to the market&apos;s high price dynamics.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P4">The development of effective and reliable financial predictions is a demanding and complex task. Therefore, this field is highly relevant for <text:soft-page-break/>developing and utilizing self-organizing and self-learning systems for prediction. The application of Artificial Neural Networks for predicting time series in the field of the economy has been considered by various authors, such as Dunis [1], Giles [2], and Moody [3].</text:p><text:p text:style-name="P4"/><text:p text:style-name="P4">A widely-used method employs so-called Feed Forward Neural Networks (Haykin [4]). While Feed Forward Neural Networks types of networks are very effective, they suffer from a fundamental flaw: they lack short-term memory. This problem could be avoided by using Recurrent Neural Networks. However, Recurrent Neural Networks, on the other hand, present difficulties due to their challenge in employing precise gradient methods during the learning phase (Werbos [5]). A possible solution is a combined approach for training Artificial Neural Networks using evolutionary algorithms, as <text:span text:style-name="T12">(</text:span>Yao [6]<text:span text:style-name="T12">)</text:span> and several other authors proposed.</text:p><text:p text:style-name="P4"/><text:p text:style-name="P4">Evolutionary algorithms show significantly better results for optimum search in complex multidimensional spaces with many local optima, in which case gradient-based methods will likely get stuck (Holland [7]). In this paper, we present a model of a self-learning system for predicting time series based on ANN and DE in a distributed environment.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P69" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc556_3689700921"/><text:soft-page-break/><text:span text:style-name="T9">1.2 </text:span>Problem <text:span text:style-name="T2">D</text:span>e<text:span text:style-name="T2">fi</text:span>nition<text:bookmark-end text:name="__RefHeading___Toc556_3689700921"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P8">Several specific aspects come into play in the realm of time series prediction. Firstly, a series of values is tightly intertwined with the temporal axis, forming time-value pairs. Values within a time series exhibit interdependence; they are not isolated entities. Instead, these values are intricately linked with newer values stemming from preceding ones. For instance, consider the exchange rate between the EUR and USD. It is exceedingly uncommon for this rate to undergo dramatic shifts over two consecutive trading days. Instead, the rate displays a gradual and continuous evolution. This characteristic rate serves as a depiction of the trading dynamics between Europe and the USA.</text:p><text:p text:style-name="P8"/><text:p text:style-name="P8">The challenge in predicting financial time series lies in leveraging historical price values to construct a prediction model, subsequently employing this model to forecast future price values. If the model proves accurate, decision-makers can harness the projected values to mitigate investment risks.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P69" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc472_3627904809"/>1.3 <text:span text:style-name="T10">Machine Learning</text:span> Tools<text:bookmark-end text:name="__RefHeading___Toc472_3627904809"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P9"><text:soft-page-break/>Artificial Neural Networks are mathematical models inspired by natural neural networks. They consist of artificial neurons connected by a series of links. Information is input into the network and then propagated through internal layers (neurons) to produce an output. ANNs are designed to be self-adaptive systems capable of altering their internal structure based on external or internal information encountered during the learning process. They represent intricate relationships between inputs and outputs (functional relations) or identify patterns within a dataset (data mining).</text:p><text:p text:style-name="P9"/><text:p text:style-name="P9">The concept of ANNs draws inspiration from biological central nervous systems and their components, such as neurons, axons, dendrites, and synapses. An ANN comprises a network of uncomplicated processing elements that collectively manifest complex global behaviors via interconnections between processing units and their parameters. In practical terms, utilizing ANNs involves employing algorithms to adjust connections&apos; strength (weights), thereby achieving the desired signal flow.</text:p><text:p text:style-name="P9"/><text:p text:style-name="P9">ANNs resemble natural neural networks in that the processing elements execute network operations collectively and simultaneously. In modern computing, the ANN approach is often combined with non-adaptive techniques to yield improved practical outcomes. The primary advantage of employing ANNs is their capacity to deduce a function from observed <text:soft-page-break/>examples. This proves particularly advantageous when tackling problems where the intricacy of the studied process (or its accompanying data) renders manual function derivation exceedingly challenging.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P10">Differential Evolution is a population-based meta-heuristic optimization method rooted in evolution. DE was developed by Kenneth Price and Rainer Storn [8] and drew inspiration from classical genetic algorithms (GAs). However, meta-heuristics like DE do not guarantee the discovery of an optimal solution. DE finds application in exploring complex, high-dimensional search spaces. While it finds primary use in continuous problems involving real numbers, its applicability to discrete problems is limited. Notably, DE does not mandate differentiability, a requirement in classical optimization techniques such as gradient descent.</text:p><text:p text:style-name="P10"/><text:p text:style-name="P10">Much like classical GAs, DE employs a population of potential solutions. It generates fresh candidates by melding existing ones using crossover, mutation, and selection rules. The best candidates are retained based on objective function evaluation, obviating the need for gradient usage.</text:p><text:p text:style-name="P10"/><text:p text:style-name="P10">A key distinction arises in the mutation operator in comparing DE with GA. DE&apos;s mutation relies on the calculation of difference vectors, rendering it considerably more potent than GA&apos;s mutation approach, which adjusts individual values within the mutated chromosome. One drawback of the <text:soft-page-break/>difference vector approach lies in its difficulty in effecting minor changes to discrete values.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P69" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc474_3627904809"/>1.4 <text:span text:style-name="T10">Forecasting </text:span>Organization<text:bookmark-end text:name="__RefHeading___Toc474_3627904809"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P3">The forecasting model is built upon a standard Artificial Neural Network and employs a combined training approach involving Differential Evolution and back-propagation. The ANN&apos;s topology is a research subject and is parameterized on a remote server. Neuronal connections can take three forms: strictly forward, forward-backward, and fully connected (establishing connections with all other neurons, including themselves). A linear summation function and a sigmoid activation function characterize each neuron.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3"><text:soft-page-break/><draw:frame draw:style-name="fr2" draw:name="Frame1" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="0"><draw:text-box fo:min-height="2.6201in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image1" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="2.6201in" style:rel-height="scale" draw:z-index="1"><draw:image xlink:href="Pictures/1000000000000A1E00000549EAF7DA525E2C6E7F.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure0" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">1</text:sequence>: Computers organization</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P3"/><text:p text:style-name="P12">In Differential Evolution, a variant of genetic algorithms, each chromosome represents a set of weights for a specific topology of an Artificial Neural Network. As depicted in Figure 1, the overall DE population is showcased on a remote server. Individual computational units (client machines) load subsets of the global DE population (as illustrated in step 1 of Figure 3) and engage in localized DE-BP mixed ANN training (depicted in step 2 of Figure 3). At regular intervals, these local machines establish connections with the remote server. During these connections, the local populations are updated, and the outcomes of local computations are reported (as shown in step 3 of Figure 3).</text:p><text:p text:style-name="P12"/><text:p text:style-name="P12"><text:soft-page-break/>Due to DE&apos;s remarkable parallelism, no theoretical constraint exists on the number of computational units that can be employed. However, the remote server serves as a bottleneck in the system due to technical limitations. The server&apos;s capacity determines the maximum number of simultaneously connected clients. Notably, each client is not required to maintain a constant connection with the remote server. Consequently, this technical limitation can be easily surmounted. Each computational unit can perform calculations for weeks or even months before necessitating a connection to the remote server.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3"><draw:frame draw:style-name="fr2" draw:name="Frame2" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="2"><draw:text-box fo:min-height="3.1661in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image2" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="3.1661in" style:rel-height="scale" draw:z-index="3"><draw:image xlink:href="Pictures/1000000000000948000005DC201979DAD9828B19.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure1" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">2</text:sequence>: ANN training <text:span text:style-name="T4">with</text:span> DE-BP</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P3"/><text:p text:style-name="P13"><text:soft-page-break/>As depicted in Figure 2, the DE-based training process comprises five fundamental steps:</text:p><text:p text:style-name="P13">1. Loading a set of weights (a chromosome) from DE into the ANN.</text:p><text:p text:style-name="P13">2. Loading training examples into the ANN.</text:p><text:p text:style-name="P13">3. Calculating prediction values.</text:p><text:p text:style-name="P13">4. Computing the total prediction error (older data carry less impact on the calculated error).</text:p><text:p text:style-name="P13">5. Estimating chromosome fitness.</text:p><text:p text:style-name="P13"/><text:p text:style-name="P13">Every set of weights (chromosome) is introduced into the ANN structure. Subsequently, each input value undergoes a feed-forward process. The resultant output value is then juxtaposed with the anticipated value, and the disparity between them contributes to the overall error associated with this specific set of weights. This calculated total error serves as the fitness value for the DE population. The primary objective of DE optimization is to minimize the total error of the ANN. Due to the real-time nature of the training process (where new data continually emerges over time), the pursuit of total error minimization remains ongoing.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3"><text:soft-page-break/><draw:frame draw:style-name="fr2" draw:name="Frame3" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="4"><draw:text-box fo:min-height="2.8555in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image3" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="2.8555in" style:rel-height="scale" draw:z-index="5"><draw:image xlink:href="Pictures/1000000000000882000004D8087FEE1B151F6F0D.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure2" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">3</text:sequence>: Computation on a local machine</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P3"/><text:p text:style-name="P14">All computations are carried out locally, as depicted in Figure 3. Local computations encompass the Differential Evolution (DE) training loop, involving crossover and mutation operations. These operations are elaborated upon in Figures 4 and 5, respectively. Parent chromosomes are designated for crossover via the Genetic Algorithm (GA) selection rule. While various selection rules exist, the current model randomly selects two parents, determining the surviving parent based on a survival percentage.</text:p><text:p text:style-name="P14"/><text:p text:style-name="P14">The crossover operation is considered disruptive to the Artificial Neural Network (ANN) training process, and consequently, it can be regulated as <text:soft-page-break/>a parameter. Subsequently, the mutation phase is executed. The conventional DE mutation involves the summation of mutated chromosomes with a weighted difference vector derived from the discrepancy between two other randomly selected chromosomes and then multiplied by a weight coefficient. This mutation operator is a distinctive advantage of DE when juxtaposed with GA.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3"><text:soft-page-break/><draw:frame draw:style-name="fr2" draw:name="Frame4" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="6"><draw:text-box fo:min-height="4.7043in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image4" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="4.7043in" style:rel-height="scale" draw:z-index="7"><draw:image xlink:href="Pictures/100000000000070B0000069B6411A92D23C25DC2.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure3" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">4</text:sequence>: Crossover</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P3"/><text:p text:style-name="P3"><text:soft-page-break/><draw:frame draw:style-name="fr2" draw:name="Frame5" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="8"><draw:text-box fo:min-height="4.2898in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image5" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="4.2898in" style:rel-height="scale" draw:z-index="9"><draw:image xlink:href="Pictures/1000000000000918000007C78D825628614DB685.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure4" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">5</text:sequence>: Mutation</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P3"/><text:p text:style-name="P15">Local copies of ANNs are distributed to each computational node for parallel training. Differential Evolution and Backpropagation techniques are applied on the local machines, with constant synchronization with the remote server. When DE is used for training the ANN, the issue of slow learning rates can be mitigated by transitioning to BP training. This process involves eliminating all recurrent connections, as illustrated in Figure 6.</text:p><text:p text:style-name="P15"><text:soft-page-break/></text:p><text:p text:style-name="P15"><draw:frame draw:style-name="fr2" draw:name="Frame6" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="10"><draw:text-box fo:min-height="2.4547in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image6" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="2.4547in" style:rel-height="scale" draw:z-index="11"><draw:image xlink:href="Pictures/100000000000062500000302B66BC73B6A9ADA13.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure5" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">6</text:sequence>: Switching ANN topology for BP training</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P15"/><text:p text:style-name="P15">The rate of learning convergence serves as a valuable indicator for determining when to switch between DE and BP and vice versa. Furthermore, it is worth noting that BP can be conceptualized as a specific case of Genetic Algorithm (GA) mutation.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P68" text:outline-level="1"><text:bookmark-start text:name="__RefHeading___Toc351_966847004"/>2 Distributed Evolutionary Computing Migration Strategy by Accidental Node Involvement<text:bookmark-end text:name="__RefHeading___Toc351_966847004"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P16">Distributed evolutionary algorithms are implemented across heterogeneous computing nodes. In a distributed environment, it is common for these nodes to vary in terms of operating systems and hardware configurations. Such an environment presents significant challenges, particularly concerning network latency. Specific evolutionary optimization algorithms lend themselves well to distributed computing implementation due to their high level of parallel scalability. Typically, only the fitness function calculations are distributed synchronously or asynchronously.</text:p><text:p text:style-name="P16"/><text:p text:style-name="P16">In the former scenario, the population is solely hosted on the primary node. In the latter scenario, each node maintains a portion of the distributed population, a configuration known as the island model. Another prevalent approach relies on shared memory, granting each computing node access to the entire population - a model known as the fine-grained model. Various other models represent hybridizations of these basic approaches.</text:p><text:p text:style-name="P16"><text:soft-page-break/></text:p><text:p text:style-name="P16">Within the island model, a pivotal parameter pertains to the migration strategy. The most commonly employed node topology is the ring topology, where each node periodically forwards its best-performing individual to the subsequent node in the ring. However, exploring a hybrid model incorporating star topology and involving neighboring nodes could yield improvements.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P69" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc979_966847004"/>2.1 Introduction<text:bookmark-end text:name="__RefHeading___Toc979_966847004"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P3">Evolutionary Algorithms (EAs) represent efficient search methods grounded in natural selection and recombination principles. They have found successful applications in solving problems across various domains, including business, engineering, and science (Goldberg [9], Pappa [10]). By harnessing the power of EAs, viable solutions can be discovered within a reasonable time-frame. For more minor problems, a single computing node can suffice. However, as problems increase in size or complexity, the time required to find suitable solutions also escalates.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3">Given the burgeoning popularity of parallel computing, employing it has emerged as a promising strategy to expedite EAs. Certain distributed evolutionary algorithms (DEAs) operate with a singular population, while <text:soft-page-break/>others divide the population into multiple, relatively independent sub-populations. A comprehensive classification can be found in existing literature (Adamidis [11], Gordon [12], Lin [13]). In distributed computing, the migration strategy is pivotal in implementing DEAs.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3">In cases where EA&apos;s population is distributed among numerous computing nodes, each node possesses a portion of the population referred to as a sub-population. In such instances, local recombination and fitness function evaluations are conducted. To enhance optimization convergence, the computing nodes exchange individuals, a process is known as migration. Several parameters govern this migration process, shaping a strategy for information interchange in DEAs.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3">The primary parameter pertains to traveler selection, determining the approach for choosing an individual from the sub-population to be dispatched. In most scenarios, the optimal individual is chosen. The second crucial parameter is migration frequency, dictating how often selected individuals traverse between sub-populations. This parameter is specific to the problem and is typically adjusted through experimental means. The third and most pivotal parameter concerns the migration destination, signifying how nodes are structured in terms of topology and how each traverses within this topology.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3"><text:soft-page-break/>One option for this parameter is individual broadcasting, wherein each node receives a copy of selected travelers from other nodes. An alternative is a ring topology, where each traveler migrates to the adjacent node. A grid topology is also viable (Spiessens [14], Kruger [15]), with each node having four neighbors. Numerous other topologies exist, including hierarchical, 3D-based, hybrid, and more.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3">A distribution strategy grounded in the participation of incident nodes (where volunteers are expected to join the project and contribute computing power) holds the potential to address these challenges. Computing nodes are structured in a star topology, and the island model (Tanese [16], Uchida [17]) is applied to DEAs.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P69" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc772_951591551"/>2.2 Accidental Node Involvement<text:bookmark-end text:name="__RefHeading___Toc772_951591551"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P18">The goal of optimization is to adjust the weights of an Artificial Neural Network in order to achieve improved forecasting. Training an ANN with Evolutionary Algorithms (EAs) can be highly computationally intensive, especially when dealing with ANNs with more than 400 weights, depending on their topology. Due to the time-consuming nature of EA-based ANN training, the computing nodes should operate relatively autonomously.</text:p><text:p text:style-name="P18"><text:soft-page-break/></text:p><text:p text:style-name="P18">The distributed system is structured in a star topology, comprising a lightweight central node (server) and heavily loaded remote computing nodes (clients). In the context of Differential Evolution Algorithm (DEA) implementation, the island model is suitable. A global EA population is situated in the central node, while numerous local EA populations are distributed among the remote computing nodes. Each remote computing node can join or leave the system at any moment, functioning asynchronously (Figure 7). Upon a new client joining, the central node transmits a subset of the global population. Subsequently, communication between the central and remote nodes ceases, allowing the remote node to evolve the local EA population using a sequential EA approach.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3"><text:soft-page-break/><draw:frame draw:style-name="fr2" draw:name="Frame7" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="12"><draw:text-box fo:min-height="4.9866in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image7" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="4.9866in" style:rel-height="scale" draw:z-index="13"><draw:image xlink:href="Pictures/10000000000004150000040FC03E2C163005CE4E.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure6" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">7</text:sequence>: Accidental node involvement</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P3"/><text:p text:style-name="P3">The communication between the remote node and the central node is reinitiated only if a better solution is found on the remote node, which must then be reported to the central node. Through this organization of the calculation process, each remote node can operate for hours, weeks, <text:soft-page-break/>or even months before sending any information to the central node. In practical terms, a failure of the central node will not impact the performance of the remote nodes. Even in a central node failure, the remote nodes will transmit their results once the central node becomes available.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3">The distribution of individuals occurs exclusively during the process of remote node joining. Local best-found solutions are relayed to the central node, migrating to the subsequent remote node upon incorporation. Due to the sluggish nature of EA-based training for ANNs, this distribution strategy proves highly efficient. Additionally, in financial time series forecasting domains, the training set undergoes constant fluctuations due to the incessant influx of new data. Consequently, the objective function, aimed at optimizing the total error of ANNs, remains in a state of perpetual flux. Hence, the application of continuous ANN training becomes imperative.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P69" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc632_1783862520"/>2.3 Distribution Parameterization<text:bookmark-end text:name="__RefHeading___Toc632_1783862520"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P3">Distribution is an essential operation responsible for the seamless exchange of individuals among the nodes within the DEA framework. When considering distribution, a set of parameters becomes relevant, <text:soft-page-break/>including the distribution gap, distribution rate, selection/replacement, topology, and heterogeneity. Specific parameters can be applied in the context of the Distribution Strategy by Accidental Node Involvement, while others lack reasonable significance.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3">Defining a parameter such as the distribution gap becomes intricate in the context of Accidental Node Involvement. The literature outlines two prevalent approaches. The first one involves regularly measuring the distribution gap after several steps. The second one is probabilistic, occurring in each generation with a certain probability (Gorges-Schleuter [18], Munetomo [19], Voigt [20]). The distribution gap aligns more closely with the probabilistic model. As elucidated in the preceding sections, distribution occurs only once when the computing node joins the system. From this perspective, the distribution gap follows an exponential distribution in a probabilistic manner. The model avoids the issues of ineffectiveness or super-individual problems since distribution is relatively infrequent.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3">The distribution rate represents a parameter specific to the problem, determining the number of individuals that will traverse among the local populations. Typically, it is denoted as a population percentage or absolute value. Various recommendations exist regarding the estimation of this parameter, although the prevailing approach is experimental, as <text:soft-page-break/>evidenced by studies such as <text:span text:style-name="T11">(</text:span>Tanese [16], Belding [21], and Mejia-Olvera [22]<text:span text:style-name="T11">)</text:span>. In the proposed model, this parameter is defined as a percentage (a fraction) of the global population (i.e., the population residing on the central node). When expressed in absolute terms, it equates to the magnitude of the local population (i.e., the population size on the remote node).</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3">There are two main approaches for selecting migrants: the first involves choosing the most exceptional individuals, and the second entails selecting individuals randomly. Of course, it is worth noting that numerous alternative selection methods can be employed, similar to those utilized in genetic algorithms (Baker [23], Lim [24]). In the proposed model, the selection process occurs at the central node, and random selection is utilized. The quantity of migrants selected corresponds to the size of the remote sub-population. Notably, there is no replacement procedure since distribution occurs only once, from the central to the remote node. Based on the node joining process, several identified individuals can be directed in the opposite direction (remote to central). However, these individuals will then participate in other remote sub-populations.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3">DEAs are categorized into two standard models: the stepping-stone and island models. This categorization hinges on whether individuals have the <text:soft-page-break/>freedom to migrate to any local population or are restricted to migrating only to geographically nearby islands. Numerous studies have endeavored to determine the optimal topology for a DEA, with the ring and hyper-cube topologies emerging as the most favored choices in many instances (Adamidis [11], Gordon [12], Lin [13], Mejia-Olvera [22]). In most cases, issues such as parallelization and scalability arise with fully connected and centralized topologies due to their tight connectivity. The proposed model&apos;s most suitable topology is a star configuration featuring a centralized node and relatively independent remote computing nodes. EA-based ANN training is a time-intensive process, often leading to extended periods during which remote computing nodes operate without communication with the central node. Consequently, the central node poses minimal risk to the distributed system. Disruption of the connection with the central node does not affect the local optimization process. The central node&apos;s failure only impacts new nodes attempting to join the system. Despite these drawbacks, the proposed model exhibits exceptional scalability due to the lightweight nature of the central node.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3">Accidental Node Involvement is ideally suited for heterogeneity. On each remote computing node, a distinct optimization algorithm can be employed. This approach allows for a much-improved balance between exploration and exploitation, a well-established trade-off decision in evolutionary algorithms. The relevance of this parameter varies <text:soft-page-break/>depending on the problem. For instance, in the context of the distribution of weights in artificial neural networks, it can be seamlessly implemented within a distributed system due to the availability of highly effective gradient-based training algorithms.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P68" text:outline-level="1"><text:bookmark-start text:name="__RefHeading___Toc634_1783862520"/>3 Modifications of Artificial Neural Networks<text:bookmark-end text:name="__RefHeading___Toc634_1783862520"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P3">Classical artificial neural networks have been studied and utilized in various industries for decades. While they have demonstrated successful applications in specific tasks, their performance could be better in others. Despite being extensively researched, there remain aspects within them that can be modified to achieve greater efficiency. The key aspect of artificial neural networks is their remarkable efficiency once they are trained; however, the training process is frequently characterized by slowness and inefficiency.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3">In this section, various ideas for modifying different components within artificial neural networks will be presented. These ideas offer opportunities to enhance efficiency and reduce training time.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P69" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc718_1268615346"/>3.1 Alternative Activation Function Derivative<text:bookmark-end text:name="__RefHeading___Toc718_1268615346"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P19">Classic artificial neural networks can be conceptualized as directed weighted graphs. These networks operate in two modes  training and performance. The training process involves seeking appropriate values for <text:soft-page-break/>the weights in the graph (Keremedchiev [25], Tomov [26], Zankinski [27]) to enable the network to optimally correlate information from the input with the information at the output. In classical three-layer networks, information is propagated from the input to the output, with each node receiving signals from the nodes in the preceding layer.</text:p><text:p text:style-name="P19"/><text:p text:style-name="P19">These signals are derived using a summing function, most commonly linear (involving the multiplication of the signal by the weight assigned to the connection between two nodes). The cumulative input signals are then directed to a threshold function that determines the activation level of each node.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame8" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="5.4083in" style:rel-height="scale-min" draw:z-index="14"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image8" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="5.0201in" style:rel-height="scale" draw:z-index="15"><draw:image xlink:href="Pictures/10000000000001F4000001F476EB77554A5ADDF3.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure7" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">8</text:sequence>: Decaying sine wave activation function</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P3"/><text:p text:style-name="P3"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame11" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="5.4083in" style:rel-height="scale-min" draw:z-index="20"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image11" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="5.0201in" style:rel-height="scale" draw:z-index="21"><draw:image xlink:href="Pictures/10000000000001F4000001F4E5D0BC5BEE7C2055.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure8" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">9</text:sequence>: Exponent regulated sin<text:span text:style-name="T5">e</text:span> activation function</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P3"/><text:p text:style-name="P20">Multiple activation functions are commonly used in practice (Karlik [28]). In this context, we consider a decaying function with a periodic character (as shown in Figure 8 <text:span text:style-name="T14">and Figure 9</text:span>). The periodic component is an effect of the sine component. When employing the correct training methods, <text:soft-page-break/>such as the error backpropagation method, the derivative of the activation function becomes of primary importance. The first derivative&apos;s values directly determine the extent to which the weights will change during the training process.</text:p><text:p text:style-name="P20"/><text:p text:style-name="P20"><draw:frame draw:style-name="fr1" draw:name="Frame9" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="5.4083in" style:rel-height="scale-min" draw:z-index="16"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image9" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="5.0201in" style:rel-height="scale" draw:z-index="17"><draw:image xlink:href="Pictures/10000000000001F4000001F41B49D716BE89478B.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure9" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">10</text:sequence>: First derivative of decaying sine wave activation function</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P20"><text:soft-page-break/></text:p><text:p text:style-name="P20"><draw:frame draw:style-name="fr1" draw:name="Frame12" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="5.4083in" style:rel-height="scale-min" draw:z-index="22"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image12" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="5.0201in" style:rel-height="scale" draw:z-index="23"><draw:image xlink:href="Pictures/10000000000001F4000001F4F14C431A42B0C8A1.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure10" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">11</text:sequence>: First derivative of exponent regulated sine activation function</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P20"/><text:p text:style-name="P20">In situations where the first derivative also exhibits a periodic component (as depicted in Figure <text:span text:style-name="T15">10</text:span> <text:span text:style-name="T15">and Figure 11</text:span>), it becomes possible to replace this first derivative mechanically. Such a substitution is feasible due to the <text:soft-page-break/>organizational structure of software libraries designed for working with artificial neural networks.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P21">Each neuron within the artificial neural network determines its activity level based on a normalization function called an activation function. It is crucial to standardize the output of each neuron to a specific interval, typically ranging between zero and one or, alternatively, between minus one and plus one. This necessity arises due to the varying numbers of neurons across different layers of artificial neural networks. Without normalization, the signals transmitted to the subsequent layer would be disproportionate and uneven.</text:p><text:p text:style-name="P21"/><text:p text:style-name="P21">The activation function<text:span text:style-name="T16">s</text:span> depicted in Figure 8 <text:span text:style-name="T16">and Figure 9</text:span> <text:span text:style-name="T16">have</text:span> been selected to emulate the natural saturation processes found in nature. When the sum of inputs is positive, the neuron generates a positive output value. Conversely, in the presence of a negative input sum, the neuron yields a negative output value. Simultaneously, if the input signals are excessively intense, encompassing both negative and positive values, the simulated saturation process prevents the neuron from emitting a signal.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame10" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="5.7134in" style:rel-height="scale-min" draw:z-index="18"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image10" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="5.0201in" style:rel-height="scale" draw:z-index="19"><draw:image xlink:href="Pictures/10000000000001F4000001F412CC8C5452BC8AAC.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure11" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">12</text:sequence>: First derivative alternative of decaying sine wave activation function</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P22"/><text:p text:style-name="P22">This systematic activation mechanism enables the neurons within a given layer to evenly distribute their responsibilities, thereby fostering a more balanced representation of information throughout the network.</text:p><text:p text:style-name="P3"><text:soft-page-break/></text:p><text:p text:style-name="P3">When the activation function includes a periodic component, it is also reflected in its first derivative. On one hand, the periodic aspect of the first derivative is noticeable; however, two distinct discontinuities are also clearly evident (see Figure <text:span text:style-name="T16">12</text:span>). As a result of these complexities, the convergence of the backpropagation learning algorithm becomes slower. An elegant approach to expedite the process involves replacing the first derivative with a function that closely follows the same form yet lacks both the periodic component and breakpoints (refer to Figure 10). Not only does this alternative derivative exhibit more appropriate mathematical properties but it is also computed more quickly than the original derivative.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P69" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc1666_1268615346"/>3.2 Long Short Term Memory in Multilayer Perceptron Pair<text:bookmark-end text:name="__RefHeading___Toc1666_1268615346"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P23">In artificial neural networks&apos; most commonly used case, such as the multilayer perceptron, signals are transmitted from the input to the output. Each neuron&apos;s input information is derived from the outputs of the preceding neurons. The external impact received through this process is then subjected to a summation function. The prevailing summation function is typically the linear function, where the output signals of <text:soft-page-break/>neurons are multiplied by the connection weights. However, it is essential to note that functions other than the linear function can also be applied. The outcome of the summation function is subsequently channeled into a normalization function, which determines the neuron&apos;s activation level. The literature presents a variety of proposed activation functions, some well-established, while others are not as recognized.</text:p><text:p text:style-name="P23"/><text:p text:style-name="P23">In the classical multilayer perceptron, links between neurons exist solely from the input to the output. Signals cannot propagate from the output to the input. In the backward pass, only the neurons&apos; errors are propagated. Consequently, in an artificial neural network with such a topology, there is no capacity to retain past information circulating within the network. Due to this limitation, multilayer perceptrons are not particularly well-suited for forecasting tasks, particularly in the field of financial time series forecasting. Recurrent links were introduced to address the absence of memory in multilayer perceptrons, as seen in the Jordan and Elman networks. This innovation allows for the retention of historical information within the network.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P3"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame13" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="5.8583in" style:rel-height="scale-min" draw:z-index="24"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image13" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="5.4701in" style:rel-height="scale" draw:z-index="25"><draw:image xlink:href="Pictures/10000001000003E600000441CA747C47ADF55E41.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure12" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">13</text:sequence>: A pair of multilayer perceptrons</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P3"/><text:p text:style-name="P24">Memory can be realized using a paired network of two multilayer perceptrons, as depicted in Figure 13. In this arrangement, MLP1 takes as input the historical values of the time series combined with the output of <text:soft-page-break/>MLP2. The anticipated outcome is generated at the output layer of MLP1. Additionally, the output of MLP1 serves as an input for MLP2, followed by the propagation of signals within MLP2. This process endows MLP2 with the role of a network memory component. In this implementation, both multilayer perceptrons undergo training using backpropagation to minimize errors.</text:p><text:p text:style-name="P24"/><text:p text:style-name="P24">Prior to entering MLP1, the time series values undergo normalization. The time series is segregated into two segments: the past frame (lag) and the future frame (lead), each with its specific conditional division.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P69" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc1317_2021269790"/>3.3 Self-Growing Multilayer Perceptron <text:span text:style-name="T17">f</text:span>or Time Series Forecasting<text:bookmark-end text:name="__RefHeading___Toc1317_2021269790"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P25">Numerous techniques are applied in time series forecasting (Atanasova [29]). Artificial neural networks are one technique that is successfully applied for such forecasting. Time series consist of values measured over time while maintaining a strict order of the measurements (time-value pairs). The measurement interval is usually fixed, but variations are also possible. The fundamental concept behind time series is that values are not independent across time; they are interconnected in that future values depend on past values. A forecasting problem is defined as <text:soft-page-break/>predicting future values based on past values. To achieve successful forecasting, the construction of a prediction model is necessary. Artificial neural networks stand out as proven models in time series forecasting.</text:p><text:p text:style-name="P25"/><text:p text:style-name="P25">Initially, artificial neural networks drew inspiration from biological neural systems and first appeared in the mid-20th century. The most commonly used artificial neural networks are oriented weighted graphs, with the nodes referred to as neurons. The connections between neurons carry weights, which form the core of the information presented in the network. Artificial neural networks operate in two standard modes: training and operation.</text:p><text:p text:style-name="P25"/><text:p text:style-name="P25">The training mode is executed as an optimization task involving the modification of weights in the network to enable the best learning of training patterns. Over the last four decades, numerous training algorithms have been developed, but the most popular one remains the backpropagation of the error. Backpropagation of the error is a precise numerical method and the preferred training approach in this study. The idea revolves around minimizing the total neural network error across all training examples processed. The gradient of the total error determines the direction and magnitude of the weight updates. The organization of links between neurons follows a typical pattern in artificial neural network topology. Various topologies have been extensively researched in the <text:soft-page-break/>literature, including generalized nets (Tashev [30]) and deep-learning neural networks. When dealing with noisy time series data, input information can be filtered using techniques such as a Kalman filter (Alexandrov [31]).</text:p><text:p text:style-name="P25"/><text:p text:style-name="P25">The critical concept in deep learning neural networks departs from the traditional approach, wherein instead of using hidden layers, the number of nodes in both the input and hidden layers increases during neural network training. Expanding the input layer is motivated by each new measurement expanding the time series. The training objective is to increase the input layer size to match the complete time series size.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P26">Conditionally, the time series is divided into the past and the future. The values supplied to the input of the artificial neural network are called lags, and they constitute a subset of the closest past values to the future values. The values obtained in the output of the artificial neural network are predictions, which are then compared with a subset of the future values called leads. A multilayer perceptron is employed as the base artificial neural network for the proposed model. It consists of input, hidden, and output layers.</text:p><text:p text:style-name="P26"/><text:p text:style-name="P26">In the proposed model, a set of artificial neural sub-networks is utilized, and these sub-networks are integrated into a comprehensive artificial <text:soft-page-break/>neural network. The smallest artificial neural sub-network features a 1-1-1 topology (Figure 14-left). The network is trained with examples containing only a single value in the input. The model&apos;s objective is to forecast only one value ahead of time, which is why all sub-networks yield only a single output. Figure 14-left displays just 3 intermediary training examples. All 29 input values are provided as training instances for resilient backpropagation training. Training halts once a specific epsilon level for total neural network error change is reached.</text:p><text:p text:style-name="P26"/><text:p text:style-name="P26"><draw:frame draw:style-name="fr3" draw:name="Frame14" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="3.5429in" style:rel-height="scale-min" draw:z-index="26"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image14" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="2.85in" style:rel-height="scale" draw:z-index="27"><draw:image xlink:href="Pictures/1000000000000826000004A31930735E10CAF8DB.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure13" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">14</text:sequence>: Training of artificial neural sub-networks with 1-1-1 topology (left) and 2-1-1 topology (right)</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P26"/><text:p text:style-name="P26"><text:soft-page-break/>After training the 1-1-1 topology, the weight values from the first sub-network are transferred to the second sub-network with a 2-1-1 topology (Figure 14-right). It is evident that one weight will not be transferred since it is absent in the first sub-network. This particular weight retains its value from the prior training of the largest sub-network. The time series is restructured to provide two input values and anticipate one forecasted value in the output. For the second sub-network, there exist 28 input examples, and a single output is anticipated. The training process mirrors that of the first sub-network - utilizing resilient backpropagation training. Like the first sub-network, training concludes upon reaching a specific epsilon level for total neural network error change.</text:p><text:p text:style-name="P26"/><text:p text:style-name="P26">The third sub-network adopts a 3-2-1 topology. The hidden layer&apos;s size is determined automatically by an incremental pruning algorithm. Figure 15-left exhibits two neurons in the hidden layer, though this is merely illustrative; the algorithm estimates the actual hidden layer size. The training algorithm and stopping criteria are consistent with the previous sub-networks.</text:p><text:p text:style-name="P26"/><text:p text:style-name="P26"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame15" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="3.5429in" style:rel-height="scale-min" draw:z-index="28"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image15" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="2.85in" style:rel-height="scale" draw:z-index="29"><draw:image xlink:href="Pictures/1000000000000827000004A3100690A9A715E8CD.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure14" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">15</text:sequence>: Training of artificial neural sub-networks with 3-2-1 topology (left) and 4-2-1 topology (right)</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P26"/><text:p text:style-name="P26">The fourth sub-network employs a 4-2-1 topology, and once again, the hidden layer&apos;s size is illustrative (Figure 15-right). An incremental pruning algorithm estimates the true hidden layer size. Training instances are reduced by one compared to the previous sub-network due to the increased input size by one. Training and stopping criteria align with those of the prior sub-networks. Figures 14 and 15 exclusively present the initial 4 sub-networks. In the model implementation, a multitude of additional sub-networks are involved. Sub-network topologies evolve by adding a single neuron to the input layer and adapting the hidden layer size through an incremental pruning algorithm. The ultimate aim is to attain <text:soft-page-break/>an n-m-1 topology (Figure 16), encompassing all known time series values. Numerous connections between the input and the hidden layers in Figure 16 are omitted for clarity, but both layers are fully interconnected in the model implementation.</text:p><text:p text:style-name="P26"/><text:p text:style-name="P26"><draw:frame draw:style-name="fr2" draw:name="Frame16" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="30"><draw:text-box fo:min-height="1.8626in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image16" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="1.8626in" style:rel-height="scale" draw:z-index="31"><draw:image xlink:href="Pictures/1000000000000714000002A1E531DFDE666FE3D4.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure15" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">16</text:sequence>: Training of artificial neural sub-network with n-m-1 topology as certain links between the input and hidden layers are omitted for better visualization</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P26"/><text:p text:style-name="P26">Following the training of the largest sub-network, the hierarchical process regresses back to the smallest sub-network. Weight values from the largest sub-network corresponding to the links in the smaller sub-network are extracted and integrated into the smallest sub-network. Similarly, weight values are sourced from the largest sub-network for other sub-networks, combined with the weights from the previous, smaller sub-network. For instance, the sub-network with a 4-2-1 topology borrows <text:soft-page-break/>some of its weights from the 3-2-1 sub-networks, but links absent in the smaller sub-network are taken from the largest sub-network.</text:p><text:p text:style-name="P26"/><text:p text:style-name="P26">The proposed model&apos;s fundamental concept is the incremental training of increasingly sized artificial neural networks. This training approach is inspired by natural neural systems, wherein biological cells proliferate and form connections. A common issue in artificial neural network training is network size. The training process is expedited by segmenting the largest network into smaller ones. It is widely recognized in the field of time series forecasting that the earliest measurements exert minimal influence on the forecast. The proposed model accommodates this reality by incorporating the oldest measurements in the largest sub-network, although their impact on the final forecast remains relatively minor. The model boasts a heightened degree of self-adaptation; as new values in the time series emerge, the artificial neural network&apos;s size expands, synchronizing the training and operational phases.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P69" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc2431_612792312"/>3.4 Permutations in Graph Structure<text:bookmark-end text:name="__RefHeading___Toc2431_612792312"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P29">The most commonly used approach to training a multilayer perceptron is the exact numerical method with error backpropagation. Exact numerical methods are most often gradients, which impose certain restrictions on <text:soft-page-break/>the type of activation function. Some of the limitations of exact numerical methods are overcome by using evolutionary heuristics for global optimization. Using heuristics for global optimization also provides an additional opportunity to train artificial neural networks in a distributed environment with multiple computing machines.</text:p><text:p text:style-name="P29"/><text:p text:style-name="P29">A multilayer perceptron consists of neurons (graph nodes) and connections (edges in the weight graph). Neurons are organized into layers, each connected by a weighted link to every neuron in the next layer (see Figure 17). Information from the external environment enters the input layer, passes through the inner (there may be more than one) layers, and leaves the neural network through the output layer. In addition to input, output, and ordinary neurons, the multilayer perceptron has one additional bias neuron, which constantly emits a single value and has no input edges.</text:p><text:p text:style-name="P29"/><text:p text:style-name="P29"><text:soft-page-break/><draw:frame draw:style-name="fr2" draw:name="Frame17" text:anchor-type="as-char" svg:width="5.0161in" draw:z-index="32"><draw:text-box fo:min-height="4.0126in"><text:p text:style-name="Figure"><draw:frame draw:style-name="fr5" draw:name="Image17" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="3.9598in" style:rel-height="scale" draw:z-index="33"><draw:image xlink:href="Pictures/100000010000023F0000020A1B3CFD7EF9D20CAF.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure16" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">17</text:sequence>: Three-layer network</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P29"/><text:p text:style-name="P29">Being the most common and one of the most effective algorithms for training artificial neural networks, error backpropagation has the primary task of calculating values for the weights in the network so that the network successfully learns the functional dependence between the input and output data. Error backpropagation is an exact gradient optimization method. It consists of two main phases  forward pass and backward pass. During the forward calculation, signals are passed from the network&apos;s input to its output. The error made during the forward pass is <text:soft-page-break/>determined at the network&apos;s output. The reverse pass follows based on the calculated error, where the error is propagated back through the layers. This backpropagation allows the calculation of the fractional error committed by each neuron. Using the calculated partial error, a decision is made regarding how much the weights connecting the neuron to the neurons of the previous layer should be adjusted.</text:p><text:p text:style-name="P28"/><text:p text:style-name="P28">The main task in training artificial neural networks using the error backpropagation method is to arrive at weight values in the network so that the network incurs minimal total error when given training and test examples. A primary aspiration of scientists working in the field of artificial neural networks is to search for algorithms, approaches, and methods that can accelerate the process of training artificial neural networks to the maximum extent.</text:p><text:p text:style-name="P28"/><text:p text:style-name="P28">Building upon the research and results published by (Zankinski [27]), the present study builds on the idea by modifying the error backpropagation algorithm. The modification involves deactivating a randomly chosen neuron during the weight correction process (see Figures 18-20) in the backward pass of the error backpropagation algorithm.</text:p><text:p text:style-name="P28"/><text:p text:style-name="P28"><text:soft-page-break/>In weight correction, the randomly selected neuron does not participate in the backpass. This allows some weights in the network to remain unchanged until the backward pass of the next epoch is executed.</text:p><text:p text:style-name="P28"/><text:p text:style-name="P28">It is essential to note that all neurons participate in the forward pass and contribute to the output signals of the network, including the randomly chosen one that does not participate in the backpropagation process.</text:p><text:p text:style-name="P27"/><text:p text:style-name="P27"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame18" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="4.9382in" style:rel-height="scale-min" draw:z-index="34"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image18" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="4.55in" style:rel-height="scale" draw:z-index="35"><draw:image xlink:href="Pictures/100000010000023F0000020A4AA275F3F12CAC9F.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure17" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">18</text:sequence>: First learning cycle</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P27"/><text:p text:style-name="P27"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame19" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="4.9382in" style:rel-height="scale-min" draw:z-index="36"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image19" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="4.55in" style:rel-height="scale" draw:z-index="37"><draw:image xlink:href="Pictures/100000010000023F0000020AC658451223671A88.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure18" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">19</text:sequence>: Second learning cycle</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P27"/><text:p text:style-name="P27"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame20" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="4.9382in" style:rel-height="scale-min" draw:z-index="38"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image20" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="4.55in" style:rel-height="scale" draw:z-index="39"><draw:image xlink:href="Pictures/100000010000023F0000020AF4B42A21486DCAE3.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure19" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">20</text:sequence>: Third learning cycle</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P27"/><text:p text:style-name="P27">The foundation of this proposal lies in the fact that in natural nervous systems, some nerve cells die or become overloaded with signals.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P30">As proposed by (Zankinski [27]), the algorithm for permuting neurons leads to another idea for permuting artificial neural network weights. Neuron permutation involves a more extensive scope than permuting two <text:soft-page-break/>individual weights, rendering this approach more favorable. The selection of which weights to swap is made randomly. Notably, the training algorithm remains unchanged compared to the classic backpropagation method.</text:p><text:p text:style-name="P30"/><text:p text:style-name="P30">The process of selecting neurons for weight swapping is executed randomly but with consideration to preventing neurons within the same layer from self-selecting. The permutation is not applied continuously; instead, it follows a probabilistic rate set as a constant value.</text:p><text:p text:style-name="P30"/><text:p text:style-name="P30">Weights are swapped for a single training cycle, after which they are restored to their original positions. The weight swapping occurs briefly since prolonged swapping would impede the backpropagation procedure, hindering convergence. This type of weight permutation can be likened to introducing a training noise. A crucial modification involves swapping weights with minimal differences. If the disparity between swapped weights is substantial, the algorithm becomes more detrimental to the training process than beneficial.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P31">Backpropagation is an exact numerical method. However, it has the disadvantage of becoming trapped in local optima. Numerous attempts have been made to utilize stochastic or hybrid training algorithms to address this limitation.</text:p><text:p text:style-name="P31"><text:soft-page-break/></text:p><text:p text:style-name="P31">Another idea, originating from the proposed neuron permutation by (Zankinski [27]), involves the permutation of activation functions as a modification of the backpropagation training algorithm used for multilayer perceptrons.</text:p><text:p text:style-name="P31"/><text:p text:style-name="P31"><draw:frame draw:style-name="fr1" draw:name="Frame21" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="3.4382in" style:rel-height="scale-min" draw:z-index="40"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image21" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="2.4402in" style:rel-height="scale" draw:z-index="41"><draw:image xlink:href="Pictures/100000010000064A0000030E180B89FD985739EE.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure20" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">21</text:sequence>: Components of an artificial neuron ( https://commons.wikimedia.org/wiki/File:ArtificialNeuronModel_english.png )</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P31"/><text:p text:style-name="P31">Each neuron in an artificial neural network has weights attached to it, a transfer function (usually a sum of the weighted input signals), and an activation function (Figure 21).</text:p><text:p text:style-name="P31"/><text:p text:style-name="P31"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame22" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="3.9075in" style:rel-height="scale-min" draw:z-index="42"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image22" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="2.9098in" style:rel-height="scale" draw:z-index="43"><draw:image xlink:href="Pictures/1000000100000335000001DCDB7BC7E6AA5645AD.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure21" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">22</text:sequence>: Multilayer perceptron ( https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q_o8pu8aPRGKo2Bq5KvzCQ.png )</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P31"/><text:p text:style-name="P31">The neurons within the topology of the multilayer perceptron network are organized in layers (Figure 22). At the same time, the input layer is responsible for receiving inputs, neurons in the hidden layer relay signals within the network. The transfer function involves collecting input signals through weighted multiplication. The most commonly used transfer function is summation. After signal collection, normalization is applied to emit a single signal from the neuron. This normalization is achieved using an activation function. Although various mathematical functions can serve as activations, the most prevalent options are hyperbolic tangent and <text:soft-page-break/>sigmoid functions. The activation function plays a significant role as the number of input links to a single neuron can vary from a few to hundreds.</text:p><text:p text:style-name="P31"/><text:p text:style-name="P31">The weights themselves constitute another crucial factor in neural network training. In classical multilayer perceptrons, weights are unconstrained real numbers. This implies that large negative and positive values can influence the regular functioning of artificial neural networks. The issue of summation involving a wide range of multiplications between input signals and weights can lead to uneven participation of different neurons in the network. These challenges are addressed through normalization using an appropriate activation function.</text:p><text:p text:style-name="P31"/><text:p text:style-name="P31">The proposed concept involves employing a network with an initial topology of three layers and using hyperbolic tangent as the activation function for each neuron. Several neurons are randomly selected, and their activation functions are switched from hyperbolic tangent to the sigmoid function for a single training cycle. This approach achieves the permutation of the activation functions.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P32">The permutation of weights can be expanded beyond a single artificial neural network by employing two parallel multilayer perceptrons. Each perceptron consists of three layers: input, hidden, and output. The primary multilayer perceptron utilizes the hyperbolic tangent activation <text:soft-page-break/>function, while the secondary one employs the sigmoid function. In each training cycle, with a specified probability, randomly chosen weights from the secondary multilayer perceptron are duplicated into the primary multilayer perceptron. Both artificial neural networks share an identical topology, ensuring a direct correspondence of weights between the primary and secondary networks.</text:p><text:p text:style-name="P32"/><text:p text:style-name="P32">Both networks undergo training using a back-propagation training procedure and are provided with identical input-output training examples. The similarity between the shapes of the sigmoid function and the hyperbolic tangent introduces additional noise to the primary network, which aids in escaping local optima. This enhancement facilitates improved convergence during the training phase.</text:p><text:p text:style-name="P33"/><text:h text:style-name="P68" text:outline-level="1"><text:bookmark-start text:name="__RefHeading___Toc2345_1821873452"/>4 Evolutionary Algorithms in Games Combinatorial Problems<text:bookmark-end text:name="__RefHeading___Toc2345_1821873452"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P56">Frequently, games present tasks characterized by combinatorial complexity. This inherent quality renders them captivating to individuals of all ages, sparking both intrigue and intellectual engagement. Beyond their recreational appeal, games have spurred the development of a field of study known as game theory. This discipline finds applications in a diverse array of domains, including economics, military strategy, the entertainment industry, and various other facets of life.</text:p><text:p text:style-name="P56"/><text:p text:style-name="P56">Challenges imbued with combinatorial complexity often necessitate exploring solutions through exhaustive search, dynamic optimization techniques, and, most notably, the employment of heuristics. These heuristic approaches become particularly invaluable when dealing with expansive search spaces, aiding the quest to unravel complex problems.</text:p><text:p text:style-name="P3"/><text:h text:style-name="P69" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc3265_968957226"/>4.1 Solving Combinatorial Puzzles with Parallel Evolutionary Algorithms<text:bookmark-end text:name="__RefHeading___Toc3265_968957226"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P57"><text:soft-page-break/>Rubik&apos;s Cube, a brain-teasing puzzle, was conceived and introduced to the world by Erno Rubik in the 1970s. Since its inception, it has captured the fascination of puzzle enthusiasts globally. The original Rubik&apos;s Cube comprises 333 cubical segments, each adorned with stickers in six distinct colors on the exposed sides. These six planes (331) can be manipulated through rotations of 90, 180, 270, or 360 degrees relative to other parts of the puzzle. In its pristine state, all sides of the cube display a single color.</text:p><text:p text:style-name="P57"/><text:p text:style-name="P57">The challenge arises when the puzzle is scrambled through a series of random rotations of the (331) sides, creating a staggering number of possible combinations. Due to its sheer complexity, restoring the cube to its original state becomes a formidable combinatorial optimization task. To put this into perspective, there are approximately 4.325210^19 combinations, a figure elucidated by (Korf [32]), and every one of these combinations can be reached from any initial configuration. The ultimate objective is to arrive at a sequence of moves that aligns all sub-cubes based on their colors, effectively solving the puzzle. According to Korf&apos;s estimation, these resolution sequences typically range from 50 to 100 moves when the cube is thoroughly scrambled.</text:p><text:p text:style-name="P57"/><text:p text:style-name="P57">When confronted with an optimization problem that involves a sequence of commands, evolutionary algorithms emerge as ideal candidates for <text:soft-page-break/>finding optimal or sub-optimal solutions. In this context, we explore the application of parallel genetic algorithms to tackle the Rubik&apos;s Cube challenge. Moreover, we can enhance the evaluation function by incorporating the Hausdorff distance component, further refining the search for optimal solutions.</text:p><text:p text:style-name="P3"/><text:p text:style-name="P58">Genetic algorithms are global optimization strategies inspired by the theories of biological evolution. The role of genetic algorithms and genetic programming within the realm of meta-heuristics is well illustrated in Figure 23.</text:p><text:p text:style-name="P58"/><text:p text:style-name="P58"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame23" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="5.9126in" style:rel-height="scale-min" draw:z-index="44"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image23" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="4.6098in" style:rel-height="scale" draw:z-index="45"><draw:image xlink:href="Pictures/1000000100000276000002430991FFA5714E2F9F.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure22" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">23</text:sequence>: Euler diagram depicting the various classifications of metaheuristics ( https://upload.wikimedia.org/wikipedia/commons/c/c3/Metaheuristics_classification.svg )</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P58"/><text:p text:style-name="P58">In this approach, solutions to a specific problem are represented as vectors of values within the solution space. These selected solution <text:soft-page-break/>vectors collectively constitute the algorithm&apos;s population. The most common method for establishing the initial population is generating random vectors. Each new generation emerges in the population following the recombination of selected individuals. In genetic algorithms, recombination is achieved through two consecutive operators: crossover and mutation. The application of the selection operator determines the selection of individuals for the recombination process. It is pretty common to apply the elitism rule during the selection process. Elitism ensures that a certain percentage of the best-found solutions persist until the very end of the optimization process. A stopping criterion is required since genetic algorithm-based optimization is an iterative process. The most commonly used stopping criterion is a predetermined number of generations for the genetic algorithm.</text:p><text:p text:style-name="P58"/><text:p text:style-name="P58">Genetic algorithms serve as the foundation for genetic programming in this research. In this context, each element of the solution vector represents an operation applied to the state of the Rubiks cube. An ordered sequence of such instructions constitutes an algorithmic program. Genetic algorithms are particularly suitable for implementation in parallel or distributed computing environments because there is no direct intermediate relationship between individuals within a given population. The genetic algorithm&apos;s population can be easily divided into numerous sub-populations, distributed across multiple processors/cores, <text:soft-page-break/>or even heterogeneous computers within a cluster. The preferred approach involves partitioning the global population. However, in cases where only the fitness value calculation is time-consuming, the population is retained on the central processor/computer, with only fitness value calculations being dispatched to other contributing processors/computers.</text:p><text:p text:style-name="P34"/><text:p text:style-name="P34">When a sub-population distribution calculation scheme is selected, a strategy for individual migration should be implemented. Migration between different islands is necessary for the best-found solutions to be available in some or all sub-populations. When the implementation of the calculation is organized as a donated distributed computing project and a new remote contributing computer is included, a fresh subset of the global population can be provided. With such a strategy, solutions are much more thoroughly investigated, resulting in improved space exploration.</text:p><text:p text:style-name="P34"/><text:p text:style-name="P36">The core of the optimization code revolves around the representation of the Rubiks Cube in computer memory. For this concept, the cube is depicted using six two-dimensional arrays, one for each side, each with dimensions of 3x3. These arrays&apos; values are integer numbers corresponding to the cube&apos;s colors. While more advanced digital <text:soft-page-break/>representations exist, as suggested by (Korf [32]), this approach proves to be more practical from an algorithmic perspective.</text:p><text:p text:style-name="P36"/><text:p text:style-name="P36">Data structures constitute the first aspect of the modeling process, while the second aspect involves the algorithmic operations carried out on these data structures. Since the cube has six sides, the minimum number of operations needed for manipulating the cube is six. These operations are denoted by six capital letters, as proposed by (Randall [33]):</text:p><text:p text:style-name="P36">-T (Top) - 90 clockwise rotation of the top side.</text:p><text:p text:style-name="P36">-L (Left) - 90 clockwise rotation of the left side.</text:p><text:p text:style-name="P36">-B (Back) - 90 clockwise rotation of the back side.</text:p><text:p text:style-name="P36">-R (Right) - 90 clockwise rotation of the right side.</text:p><text:p text:style-name="P36">-F (Front) - 90 clockwise rotation of the front side.</text:p><text:p text:style-name="P36">-D (Down) - 90 clockwise rotation of the down side.</text:p><text:p text:style-name="P36"/><text:p text:style-name="P36">This set of six operations forms the minimal, fully functional grammar of the Rubik&apos;s Cube. Extended grammars are also possible by including counterclockwise operators (+T, +L, +B, +R, +F, +D, -T, -L, -B, -R, -F, -D).</text:p><text:p text:style-name="P36"/><text:p text:style-name="P36">The next level of grammar extension involves the addition of the number of turns (+1T, +2T, +3T, +1L, +2L, +3L, +1B, +2B, +3B, +1R, +2R, +3R, +1F, +2F, +3F, +1D, +2D, +3D, -1T, -2T, -3T, -1L, -2L, -3L, -1B, -2B, -3B, -1R, -2R, -3R, -1F, -2F, -3F, -1D, -2D, -3D).</text:p><text:p text:style-name="P35"><text:soft-page-break/></text:p><text:p text:style-name="P35">With these proposed ideas for a formal Rubiks Cube grammar, the preferred approach is to represent genetic algorithm individuals as formal grammar sentences with variable lengths. Each letter can appear at any position and can be repeated multiple times within the chromosome. As mentioned in (Korf [32]), the expected average length of the chromosomes can range between 50 and 100.</text:p><text:p text:style-name="P35"/><text:p text:style-name="P35">A single cut point is selected for population crossover, although other options, as suggested by (Poli [34]), are also applicable. As a mutation operator, random changes to a single instruction are chosen. Randomly selected parents are used for selection, but the elitism rule is applied. The instructions encoded within the individual are applied to a scrambled cube to evaluate the newly created individuals. Subsequently, the cube&apos;s state is compared to the target state (a solved cube). For each pair of cube sides, the Euclidean distance is calculated. Following this, the maximum of the minimum distances is determined based on the rules of the Hausdorff distance. The resulting fitness value is positive because the Euclidean distance is calculated using positive integers (mapping the cube&apos;s colors to integers), and the Hausdorff distance is a calculation of the maximum of the minimums. A smaller fitness value indicates a better solution to the puzzle.</text:p><text:p text:style-name="P34"/><text:h text:style-name="P69" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc3267_968957226"/><text:soft-page-break/>4.2 Virtual Reels in Slot Machines<text:bookmark-end text:name="__RefHeading___Toc3267_968957226"/></text:h><text:p text:style-name="P34"/><text:p text:style-name="P37">In gambling, slot machines are one of the most prevalent forms of betting entertainment. In today&apos;s digital age, these machines come in two primary formats: standalone electronic devices and web applications accessible over the Internet. Modern slot machines, while preserving the classic charm, have undergone a transformation. They now operate with virtual reels that spin and halt at randomly selected positions, with only segments of these virtual reels visible on the screen.</text:p><text:p text:style-name="P37"/><text:p text:style-name="P37">The key to winning lies in forming specific combinations based on preset patterns, with each symbol carrying its own weight in the process. The rarest symbols hold the promise of the highest payouts, while the more common ones yield comparatively more minor rewards. Achieving the proper arrangement of symbols on the virtual reels is a discrete, combinatorial challenge, intending to meet predetermined statistical parameters like return to player and volatility.</text:p><text:p text:style-name="P37"/><text:p text:style-name="P37">Given the intricate nature of this task, meta-heuristic algorithms have found extensive application in optimizing these arrangements. These algorithms ensure slot machines deliver thrilling and potentially lucrative gaming experiences that keep players returning for more.</text:p><text:p text:style-name="P34"/><text:p text:style-name="P38"><text:soft-page-break/>The primary focus of this proposition revolves around the distribution of symbols on the machine reels. Each reel contains ordered symbols, typically represented with numerical values in mathematical models. The sequence in which these symbols appear on the reel follows a discrete probability distribution. Different winning combinations can emerge on the game screen based on this distribution. Mathematicians responsible for designing slot machine reels have the task of selecting a discrete probability distribution that yields a specific RTP value. RTP is determined by dividing the money won by the lost and multiplying the result by one hundred. The RTP value holds significant importance for slot machine vendors as it is a crucial gambling parameter subject to government regulation. The RTP value carries mathematical significance as an expected value. While this discussion excludes volatility, another notable parameter in slot machine games, it is worth mentioning that it is less frequently considered during government regulatory processes. The proposed approach for optimizing the discrete distribution of symbols could prove beneficial for mathematicians developing new slot gambling games.</text:p><text:p text:style-name="P38"/><text:p text:style-name="P38">Slot machines operate on the fundamental concept of spinning reels. Initially, slot reels were mechanical, and players manually pulled the game handle to initiate the spin. Today, most slot machines employ computerized game reels that exist solely in a virtual realm, with stops <text:soft-page-break/>being selected by a Random Number Generator (RNG) (Brysbaert [35]), which provides more information. In most games, five independent reels are presented, although there are variations with three or more than five reels. When the push button is pressed, the reels begin to spin, with each one coming to a stop sequentially. The player&apos;s winnings are determined based on combinations of symbols displayed on the screen. Each game features its unique paytable, typically accessible to the player on a separate screen. Certain symbols appear more frequently on the reels than others. Less common symbols lead to winning combinations less frequently, resulting in larger payouts for the player.</text:p><text:p text:style-name="P38"/><text:p text:style-name="P38">The primary characteristic defining each slot machine is its RTP percentage. This parameter is calculated by taking the ratio of winnings to losses and multiplying it by one hundred. RTP percentages can range from 80% in Las Vegas to as high as 98% in some EU member states. In the United Kingdom, fruit machines are legally mandated to pay out a minimum percentage within a short time-frame (Parke [36]), which offers further insights). Typically, RTP percentages exceed 90%. Achieving the desired RTP necessitates collaboration between mathematicians and game designers to strategically populate game reels with appropriate symbols according to a discrete distribution (Osesa [37]), providing additional information.</text:p><text:p text:style-name="P34"/><text:p text:style-name="P39"><text:soft-page-break/>Genetic algorithms, as search heuristics inspired by the process of natural selection (Eiben [38], Ting [39]), can be efficiently employed in problem-solving. They are routinely used to generate points (candidate solutions) within the solution space. By applying techniques such as inheritance (crossover), mutation, and selection, these generated points can move closer to the optima. GAs are also classified as population-based algorithms because each point in the solution space represents an individual within the GA population. Each individual possesses a set of properties that are subject to mutation and modification, usually through crossover. The traditional representation of these properties is binary, using a sequence of 0s and 1s, although other encodings, such as binary trees, are also available.</text:p><text:p text:style-name="P39"/><text:p text:style-name="P39">The optimization process typically begins with a randomly generated population of individuals but is subject to implementation details. This optimization process is iterative, with each iteration referred to as a generation. For each individual within a generation, a fitness value is calculated. This fitness value typically corresponds to the objective function under optimization. The fittest individuals within the population are selected according to a specified selection rule and are then recombined through crossover and/or mutation to form a new generation. This new generation is subsequently used in the next iteration of the algorithm. Algorithm termination is usually triggered by reaching a <text:soft-page-break/>maximum number of generations or attaining the desired level of fitness value.</text:p><text:p text:style-name="P39"/><text:p text:style-name="P39">To run GAs successfully, two key components need to be provided:</text:p><text:p text:style-name="P39">1. Genetic representation of the solution space (solution domain).</text:p><text:p text:style-name="P39">2. An appropriate fitness function for evaluating the solution domain.</text:p><text:p text:style-name="P39"/><text:p text:style-name="P39">Once these two conditions are met, GAs can proceed with population initialization and iterative population improvement by repetitively applying selection, crossover, mutation, and individual evaluation.</text:p><text:p text:style-name="P34"/><text:p text:style-name="P40">In the optimization model, each individual consists of slot machine symbols represented as numbers distributed on the reels. Each symbol on the reels corresponds to a single integer number, and both the symbol and its position on the reel hold significance. The solution domain is finite and discrete, with each position on each reel being a single integer number from a list of possible game symbols.</text:p><text:p text:style-name="P40"/><text:p text:style-name="P40">Population initialization is typically achieved through random generation. However, the proposed model employs an initial reels configuration to initiate the population. This initialization process involves introducing random noise to the initial reels configuration. The population size varies <text:soft-page-break/>and is determined through experimental estimation, ranging from several individuals to hundreds or even thousands.</text:p><text:p text:style-name="P40"/><text:p text:style-name="P40">During the selection process, individuals are chosen based on their fitness values. Some selection methods favor the best individuals, while others prefer a random population subset. The fitness function is problem-specific and is defined over the genetic representation as a measure of the quality of the represented solution. The fitness function calculates the absolute difference between the desired RTP and the obtained RTP in the proposed model. To calculate the obtained RTP, Monte-Carlo simulation is utilized to estimate the behavior of the slot machine in either 100,000 or 1,000,000 separate runs. The elitism rule is also applied to ensure that the best individual survives between generations.</text:p><text:p text:style-name="P40"/><text:p text:style-name="P40">A pair of parent individuals are selected from the chosen population subset for the crossover operation. A single-point cut is employed to recombine attributes of the first and second parent to create a child individual. Further research is needed to determine if using more than two individuals as parents is advantageous. Following the crossover, mutation is applied to the child by randomly selecting a symbol and replacing it with another randomly selected symbol.</text:p><text:p text:style-name="P40"/><text:p text:style-name="P40"><text:soft-page-break/>Termination criteria include a maximum number of generations and manual observation/termination of the process. The final solution, discovered through genetic algorithms, is an integer vector. For instance, in the case of a slot game with 5 reels (displayed on the screen as 5 columns and 3 rows), each reel having 63 symbols, the GA&apos;s final solution would be an integer vector with 5x63 values.</text:p><text:p text:style-name="P34"/><text:p text:style-name="P41">In 1984, Inge Telnaes received a patent for a device titled &quot;Electronic Gaming Device Utilizing a Random Number Generator for Selecting the Reel Stop Positions&quot;, US Patent 4448419 (Inge [40]). Slot machines are the most popular casino gambling method, constituting approximately 70 percent of the average US casino income (Cooper [41]). The size of this market has led to the development of more advanced optimization approaches in game design, with one of the options being Discrete Differential Evolution. Such a discrete distribution of virtual reels can be achieved through discrete optimization, adhering to specified constraints such as desired RTP, prize equalization, and symbol diversity. Symbol diversity can be easily calculated without simulation based on these three criteria, while RTP and prize equalization require Monte Carlo simulations to be incorporated into the DDE cost function.</text:p><text:p text:style-name="P41"/><text:p text:style-name="P41">A multi-criteria cost function is transformed into a single criterion through linear transformation, with coefficients assigned to each criterion. The <text:soft-page-break/>decision maker selects coefficients according to their personal preferences. In this idea proposition, score 1 was assigned to symbol diversity, 100 to RTP, and 10 to prize equalization. These numbers were chosen in accordance with the relative importance of each criterion. Symbol diversity pertains to the arrangement of symbols of the same kind next to each other within a single reel. Achieving symbol diversity is straightforward through simple swaps of symbols that are in an inappropriate order. Slot machine reels are treated as individuals in DDE optimization, allowing for control over symbol diversity even before the Monte Carlo simulation. This cost function is preferred for this criterion to facilitate the exploration of the solution space.</text:p><text:p text:style-name="P34"/><text:p text:style-name="P43">Differential Evolution is one of the stochastic optimization algorithms. It addresses the following search problem: Minimizing an objective function, which represents a mapping from a parameter vector &apos;x&apos; in an n-dimensional real-value space to a one-dimensional real-value space. DE encompasses self-organization for mutation, crossover, and selection, but its strategy parameters are chosen empirically (Price [42]).</text:p><text:p text:style-name="P43"/><text:p text:style-name="P43">DE exhibits similarities to traditional evolutionary algorithms; however, it does not utilize binary encoding like a simple genetic algorithm (Goldberg [43]), and it does not rely on a probability density function to self-adapt its parameters, as is the case in Evolution Strategy (Hans-Paul [44]). DE <text:soft-page-break/>distinguishes itself through its mutation process, executed based on the distribution of solutions within the population. Consequently, search directions and potential step sizes are contingent on the selected individuals&apos; positions to compute the mutation values. The most widely recognized model is referred to as DE/rand/1/bin, where DE stands for Differential Evolution, &quot;rand&quot; signifies that individuals chosen to calculate the mutation values are selected randomly, &quot;1&quot; denotes the number of pairs of solutions chosen, and &quot;bin&quot; indicates the use of binomial recombination (Mezura-Montes [45]).</text:p><text:p text:style-name="P42"/><text:p text:style-name="P42">Like other Evolutionary Algorithms, DE cannot handle constrained optimization. In this context, a strict constraint stipulates that slot machine reels must adhere to specific criteria, which depend on the game rules. For instance, the reels must exclusively consist of valid symbols. This constraint is manually ensured after each new individual&apos;s reproduction, wherein a randomly selected valid symbol replaces each invalid symbol. This correction can be viewed as an addition to the mutation operation.</text:p><text:p text:style-name="P42"/><text:p text:style-name="P42">In this case, DDE is applied for goal optimization. The original DE is modified to compute the weighted difference vector using discrete values. Instead of a conventional difference, a normalized discrete difference vector is employed. This difference vector comprises three common values: minus one, zero, and plus one.</text:p><text:p text:style-name="P34"><text:soft-page-break/></text:p><text:p text:style-name="P44">DDE individuals are represented as 2D arrays of symbols, referred to as reels. All symbols are represented as integer numbers. Each DDE individual is a point in the solution space, a discrete finite space. To be considered valid, each reel should contain only integer numbers from the available symbols. In many game designs, symbols are numbered from 3 to 12. Symbols from 0 to 2 are typically reserved for special symbols known as wilds, while symbols between 14 and 16 are typically reserved for special symbols called scatters. For simplicity&apos;s sake, the slot machine&apos;s model is assumed not to have any wild or scatter symbols.</text:p><text:p text:style-name="P44"/><text:p text:style-name="P44">The model does not include free spins but a simple bonus game simulating a bingo game. There is a bonus prize for completing a bingo line and another for achieving bingo.</text:p><text:p text:style-name="P44"/><text:p text:style-name="P44">Population initialization is carried out by manually constructing reels. Some of the individuals are shuffled initially to introduce population diversity. The population size is determined experimentally and may range from a few individuals to hundreds or even thousands.</text:p><text:p text:style-name="P44"/><text:p text:style-name="P44">The first step of DDE optimization involves selecting a target vector, a base vector, and two other vectors to be used for the weighted difference vector. All four vectors are chosen randomly, a slight deviation from the <text:soft-page-break/>original DE algorithm. A discrete difference vector is calculated in the second step, with the only valid values minus one, zero, and plus one. The third step involves mutation, adding the difference vector to the base vector. Any invalid symbol numbers (in this case, 2 or 13) are randomly replaced with valid ones (ranging from 3 to 12). The fourth step entails a crossover operation between the base vector and the mutated vector, employing binomial crossover. The final fifth step is associated with calculating the fitness value and deciding which vector to retain for the next generation, either the target vector or the newly recombined one.</text:p><text:p text:style-name="P44"/><text:p text:style-name="P44">This constitutes a multi-criteria problem. Assigning weights to the three criteria are used as linear equations, effectively converting the problem into a single-criteria one. The decision maker is responsible for selecting the coefficients for each criterion. This research assigns a weight of 1 to symbol diversity, 100 to target RTP, and 10 to prize equalization. Monte-Carlo simulation is employed for estimating target RTP and prize equalization. Symbol diversity is directly calculated from the reels. To enhance accuracy in Monte-Carlo simulations, 10 sets of 1,000,000 separate slot game runs are executed.</text:p><text:p text:style-name="P44"/><text:p text:style-name="P44">The maximum number of recombinations is utilized as an optimization termination criterion. Manual observation and termination of the process are also possible. The final solution, obtained through DDE, is represented <text:soft-page-break/>as an integer matrix. This matrix can be directly applied as slot machine reel strips. For instance, if a slot game features 5 reels (visible on the screen as 5 columns and 3 rows), and each reel consists of 63 symbols, the final DDE solution would be an integer matrix with dimensions of 5x63 values.</text:p><text:p text:style-name="P34"/><text:p text:style-name="P45">In almost all cases, gambling games are mathematically unfair, meaning long-term players incur losses against the operator. This loss rate is quantified by the Return to Player (RTP) percentage, which typically ranges between 90% and 98% in many countries where gambling is legalized. However, exceptions exist, such as Nevada, where the RTP can be considerably lower.</text:p><text:p text:style-name="P45"/><text:p text:style-name="P45">The RTP carries significant statistical significance. For instance, if a player wagers $100 on a game with a 95% RTP, statistically speaking, in a single session, they can expect to receive $95 back. The RTP of a game is directly determined by the arrangement of symbols on virtual reels. From a mathematical perspective, there is no justification for keeping the distribution of symbols on the reels concealed from the players. It is well-established that gambling games are inherently mathematically biased, and legal regulators closely oversee all aspects of gambling games.</text:p><text:p text:style-name="P45"/><text:p text:style-name="P45"><text:soft-page-break/>Consequently, a player&apos;s knowledge of the virtual reels&apos; specific content does not give them any advantage. This situation is analogous to roulette, where players are fully aware of the order and colors of numbers. However, slot machines add an element of mystery by not disclosing the virtual reel configurations in the game rules.</text:p><text:p text:style-name="P45"/><text:p text:style-name="P45">If someone wishes to estimate a game&apos;s RTP without access to the original game source code, the only viable approach is to reconstruct the sequences of the reels from observed data chunks (Vaidyanathan [46]). This task can be time-consuming due to its highly combinatorial nature (Lewis [47]).</text:p><text:p text:style-name="P45"/><text:p text:style-name="P47">Sequence reconstruction is a frequently encountered problem in genetics. The essence of this issue lies in the challenge of reconstructing a complete sequence of ordered information when only fragments of it are known. In the realm of genetics, reconstruction pertains to sequences comprised of the four nucleotide bases: cytosine (C), guanine (G), adenine (A), and thymine (T). Sequence reconstruction also finds applications in other domains, such as cryptography and encoding. </text:p><text:p text:style-name="P47"/><text:p text:style-name="P47">In this study, an approximate reconstruction of sequences is pursued. The innovative approach involves searching for an optimal solution within the space of chunks rather than within the space of complete sequences.</text:p><text:p text:style-name="P45"><text:soft-page-break/></text:p><text:p text:style-name="P45">In most sequencing problems, the ultimate objective is the precise reconstruction of the analyzed sequence (Parsons [48]). However, achieving exact reconstruction is optional when dealing with virtual slot machine reels. It suffices for the reels to be reconstructed so that the player&apos;s subjective experience matches the original and reconstructed reels. This research proposes an approximate reconstruction of slot machine reels using genetic algorithms. The quality of the solutions provided by the genetic algorithm is assessed by calculating the Euclidean distance between the chunks of the candidate solution and those from the original sequences. The ultimate aim of this sequencing is to reconstruct virtual reels with properties identical to the original ones, even though an exact match between the reconstructed and original sequences may not be achieved.</text:p><text:p text:style-name="P34"/><text:p text:style-name="P46">The sequencing of virtual slot machine reels can be achieved through the exact reconstruction of the reels, but this level of precision is unnecessary. It suffices to achieve identical gameplay behavior with reconstructed reels in front of the players. When approximate reconstruction is applicable, the process commences with the collection of virtual reel chunk samples. In most cases, the length of the reel is not known in advance. In such instances, statistical analysis should be conducted to estimate the necessary number of samples for the most accurate reconstruction <text:soft-page-break/>possible. A histogram of the chunks can unveil the frequency of appearance for each observed chunk.</text:p><text:p text:style-name="P46"/><text:p text:style-name="P46">Chromosomes are encoded as candidate sequences derived from the set of possible game symbols within a particular reel. Chunk samples are extracted from the candidate sequence, mirroring the number taken from the original sequence. All candidate solution quality assessments are based on these sampled chunks.</text:p><text:p text:style-name="P46"/><text:p text:style-name="P46">Estimating fitness value involves calculating the average Euclidean distance for a sorted set of chunks in both the candidate and the original sequences as pairs. Sorting ensures that candidate chunks correspond to their respective original chunks when calculating the Euclidean distance between pairs. Original chunks are sorted only once when initially collected as samples. Candidate chunks are sorted each time the candidate sequence changes, such as crossover and/or mutation. To obtain the average value offered as chromosome fitness, all distances between chunk pairs are summed and divided by the number of chunks. The average Euclidean distance is negated, as lower fitness corresponds to a more significant deviation of the candidate solution from the original sequence. Embracing this fitness value estimation implies that the fitness value of the original sequence&apos;s distance from itself is zero, the highest possible fitness value.</text:p><text:p text:style-name="P46"><text:soft-page-break/></text:p><text:p text:style-name="P46"><draw:frame draw:style-name="fr1" draw:name="Frame24" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="4.0783in" style:rel-height="scale-min" draw:z-index="46"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image24" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="3.6902in" style:rel-height="scale" draw:z-index="47"><draw:image xlink:href="Pictures/1000000000000404000002F5472EFE60EDB51585.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure23" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">24</text:sequence>: Original sequences of five virtual reels</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P46"/><text:p text:style-name="P46">Fitness value estimation relies on the chunk list, whereas crossover and mutation operate on candidate sequences. Any change in the candidate sequences prompts an immediate recalculation of chunk samples. Modified uniform crossover is employed, introducing a normal distribution (mean of 50% and standard deviation of 20%) for determining the participation rate of the two parents. This means that one of the parents has a more significant influence on offspring formation. Since the length of the original sequence is typically unknown, the offspring <text:soft-page-break/>sequence&apos;s length must be estimated during the crossover process. The number of unique symbols in the virtual reels determines the lower bound for candidate sequence length, while the total length of all chunks combined defines the upper bound. Estimating candidate sequence length enables the genetic algorithm to optimize this parameter.</text:p><text:p text:style-name="P46"/><text:p text:style-name="P46">In most cases, parents have varying lengths, resulting in offspring that differ in length from the parents. Values are adjusted iteratively, starting from the beginning when the offspring exceeds the parents&apos; length. This approach naturally produces longer offspring, as virtual reels are used in looping during real-time gameplay. Mutation involves randomly replacing a single value in the candidate sequence with the random value drawn from the chunks of the original sequence.</text:p><text:p text:style-name="P46"/><text:p text:style-name="P46"><text:soft-page-break/><draw:frame draw:style-name="fr1" draw:name="Frame25" text:anchor-type="as-char" svg:width="5.0161in" style:rel-width="100%" svg:height="4.0783in" style:rel-height="scale-min" draw:z-index="48"><draw:text-box><text:p text:style-name="Figure"><draw:frame draw:style-name="fr4" draw:name="Image25" text:anchor-type="paragraph" svg:width="5.0161in" style:rel-width="100%" svg:height="3.6902in" style:rel-height="scale" draw:z-index="49"><draw:image xlink:href="Pictures/1000000000000404000002F557A75994DDEDD677.png" xlink:type="simple" xlink:show="embed" xlink:actuate="onLoad" draw:mime-type="image/png"/></draw:frame>Figure <text:sequence text:ref-name="refFigure24" text:name="Figure" text:formula="ooow:Figure+1" style:num-format="1">25</text:sequence>: Reconstructed sequences of five virtual reels</text:p></draw:text-box></draw:frame></text:p><text:p text:style-name="P46"/><text:p text:style-name="P46">For selection, three different chromosomes are randomly chosen. The two with better fitness among the three are selected as parents. The third chromosome is chosen for removal from the population, allowing the newly generated offspring only if it demonstrates superior fitness. This selection operator indirectly enforces the elitism rule.</text:p><text:p text:style-name="P34"/><text:p text:style-name="P34">Genetic algorithms combinatorially arrange the individual chunks of the game&apos;s reels. This approach is stochastic, and arriving at a possible solution is a purely probabilistic process. Additionally, it is possible to <text:soft-page-break/>propose a significantly more deterministic approach based on tree structures to generate all potential arrangements according to the observed chunks for reel reconstruction. </text:p><text:p text:style-name="P34"/><text:p text:style-name="P34">The task of reel reconstruction is quite similar to the task of constructing new reels. The primary difference lies in the observations of the game screen and the requirement for the observed patterns to be accurately represented in the reconstructed reels.</text:p><text:p text:style-name="P34"/><text:p text:style-name="P48">The primary objective of this proposition is to infer the discrete probability distribution of symbols across virtual reels. Each virtual reel comprises symbols, typically represented as numbers in the programmatic context. The arrangement of these symbols is a critical aspect of game design, often undertaken by statisticians, although not exclusively (Keremedchiev [49]). Mathematicians responsible for designing slot machine games must carefully select a symbol distribution that yields a specific RTP. It is worth noting that symbol distribution impacts RTP and influences game volatility and the overall gameplay experience, which is essential for player engagement.</text:p><text:p text:style-name="P48"/><text:p text:style-name="P48">The RTP of a game is calculated by dividing the total winnings by the total losses and then multiplying the result by one hundred to express it as a percentage. RTP is a standard slot machine characteristic and is subject to <text:soft-page-break/>government laws and regulations. In mathematical terms, RTP represents an expected value. In this research, RTP holds particular significance because it provides the sole means of assessing the accuracy of reconstructed reels. Even though the specifics of the reels are kept confidential as trade secrets, information about the paytable, betting lines, and the RTP of each game is widely available. The primary objective in reconstructing the reels is to ensure that the outcomes closely align with the known RTP.</text:p><text:p text:style-name="P34"/><text:p text:style-name="P49">In the literature, Monte-Carlo Search is better known as Monte-Carlo Tree Search (MCTS). It focuses on analyzing the most promising moves, using random search space sampling to expand the search tree. We will use the term Monte-Carlo Search because, in the case of this research, the structure of optimization is a graph structure, not a tree structure.</text:p><text:p text:style-name="P49"/><text:p text:style-name="P49">MCS is a heuristic search algorithm in the decision-construction process. It has everyday applications in gameplay. MCS is used in cases where building a graph is time-consuming, rendering all exact number algorithms inefficient. Games like Chess and Go are the most renowned examples of MCS usage. MCS is applicable not only in deterministic games but also in games like Poker.</text:p><text:p text:style-name="P49"/><text:p text:style-name="P49"><text:soft-page-break/>The foundation of MCS lies in analyzing the most promising further constructions. Search options are expanded via random sampling of the search space. The application of MCS in the process is based on numerous attempts. The reel is reconstructed to the maximum allowed length in each run by randomly selecting observed patterns. Each reel reconstruction is assessed based on well-defined reel start and end points and rare, game-dependent patterns.</text:p><text:p text:style-name="P49"/><text:p text:style-name="P49">At the proposed algorithm&apos;s core is screen pattern classification and the utilization of these patterns as operations in finite-state machines.</text:p><text:p text:style-name="P49"/><text:p text:style-name="P49">As input for the algorithm, screen observations are employed. Each reel is observed a specified number of times (approximately 600 times through experimentation). Observed patterns can be created manually, but image processing algorithms can be employed for automated symbol identification. The slot machine screens&apos; images are relatively static, making them ideal candidates for image processing procedures.</text:p><text:p text:style-name="P49"/><text:p text:style-name="P49">The data input processing follows these steps:</text:p><text:p text:style-name="P49">1. Input data as a sequence of characters (observed patterns).</text:p><text:p text:style-name="P49">2. Identify unique patterns and represent them as nodes in the transitions graph.</text:p><text:p text:style-name="P49"><text:s text:c="4"/>2.1 For each node, check for matches with others.</text:p><text:p text:style-name="P49"><text:soft-page-break/><text:s text:c="4"/>2.2 If a match is found, store it as an edge in the transitions graph.</text:p><text:p text:style-name="P49"><text:s text:c="4"/>2.3 If no match is found, repeat step 2.1.</text:p><text:p text:style-name="P49">3. Calculate symbol frequencies in observed patterns.</text:p><text:p text:style-name="P49">4. Initialize the reel start with a known starting pattern.</text:p><text:p text:style-name="P49"><text:s text:c="4"/>4.1 Select a random transition based on the current reel ending.</text:p><text:p text:style-name="P49"><text:s text:c="4"/>4.2 Repeat until the reel reaches its maximum length or the end pattern is encountered.</text:p><text:p text:style-name="P49">5. Evaluate and store the generated solutions.</text:p><text:p text:style-name="P49">6. Return to step 4 until a predefined time is reached.</text:p><text:p text:style-name="P49">7. Finish.</text:p><text:p text:style-name="P49"/><text:p text:style-name="P49">In step 2.1, one pattern matches the end of another found at the beginning of the second pattern. Symbol frequencies in observed patterns (step 3) are employed as statistical estimations for the quality of the generated solution. If the frequencies of symbols in the generated reel closely resemble the frequencies of observed patterns, the generated reel should be closer to the original one. Due to less frequent symbols in the reels, each virtual reel can be described with a well-known pattern for the start and end. These patterns typically revolve around special symbols. Well-known patterns can be checked between the start and end, serving as control sequences. In step 5, three types of evaluation are performed. First, symbol frequencies are calculated, followed by the Euclidean distance between the two frequency vectors. Subsequently, the count of <text:soft-page-break/>missing observed patterns in the generated reel is determined. Finally, the generated pattern is dissected into pieces of the size of the observed patterns, and each piece is checked against the transitions nodes list. If a piece is absent from the list, it indicates that the reel contains a piece not observed during the data collection stage.</text:p><text:p text:style-name="P34"/><text:p text:style-name="P50">In computerized slot machines, apart from the base game, additional features like free spins and various bonus games are often added. The RTP, expressed as a percentage, is determined by multiplying the ratio of the total amount won to the total amount bet by one hundred (Kamanas [50]). This RTP indicator represents the mathematical expectation of winnings. It indicates how many out of 100 bets can be expected to be won, on average, during one spin of the reels. For example, in a game with a 98% RTP, betting 100 coins yields an average return of 98 coins as winnings.</text:p><text:p text:style-name="P50"/><text:p text:style-name="P50">Slot machines are inherently skewed in favor of the house, offering players lower odds than other casino games. Different regulatory authorities worldwide allow for a range of RTP values, typically from 75% to 98%. It is economically impractical for gambling operators to set an RTP exceeding 100%, as this would result in the casino losing more money than it gains.</text:p><text:p text:style-name="P50"/><text:p text:style-name="P50"><text:soft-page-break/>In addition to RTP, each slot machine is characterized by its volatility. While legal regulators closely oversee RTP, volatility is not always mandated or monitored. Volatility in a game dictates how frequently wins occur and their size. Slot machines are often designed to align with the social preferences of the region in which they are distributed and played. For instance, American players favor high-volatility games, which offer infrequent but substantial wins. In Eastern Europe, on the other hand, players typically prefer low-volatility games, where wins occur more frequently but are smaller in size. The volatility of a game is often rated on a scale of one to five stars, indicating low to high volatility.</text:p><text:p text:style-name="P50"/><text:p text:style-name="P50">Various approaches can be employed to address different forms of uncertainty, such as the volatility inherent in gambling games. Some of these approaches rely on established optimization strategies proposed by Wald, Laplace, Hurwitz, and Savage. In certain situations, heuristic methods can also prove effective. A comprehensive review of uncertainty measures in evidence theory and an analysis of related controversies can be found in (Deng [51]). In cases where prior information is available regarding parameters, uncertainty can be quantified by considering this prior knowledge, as demonstrated in (Patra [52]).</text:p><text:p text:style-name="P34"/><text:p text:style-name="P51">The volatility of a slot machine becomes a matter of interest when the gambling product undergoes scrutiny from regulators or testing from <text:soft-page-break/>competing manufacturers. In the legal regulation of slot machines, manufacturers are required to provide comprehensive and detailed documentation for each product. This documentation should include precise values for the virtual reels. With access to the paytable, virtual reel data, winning combinations, and rules governing free spins and bonus games, calculating the RTP and volatility involves combinatorial calculations. However, only the pay-out table, pay lines/patterns, free spins rules, and bonus game rules are known when the manufacturer&apos;s documentation is unavailable. Determining RTP and volatility necessitates empirical research when the virtual reels remain undisclosed.</text:p><text:p text:style-name="P51"/><text:p text:style-name="P51">A rough estimate of the RTP value can be made by repeatedly initiating reel spins. Some research laboratories, such as GLI, approximate RTP by playing 14000 spins, considering the total winnings and total bets, and then calculating the ratio between the two. Volatility can be calculated as an estimate of deviation from the central tendency using the following formula:</text:p><text:p text:style-name="P51"/><text:p text:style-name="P51">V = C * sqrt( sum( Hi * (Pi-R)^2 ) / N )</text:p><text:p text:style-name="P51"/><text:p text:style-name="P51">Here, V represents the volatility index, C denotes the confidence interval, Hi signifies the frequency of the i-th winning combination, Pi stands for the value of the i-th win, R is the value of RTP, and N is the number of <text:soft-page-break/>spins. It is worth noting that volatility is calculated by playing only one pay line, typically the central one. Most modern slot machines are rarely played with a single pay line, and in some games, a single-line configuration is impossible.</text:p><text:p text:style-name="P51"/><text:p text:style-name="P51">In the most common game screen configuration, slot machines feature 5 reels and 3 rows, making 15 symbols (5x3) visible on the screen, allowing for various pay-out patterns. When the exact virtual reels are known, it is possible to collect sufficient statistics through Monte Carlo simulations to estimate volatility. However, when virtual reels remain unknown, information must be gathered through multiple observations of different spins and stops. Stopping the virtual reels reveals authentic snippets of the symbol order. Often, the animation simulating rotating reels does not provide genuine information about the symbols&apos; locations but images designed to create the illusion of rotation. Hence, the only reliable and authentic information is the game screen state after the spin has stopped. Additionally, it is essential to emphasize that games must be explored in real mode with actual bets, as demonstration modes frequently employ different virtual reels.</text:p><text:p text:style-name="P51"/><text:p text:style-name="P51">Monte Carlo, simulations of slot machines entail stopping the virtual reels at randomly selected positions while considering the symbols before and after the random stop. In the absence of complete information about the <text:soft-page-break/>virtual reels, observing the segments obtained from different reel stops is proposed. These resulting segments provide a statistical insight into the actual symbol positions. In the simulation, the playing screen is not constructed from actual reels. Instead, segments are chosen based on their encounter frequency.</text:p><text:p text:style-name="P34"/><text:h text:style-name="P68" text:outline-level="1"><text:bookmark-start text:name="__RefHeading___Toc2347_1821873452"/>5 Problems with Human Evaluation of Fitness Function<text:bookmark-end text:name="__RefHeading___Toc2347_1821873452"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P3">In some optimization tasks, achieving an acceptable solution cannot be determined through exact mathematical calculations but results from subjective human evaluation. An example of such tasks includes those where a picture&apos;s beauty or music&apos;s melody is evaluated. Human intuition is also something that modern computers cannot formalize yet. An example of such a situation is the intuition about whether the price of a specific exchange-traded commodity will rise or fall. People process information in both their conscious and subconscious minds. This highly complex processing leads to what is referred to as intuition. We specify a solution, and that solution turns out to be correct without us being able to explain why that particular solution is chosen. In such tasks involving evolutionary algorithms, there is no alternative but to create an evaluation function through a dialogue between humans and computing machines.</text:p><text:p text:style-name="P52"/><text:h text:style-name="P69" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc2207_1422372469"/><text:span text:style-name="T18">5.1 </text:span>Human Evaluation in Art<text:bookmark-end text:name="__RefHeading___Toc2207_1422372469"/></text:h><text:p text:style-name="P52"/><text:p text:style-name="P72"><text:soft-page-break/>Due to the high degree of formalism in music, many composers developed various compositional methods over the centuries. With computers capable of playing music, the concept of automatic music generation has garnered the attention of numerous researchers.</text:p><text:p text:style-name="P72"/><text:p text:style-name="P72">Many systems for generating musical scores existed long before the advent of computers. One such system was Mozart&apos;s Musikalisches Wurfelspiel, which utilized dice throws to select measures from a vast collection of small phrases randomly. When combined, these phrases created musical pieces that human players could perform. While these works were not composed using modern computers, Mozart employed rudimentary algorithmic techniques that are now commonly used with electronic binary digital computers since the Second World War.</text:p><text:p text:style-name="P72"/><text:p text:style-name="P72">The world&apos;s first computer-generated digital music originated in Australia, created by programmer Geoff Hill on the CSIRAC computer designed and built by Trevor Pearcey and Maston Beard. Subsequently, Iannis Xenakis was one of the earliest composers to write music with a computer. He developed programs in the FORTRAN language that generated numerical data, which he then transcribed into scores for traditional musical instruments. An example of his work is ST/48 from 1962. Although Xenakis could have composed this music manually, the computer&apos;s <text:soft-page-break/>computational power was essential for the complex calculations required to transform probabilistic mathematics into musical notation.</text:p><text:p text:style-name="P72"/><text:p text:style-name="P72">Computers have also been employed to mimic the music of renowned past composers like Mozart. One contemporary exponent of this technique is David Cope. He authored computer programs that analyze the works of other composers to produce new compositions in a similar style. His program, Experiments in Musical Intelligence, is famous for creating &quot;Mozart&apos;s 42nd Symphony&quot; and has significantly impacted composers such as Bach and Mozart. He also combines his creations with those of the computer in his own pieces.</text:p><text:p text:style-name="P72"/><text:p text:style-name="P72">Some critics argue that computers cannot produce music of the same quality as great composers. In contrast, others question whether this is the primary objective of producing music in this manner.</text:p><text:p text:style-name="P72"/><text:p text:style-name="P72">Joy Schoenberger&apos;s project (Schoenberger [53]) aims to evolve a compositional model with a genetic map based on the piece&apos;s characteristics to be composed. The rules of Western Tonal Theory dictate the fitness function, and composition occurs at the phrase level. The resulting song is a coherent work due to shared genotypes among its phrases with differing phenotypes.</text:p><text:p text:style-name="P72"/><text:p text:style-name="P72"><text:soft-page-break/>Bruce Jacob describes a composition process that blends the best of two extremes: traditional stochastic methods seen in M and Jam Factory and complex rule-based systems like EMI or Cypher. This approach achieves the simplicity of a stochastic process and the determinism of a rule-based system (Jacob [54]).</text:p><text:p text:style-name="P72"/><text:p text:style-name="P72">Scott Draves conducts exciting work in his project Electric Sheep (Draves [55]). Electric Sheep is a distributed screen-saver that harnesses idle computers into a render farm to animate and evolve artificial life forms called sheep. Users&apos; votes form the basis for the genetic algorithm&apos;s fitness function in the space of fractal animations. Users can also manually design sheep for inclusion in the gene pool. Electric Sheep serves as an amplifier of human collaborators&apos; creativity rather than a traditional genetic algorithm optimizing a fitness function.</text:p><text:p text:style-name="P72"/><text:p text:style-name="P72">The primary challenge for researchers is achieving global harmony. In Schoenberger&apos;s work, they achieved excellent local harmony (a few tones), but global results could have been more satisfying. In Draves&apos; project, they successfully achieved harmony at a global level, but it was applied to movie composition. A combined approach that incorporates the strengths of both methods is proposed for music improvisation through interaction between humans and computers.</text:p><text:p text:style-name="P72"/><text:p text:style-name="P72"><text:soft-page-break/></text:p><text:p text:style-name="P52"/><text:h text:style-name="P69" text:outline-level="2"><text:bookmark-start text:name="__RefHeading___Toc2209_1422372469"/><text:span text:style-name="T18">5.2 </text:span>Intuitive Human Evaluation<text:bookmark-end text:name="__RefHeading___Toc2209_1422372469"/></text:h><text:p text:style-name="P52"/><text:p text:style-name="P52"/><text:p text:style-name="P52"/><text:h text:style-name="P68" text:outline-level="1"><text:bookmark-start text:name="__RefHeading___Toc2349_1821873452"/>6 Evolutionary Algorithms in Vectorization Problems<text:bookmark-end text:name="__RefHeading___Toc2349_1821873452"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P3"/><text:p text:style-name="P53"/><text:h text:style-name="P68" text:outline-level="1"><text:bookmark-start text:name="__RefHeading___Toc2351_1821873452"/>7 Evolutionary Algorithms in Combination with Curve Fitting<text:bookmark-end text:name="__RefHeading___Toc2351_1821873452"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P3"/><text:p text:style-name="P54"/><text:h text:style-name="P68" text:outline-level="1"><text:bookmark-start text:name="__RefHeading___Toc2353_1821873452"/>8 Evolutionary Algorithms in Multi-modal or Multi-objective Problems<text:bookmark-end text:name="__RefHeading___Toc2353_1821873452"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P3"/><text:p text:style-name="P55"/><text:h text:style-name="P68" text:outline-level="1"><text:bookmark-start text:name="__RefHeading___Toc2355_1821873452"/>9 Modification of Components in Evolutionary Algorithms<text:bookmark-end text:name="__RefHeading___Toc2355_1821873452"/></text:h><text:p text:style-name="P3"/><text:p text:style-name="P3"/><text:p text:style-name="P3"/><text:bibliography text:style-name="Sect1" text:name="Bibliography1"><text:bibliography-source><text:index-title-template text:style-name="Bibliography_20_Heading">Bibliography</text:index-title-template><text:bibliography-entry-template text:bibliography-type="article" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="book" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="booklet" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="conference" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="custom1" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="custom2" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="custom3" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="custom4" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="custom5" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="email" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="inbook" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="incollection" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="inproceedings" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="journal" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="manual" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="mastersthesis" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="misc" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="url"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="phdthesis" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="proceedings" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="techreport" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="unpublished" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template><text:bibliography-entry-template text:bibliography-type="www" text:style-name="Bibliography_20_1"><text:index-entry-bibliography text:bibliography-data-field="identifier"/><text:index-entry-span>: </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="author"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="title"/><text:index-entry-span>, </text:index-entry-span><text:index-entry-bibliography text:bibliography-data-field="year"/></text:bibliography-entry-template></text:bibliography-source><text:index-body><text:index-title text:style-name="Sect1" text:name="Bibliography1_Head"><text:p text:style-name="P17">Bibliography</text:p></text:index-title></text:index-body></text:bibliography><text:p text:style-name="P3"><text:span text:style-name="T6">[1] </text:span>Dunis, C.L. Williams, M., Modelling and trading the eur/usd exchange rate: Do neural network models perform better? Derivatives Use, Trading and Regulation, 8(3), pp. 211-239 (2002)</text:p><text:p text:style-name="P6">[<text:span text:style-name="T7">2</text:span>] Giles, C.L., Lawrence, S. Tsoi, A.C., Noisy time series prediction using a recurrent neural network and grammatical inference. Machine Learning, 44(1/2), pp. 161-183 (2001)</text:p><text:p text:style-name="P7">[3] Moody, J.E., Economic forecasting: Challenges and neural network solutions. Proceedings of the International Symposium on Artificial Neural Networks, Hsinchu, Taiwan (1995)</text:p><text:p text:style-name="P7">[4] Haykin, S., Neural Networks, A Comprehensive Foundation. Prentice-Hall, Inc., 2nd edition (1999)</text:p><text:p text:style-name="P7">[5] Werbos, P., Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78(10), pp. 1550-1560 (1990)</text:p><text:p text:style-name="P7">[6] Yao, X., Evolving artificial neural networks. Proc of the IEEE, 87(9), pp. 1423-1447 (1999)</text:p><text:p text:style-name="P7">[7] Holland, J., Adaptation In Natural and Artificial Systems. The University of Michigan Press (1975)</text:p><text:p text:style-name="P11">[8] Storn, R., Price, K. Differential evolution - a simple and efficient heuristic for global optimization over continuous spaces. Journal of Global Optimization 11, 341-359 (1997)</text:p><text:p text:style-name="P7"><text:soft-page-break/>[9] Goldberg, D.E.: Genetic and evolutionary algorithms come of age. Commun. ACM 37(3), 113-119 (1994)</text:p><text:p text:style-name="P7">[10] Pappa, G., Ochoa, G., Hyde, M., Freitas, A., Woodward, J., Swan, J.: Contrasting meta-learning and hyper-heuristic research: the role of evolutionary algorithms. Genet. Program. Evolvable Mach. 15(1), 335 (2014)</text:p><text:p text:style-name="P7">[11] Adamidis, P.: Review of parallel genetic algorithms bibliography. Technical Report version 1, Aristotle University of Thessaloniki, Thessaloniki, Greece (1994)</text:p><text:p text:style-name="P7">[12] Gordon, V.S., Whitley, D.: Serial and parallel genetic algorithms as function optimizers. In: Forrest S. (ed.) Proceedings of the Fifth International Conference on Genetic Algorithms, pp. 177183. Morgan Kaufmann, San Mateo (1993)</text:p><text:p text:style-name="P7">[13] Lin, S.-C., Punch, W., Goodman, E.: Coarse-grain parallel genetic algorithms - categorization and new approach. In: Sixth IEEE Symposium on Parallel and Distributed Processing, IEEE Computer Society Press, Los Alamitos, CA (1994)</text:p><text:p text:style-name="P7">[14] Spiessens, P., Manderick, B.: A massively parallel genetic algorithm. In: Belew, R.K., Booker, L.B. (eds.) Proceedings of the 4th International Conference on Genetic Algorithms, pp. 279286. Morgan Kaufmann, San Francisco (1991)</text:p><text:p text:style-name="P7">[15] Kruger, F., Wagner, D., Collet, P.: Massively parallel generational GA on GPGPU applied to power load profiles determination. In: Legrand, P., <text:soft-page-break/>Corsini, M.-M., Hao, J.-K., Monmarche, N., Lutton, E., Schoenauer, M. (eds.) EA 2013. LNCS, vol. 8752, pp. 227239. Springer, Heidelberg (2014)</text:p><text:p text:style-name="P7">[16] Tanese, R.: Distributed genetic algorithms. In: Schaffer, J.D. (ed.) Proceedings of the 3rd International Conference on Genetic Algorithms, pp. 434439. Morgan Kaufmann, San Francisco (1989)</text:p><text:p text:style-name="P7">[17] Uchida, T., Matsuzawa, T., Inoguchi, Y.: The influence of elitism strategy on migration intervals of a distributed genetic algorithm. In: Proceedings in Adaptation, Learning and Optimization, vol. 2, pp. 363374 (2015)</text:p><text:p text:style-name="P7">[18] Gorges-Schleuter, M.: ASPARAGOS An asynchronous parallel genetic optimisation strategy. In: Schaffer, J.D. (ed.) Proceedings of the 3rd ICGA, pp. 422427. Morgan Kaufmann, San Francisco (1989)</text:p><text:p text:style-name="P7">[19] Munetomo, M., Takai, Y., Sato, Y.: An efficient migration scheme for subpopulation-based asynchronously parallel GAs. Technical Report HIER-IS- 9301, Hokkaido University (1993)</text:p><text:p text:style-name="P7">[20] Voigt, H.M., Santibanez-Koref, I., Born, J.: Hierarchically structured distributed genetic algorithms. In: Manner, R., Manderick, B. (eds.) Proceedings of the International Conference Parallel Problem Solving from Nature, vol. 2, pp. 155164. North-Holland, Amsterdam (1992)</text:p><text:p text:style-name="P7">[21] Belding, T.C.: The distributed genetic algorithm revisited. In: Eshelman, L.J. (ed.) Proceedings of the 6th International Conference on GAs, pp. 122129. Morgan Kaufmann, San Francisco (1995)</text:p><text:p text:style-name="P7"><text:soft-page-break/>[22] Mejia-Olvera, M., Cantu-Paz, E.: DGENESIS-software for the execution of distributed genetic algorithms. In: Proceedings of the XX Conferencia Latinoamericana de Informatica, pp. 935946. Monterrey, Mexico (1994)</text:p><text:p text:style-name="P7">[23] Baker, J.E.: Reducing bias and inefficiency in the selection algorithm. In: Grefenstette, J.J. (ed.) Proceedings of the Second International Conference on Genetic Algorithms, pp. 1421. Lawrence Erlbaum Associates Publishers, Hillsdale (1987)</text:p><text:p text:style-name="P7">[24] Lim, T.Y.: Structured population genetic algorithms: a literature survey. Artif. Intell. Rev. 41(3), 385399 (2014)</text:p><text:p text:style-name="P7"><text:span text:style-name="T13">[</text:span>25<text:span text:style-name="T13">]</text:span> Keremedchiev, D., Barova, M., Tomov, P., Mobile application as distributed computing system for artificial neural networks training used in perfect information games, Proceedings of 16th International scientific conference UNITECH16, Gabrovo, vol. 2, pp.389-393 (2016)</text:p><text:p text:style-name="P7"><text:span text:style-name="T13">[</text:span>26<text:span text:style-name="T13">]</text:span> Tomov, P., Monov, V., Artificial Neural Networks and Differential Evolution Used for Time Series Forecasting in Distributed Environment, Proceedings of International conference AUTOMATICS AND INFORMATICS, pp. 129-132, Sofia (2016)</text:p><text:p text:style-name="P7"><text:span text:style-name="T13">[</text:span>27<text:span text:style-name="T13">]</text:span> Zankinski, I., Stoilov, T., Effect of the Neuron Permutation Problem on Training Artificial Neural Networks with Genetic Algorithms in Distributed Computing, Proceedings of XXIV International Symposium Management of Energy, Industrial and Environmental Systems, Bankya, pp. 53-55 (2016)</text:p><text:p text:style-name="P7">[28] Karlik, B., Vehbi, A., Performance Analysis of Various Activation Functions in Generalized MLP Architectures of Neural Networks. <text:soft-page-break/>International Journal of Artificial Intelligence And Expert Systems (IJAE), vol. 1, no. 4, pp. 111-122 (2011)</text:p><text:p text:style-name="P7">[29] Atanasova T., Barova, M.: Exploratory analysis of time series for hypothesize feature values. In: Proceedings of International Scientific Conference UniTech17, Gabrovo, Bulgaria, vol. 2, pp. 399403 (2017)</text:p><text:p text:style-name="P7">[30] Tashev, T., Hristov, H.: Modeling of synthesis of information processes with generalized nets. In: Drinov, M. (ed.) Cybernetics and Information Technologies, vol. 2, pp. 92104. Academic Publishing House, Sofia (2003)</text:p><text:p text:style-name="P7">[31] Alexandrov, A.: Ad-hoc Kalman filter based fusion algorithm for real-time wireless sensor data integration. Flexible Query Answering Systems 2015. AISC, vol. 400, pp. 151159. Springer, Cham (2016)</text:p><text:p text:style-name="P7">[32] Korf, R.: Finding optimal solutions to Rubiks cube using pattern databases. In: AAAI-1998 Proceedings, pp. 700705. AAAI Press, Menlo Park (1998)</text:p><text:p text:style-name="P7">[33] Randall, K.: Cilk - Efficient Multithreaded Computing. Doctor of Philosophy Thesis in Computer Science and Engineering, Massachusetts Institute of Technology, USA (1998)</text:p><text:p text:style-name="P7">[34] Poli, R., Kozak, J.: Genetic programming. In: Burke, E.K., Kendall, G. (eds.) Search Methodologies, pp. 143185. Springer, Boston (2014)</text:p><text:p text:style-name="P7">[35] Brysbaert, M.: Algorithms for randomness in the behavioral sciences: A tutorial Behavior Research Methods, Instruments, and Computers vol.22 issue 1, pp.45-60, (1991)</text:p><text:p text:style-name="P7"><text:soft-page-break/>[36] Parke, J., Griffiths, M.: The psychology of the fruit machine: The role of structural characteristics (revisited), Paper presented at the annual conference of The British Psychological Society, Surrey, England (2001)</text:p><text:p text:style-name="P7">[37] Osesa, N.: Bitz and Pizzas Optimal stopping strategy for a slot machine bonus game, OR Insight, 22, pp.3144 (2009)</text:p><text:p text:style-name="P7">[38] Eiben, A. E: Genetic algorithms with multi-parent recombination. PPSN III: Proceedings of the International Conference on Evolutionary Computation. The Third Conference on Parallel Problem Solving from Nature: 7887, (1994)</text:p><text:p text:style-name="P7">[39] Ting, Chuan-Kang: On the Mean Convergence Time of Multi-parent Genetic Algorithms Without Selection. Advances in Artificial Life: 403412, (2005)</text:p><text:p text:style-name="P7">[40] Inge, S.: Electronic gaming device utilizing a random number generator for selecting the reel stop positions. US 4448419 A, Published 1984-05-15 (1984)</text:p><text:p text:style-name="P7">[41] Cooper, M.: How slot machines give gamblers the business. The Atlantic Monthly Group. Retrieved 2008-04-21 (2005)</text:p><text:p text:style-name="P7">[42] Price K.: An introduction to differential evolution. In David Corne, Marco Dorigo, and Fred Glover, editors, New Ideas in Optimization, Mc Graw-Hill, UK, 79-108 (1999)</text:p><text:p text:style-name="P7">[43] Goldberg D.: Genetic Algorithms in Search, Optimization and Machine Learning. Addison-Wesly Publishing Co., Reading, Massachusetts (1989)</text:p><text:p text:style-name="P7"><text:soft-page-break/>[44] Hans-Paul Schwefel, editor: Evolution and Optimization Seeking. John Wiley and Sons, New York (1995)</text:p><text:p text:style-name="P7">[45] Mezura-Montes, E., Velazquez-Reyes, J., Coello, C.: Modified Differential Evolution for Constrained Optimization. IEEE Congress on Evolutionary Computation, Vancouver, 25-32 (2006)</text:p><text:p text:style-name="P7">[46] Vaidyanathan, P.P., Phoong, S.M.: Reconstruction of Sequences from Nonuniform Samples, Proceedings of International Symposium on Circuits and Systems, vol. 1, 601-604 (1995)</text:p><text:p text:style-name="P7">[47] Lewis, P.O.: A Genetic Algorithm for Maximum-Likelihood Phylogeny Inference Using Nucleotide Sequence Data, Molecular Biology and Evolution, vol. 15, no. 3, 277-283 (1998)</text:p><text:p text:style-name="P7">[48] Parsons, R., Johnson, M.: A Case Study in Experimental Design Applied to Genetic Algorithms with Applications to DNA Sequence Assembly, American Journal of Mathematical and Management Sciences, vol. 17, no. 3-4, 369-396 (1997)</text:p><text:p text:style-name="P7">[49] Keremedchiev, D., Tomov, P., Barova, M.: Slot Machine Base Game Evolutionary RTP Optimization, Numerical Analysis and Its Applications, Lecture Notes in Computer Science, vol 10187. Springer, Cham (2017)</text:p><text:p text:style-name="P7">[50] Kamanas, P., Sifaleras, A., Samaras, N.: Slot machine RTP optimization using variable neighborhood search, Mathematical Problems in Engineering, 8784065 (2021)</text:p><text:p text:style-name="P7">[51] Deng, Y.: Uncertainty measure in evidence theory, Science China Information Sciences 63, 210201 (2020)</text:p><text:p text:style-name="P7"><text:soft-page-break/>[52] Patra, L. K., Kayal, S., Kumar, S.: Measuring uncertainty under prior information, IEEE Transactions on Information Theory 66(4), 25702580 (2020)</text:p><text:p text:style-name="P7">[53] Schoenberger, J.: Genetic Algorithms for Musical Composition with Coherency Through Genotype, Master&apos;s thesis, College of William and Mary (2002)</text:p><text:p text:style-name="P7">[54] Jacob, B.: Composing With Genetic Algorithms, Proceedings of the 1994 International Computer Music Conference, 452-455 (1995)</text:p><text:p text:style-name="P7">[55] Draves, S.: The Electric Sheep Screen-Saver: A Case Study in Aesthetic Evolution, Applications of Evolutionary Computing, Lecture Notes in Computer Science, vol 3449, Springer, Berlin, Heidelberg (2005)</text:p><text:p text:style-name="P7"/><text:p text:style-name="P7"/><text:p text:style-name="P7"/><text:p text:style-name="P7"/><text:p text:style-name="P7"/><text:p text:style-name="P7"/><text:table-of-content text:style-name="Sect1" text:name="Table of Contents1"><text:table-of-content-source text:outline-level="10"><text:index-title-template text:style-name="Contents_20_Heading">Table of Contents</text:index-title-template><text:table-of-content-entry-template text:outline-level="1" text:style-name="Contents_20_1"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="2" text:style-name="Contents_20_2"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="3" text:style-name="Contents_20_3"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="4" text:style-name="Contents_20_4"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="5" text:style-name="Contents_20_5"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="6" text:style-name="Contents_20_6"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="7" text:style-name="Contents_20_7"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="8" text:style-name="Contents_20_8"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="9" text:style-name="Contents_20_9"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template><text:table-of-content-entry-template text:outline-level="10" text:style-name="Contents_20_10"><text:index-entry-link-start text:style-name="Index_20_Link"/><text:index-entry-chapter/><text:index-entry-text/><text:index-entry-tab-stop style:type="right" style:leader-char="."/><text:index-entry-page-number/><text:index-entry-link-end/></text:table-of-content-entry-template></text:table-of-content-source><text:index-body><text:index-title text:style-name="Sect1" text:name="Table of Contents1_Head"><text:p text:style-name="P61">Table of Contents</text:p></text:index-title><text:p text:style-name="P62"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc552_3689700921" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">1 Time Series Prediction by Artificial Neural Networks and Differential Evolution in Distributed Environment<text:tab/>1</text:a></text:p><text:p text:style-name="P63"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc554_3689700921" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">1.1 Introduction<text:tab/>1</text:a></text:p><text:p text:style-name="P63"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc556_3689700921" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">1.2 Problem Definition<text:tab/>4</text:a></text:p><text:p text:style-name="P63"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc472_3627904809" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">1.3 Machine Learning Tools<text:tab/>4</text:a></text:p><text:p text:style-name="P63"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc474_3627904809" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">1.4 Forecasting Organization<text:tab/>7</text:a></text:p><text:p text:style-name="P62"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc351_966847004" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">2 Distributed Evolutionary Computing Migration Strategy by Accidental Node Involvement<text:tab/>16</text:a></text:p><text:p text:style-name="P63"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc979_966847004" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">2.1 Introduction<text:tab/>17</text:a></text:p><text:p text:style-name="P63"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc772_951591551" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">2.2 Accidental Node Involvement<text:tab/>19</text:a></text:p><text:p text:style-name="P63"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc632_1783862520" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">2.3 Distribution Parameterization<text:tab/>22</text:a></text:p><text:p text:style-name="P62"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc634_1783862520" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">3 Modifications of Artificial Neural Networks<text:tab/>27</text:a></text:p><text:p text:style-name="P63"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc718_1268615346" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">3.1 Alternative Activation Function Derivative<text:tab/>27</text:a></text:p><text:p text:style-name="P63"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc1666_1268615346" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">3.2 Long Short Term Memory in Multilayer Perceptron Pair<text:tab/>35</text:a></text:p><text:p text:style-name="P63"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc1317_2021269790" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">3.3 Self-Growing Multilayer Perceptron for Time Series Forecasting<text:tab/>38</text:a></text:p><text:p text:style-name="P63"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc2431_612792312" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">3.4 Permutations in Graph Structure<text:tab/>45</text:a></text:p><text:p text:style-name="P62"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc2345_1821873452" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">4 Evolutionary Algorithms in Games Combinatorial Problems<text:tab/>58</text:a></text:p><text:p text:style-name="P63"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc3265_968957226" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">4.1 Solving Combinatorial Puzzles with Parallel Evolutionary Algorithms<text:tab/>58</text:a></text:p><text:p text:style-name="P63"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc3267_968957226" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">4.2 Virtual Reels in Slot Machines<text:tab/>66</text:a></text:p><text:p text:style-name="P62"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc2347_1821873452" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">5 Problems with Human Evaluation of Fitness Function<text:tab/>93</text:a></text:p><text:p text:style-name="P63"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc2207_1422372469" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link"><text:soft-page-break/>5.1 Human Evaluation in Art<text:tab/>93</text:a></text:p><text:p text:style-name="P63"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc2209_1422372469" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">5.2 Intuitive Human Evaluation<text:tab/>97</text:a></text:p><text:p text:style-name="P62"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc2349_1821873452" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">6 Evolutionary Algorithms in Vectorization Problems<text:tab/>98</text:a></text:p><text:p text:style-name="P62"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc2351_1821873452" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">7 Evolutionary Algorithms in Combination with Curve Fitting<text:tab/>99</text:a></text:p><text:p text:style-name="P62"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc2353_1821873452" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">8 Evolutionary Algorithms in Multi-modal or Multi-objective Problems<text:tab/>100</text:a></text:p><text:p text:style-name="P62"><text:a xlink:type="simple" xlink:href="#__RefHeading___Toc2355_1821873452" text:style-name="Index_20_Link" text:visited-style-name="Index_20_Link">9 Modification of Components in Evolutionary Algorithms<text:tab/>101</text:a></text:p></text:index-body></text:table-of-content><text:p text:style-name="P3"/></office:text></office:body></office:document-content>